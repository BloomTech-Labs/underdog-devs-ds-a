company,job_title,location,description
"Beacon Data, Inc.",Data Engineer,"Salt Lake City Metropolitan Area
Remote","Posted by

Matthew Cole 2nd

Managing Partner at Beacon Data, Inc.

Send InMail

ABOUT BEACON DATA

As a full stack data and growth consulting firm, Beacon leverages machine and human intelligence to clients seeking breakthrough performance. Organizations don’t just call us to architect smart data strategy – they call us to implement it. As their expert data partner, our cross functional team parachutes in to optimize data environments, clean inconsistent data, and operationalize AI / ML use cases that accelerate growth. At Beacon, impact means more than improving the bottom line: one in four clients are mission-based organizations.

BEACON PROJECTS

No Beacon project is ever the same; we work across multiple sectors, providing unique learning and development opportunities. Examples include: deduping and stitching together millions of airline customer records to increase FFP membership; decreasing idle rates for a supply chain services company by predicting optimal inventory levels; leveraging data signals to optimize fundraising efforts for the leading children’s hospital charity; architecting and implementing expert data strategy for the largest online theater ticketing company; and using ML to optimize each dial attempt and improve credit cardholder debt collection efforts for a global contact center.

WHAT YOU'LL DO

Beacon is seeking a talented Date Engineer to join our data team and help us solve complex and interesting data-centric questions and problems for our diverse clients. No two days will be the same, and you should anticipate supporting multiple clients and projects concurrently in a fast-paced and highly collaborative environment. Your team will:

Analyze the data flowing through the existing ERPs, CRMs, and other workflow systems
Structure and redesign the data flows with the aim of maintaining single source of truth for all master data and consistent reporting
Set up and maintain the appropriate data infrastructure to support the above plan on a cloud environment, preferably AWS (ex: Redshift, EC2, ELT tools, data wrangling tools, etc.)
Implement infrastructure as a code using various tools (ex: Terraform, Ansible, etc.)
Build modularized and containerized services that are platform agnostic
Acquire, ingest, and process data from multiple sources and systems into the data warehouse
Collaborate with business teams to map data fields to hypotheses and curate, wrangle, and prepare data for use in reporting and BI
Maintain the highest security standards across the entire data ecosystem
Implement and maintain BI & reporting tools
Provide business insights using consolidated data

QUALIFICATIONS

Required

Strong technical knowledge and experience in SQL, Python and *nix environments
Experience in traditional data warehousing and deploying ETL/ ELT processes
Experience working with cloud platforms (AWS and/or Azure preferred)
Meaningful experience in multiple database technologies such as traditional RDBMS (MS SQL Server, Oracle, MySQL, PostgreSQL), MPP (AWS Redshift, Snowflake, Teradata), Distributed Processing (Spark, Hadoop, EMR), NoSQL (MongoDB, DynamoDB, Cassandra, Neo4J, Titan)
Experience working with Rest APIs for data extraction
Knowledge of Information Security principles to ensure effective management of client data
Extraordinary attention to detail

Other valued qualifications

Containerization services and tools: Docker, Kubernetes, AWS ECS, AWS Fargate, etc.
Infrastructure setup tools: Terraform, Ansible, CloudFormation, etc.
Data wrangling and orchestration tools: DBT, Airflow, Meltano, etc.
Cloud platforms: AWS, Azure, GCP, etc.

REQUIREMENTS

Need to be eligible to work in USA
Willing to commute periodically to client work sites

WHO YOU'LL WORK WITH

You will work with the Beacon technical team on a daily basis, and with client representatives as needed. The Beacon senior Project Lead/Project manager provides oversight, support, and direction. Anticipate a dynamic environment with active cross-team communication and collaborative problem-solving focused on building tools for improving client effectiveness with data.

WHO YOU ARE

You are a highly collaborative individual who can lay aside your own agenda, listening to and learning from colleagues, challenging thoughtfully and prioritizing impact. You search for ways to improve things and work collaboratively with colleagues. You believe in iterative change, experimenting with new approaches, learning, and improving to move forward quickly. You also like to have fun and value authenticity.

As an equal opportunity employer, Beacon Data encourages applications from all backgrounds regardless of gender, race, disability, pregnancy, marital status, age, sexual orientation, gender reassignment, religion, or belief. We maintain a sense of community rooted in respect and consideration for all employees where any

evaluation is based simply upon individual work and team performance."
Amazon Web Services (AWS),Sr. Data & ML Engineer,"Mississippi, United States","Description

At Amazon Web Services (AWS), we’re hiring highly technical Data and Machine Learning engineers to collaborate with our customers and partners on key engagements. Our consultants will develop and deliver proof-of-concept projects, technical workshops, and support implementation projects. These professional services engagements will focus on customer solutions such as Machine Learning, Data and Analytics, HPC and more.

In this role, you will work with our partners, customers and focus on our AWS offerings such Amazon Kinesis, AWS Glue, Amazon Redshift, Amazon EMR, Amazon Athena, Amazon SageMaker and more. You will help our customers and partners to remove the constraints that prevent them from leveraging their data to develop business insights.

AWS Professional Services engage in a wide variety of projects for customers and partners, providing collective experience from across the AWS customer base and are obsessed about customer success. Our team collaborates across the entire AWS organization to bring access to product and service teams, to get the right solution delivered and drive feature innovation based upon customer needs.

You will also have the opportunity to create white papers, writing blogs, build demos and other reusable collateral that can be used by our customers. Most importantly, you will work closely with our Solution Architects, Data Scientists and Service Engineering teams.

The ideal candidate will have extensive experience with design, development and operations that leverages deep knowledge in the use of services like Amazon Kinesis, Apache Kafka, Apache Spark, Amazon Sagemaker, Amazon EMR, NoSQL technologies and other 3rd parties.

Inclusive Team Culture

Here at AWS, we embrace our differences. We are committed to furthering our culture of inclusion. We have thirteen employee-led affinity groups, reaching 85,000 employees in over 190 chapters globally. We have innovative benefit offerings, and host annual and ongoing learning experiences, including our Conversations on Race and Ethnicity (CORE) and AmazeCon (gender diversity) conferences. Amazon’s culture of inclusion is reinforced within our 16 Leadership Principles, which remind team members to seek diverse perspectives, learn and be curious, and earn trust.

Work/Life Balance

Our team puts a high value on work-life harmony. Striking a healthy balance between your personal and professional life is crucial to your happiness and success here. We are a customer-obsessed organization—leaders start with the customer and work backwards. They work vigorously to earn and keep customer trust. As such, this is a customer facing role in a hybrid delivery model. Project engagements include remote delivery methods and onsite engagement that will include travel to customer locations as needed.

Mentorship & Career Growth

Our team is dedicated to supporting new members. We have a broad mix of experience levels and tenures, and we’re building an environment that celebrates knowledge sharing and mentorship. We care about your career growth and strive to assign projects based on what will help each team member develop into a better-rounded professional and enable them to take on more complex tasks in the future.

This is a customer facing role. You will be required to travel to client locations and deliver professional services when needed.


Basic Qualifications

Bachelor’s degree in Computer Science, Engineering, Mathematics or a related field or equivalent professional or military experience
8+ years of experience of Data platform implementation
3+ years of hands-on experience in implementation and performance tuning of Kinesis, Kafka, Spark or similar implementations
Hands on experience with building data or machine learning pipeline
Experience with one or more relevant tools (Flink, Spark, Sqoop, Flume, Kafka, Amazon Kinesis)
Experience developing software code in one or more programming languages (Java, JavaScript, Python, etc)
Current experience with hands-on implementation

Preferred Qualifications

Masters or PhD in Computer Science, Physics, Engineering or Math.
Familiar with Machine learning concepts
Hands on experience working on large-scale data science/data analytics projects
Hands-on experience with technologies such as AWS, Hadoop, Spark, Spark SQL, MLib or Storm/Samza.
Experience Implementing AWS services in a variety of distributed computing, enterprise environments.
Experience with at least one of the modern distributed Machine Learning and Deep Learning frameworks such as TensorFlow, PyTorch, MxNet Caffe, and Keras.
Experience building large-scale machine-learning infrastructure that have been successfully delivered to customers.
Experience defining system architectures and exploring technical feasibility trade-offs.
3+ years experiences developing cloud software services and an understanding of design for scalability, performance and reliability.
Ability to prototype and evaluate applications and interaction methodologies.
Experience with AWS technology stack.
Written and verbal technical communication skills with an ability to present complex technical information in a clear and concise manner to a variety of audiences.

Amazon is committed to a diverse and inclusive workplace. Amazon is an equal opportunity employer and does not discriminate on the basis of race, national origin, gender, gender identity, sexual orientation, protected veteran status, disability, age, or other legally protected status. For individuals with disabilities who would like to request an accommodation, please visit https://www.amazon.jobs/en/disability/us.

For employees based in Colorado, this position starts at $122,300 per year. A sign-on bonus and restricted stock units may be provided as part of the compensation package, in addition to a range of medical, financial, and/or other benefits, dependent on the position offered.


Company - Amazon Web Services, Inc.

Job ID: A1415316"
Massdriver,ML / Data Science Engineer,"Pasadena, CA
Remote","Posted by

Cory O'Daniel 2nd

CEO & Co-Founder, Massdriver (YC W22)

Massdiver is a visual development platform fulfilling the promise of devops by empowering software engineers to design secure, and observable infrastructure in their own cloud. We are looking for a passionate Machine Learning engineer to help build both internal tools for monitoring and cost optimization of cloud resources, and helping customers launch their own machine learning pipelines into production.

Qualifications:

- You are passionate about statistics and Machine Learning modeling
- You have a deep understanding of the different parts of the data pipeline, having been exposed to data analysis, data engineering, analytics engineering and data science
- Be extremely hands-on and proficient working with Python (pandas, numpy, sklearn, etc)
- Experienced with plotting in Python (matplotlib, etc)
- Minimum 3-5 years working professionally in the machine learning and data analytics field
- A higher education degree in computer science, mathematics, physics or data science is a major plus, but not required
- Proven experience taking machine learning algorithms from R&D to production
- Proficient with SQL and data wrangling
- Strong engineering background and demonstrated experience with building data infrastructure and real-time, distributed systems
- Experience with data frameworks such as Spark, Kafka, Kubeflow, Airflow, or Flink
- Experience with cloud service providers such as AWS, GCP or Azure
- Excellent written and oral communication skills including the capability to drive requirements with customers and engineering teams and present technical concepts and results in an audience-appropriate way

Responsibilities:

In this role you will develop and validate machine learning models, research relevant statistical methods, perform data analysis, and provide recommendations for operationalizing machine learning models into production.

- Design and build Massdriver’s ML suite for monitoring and predicting outages at scale.
- Develop infrastructure for training and serving ML models at scale across multiple regions
- Collaborate with product engineers, and platform infrastructure engineers to launch impactful new initiatives; your systems will launch models that help users keep their infrastructure running and cost effective
- Serve as a trusted advisor on the application and implementation of ML at Massdriver

Benefits:

- 100% covered medical, dental and vision
- Generous equity in an early stage company set to take off
- Flexibility. Massdriver is a remote first company and we want our employees to work the way that makes them the most productive.
- Ownership. Our engineers have felt the pain of being prevented from trying new things by the overhead of devops. We empower our engineers to solve customer problems and add value.
- Growth. As we build this core team of engineers we are looking for the future leaders of our company that will help guide us as we scale."
Material,Data Engineer,"Los Angeles Metropolitan Area
Remote","ABOUT US




We are a fast-growing market research firm with an entrepreneurial culture. We’ve spent the past 40 years using analytics and research to help businesses understand their customers, and we work across industries in more than 80 countries with some of the largest brands in the world. We value diverse perspectives and believe that different voices and viewpoints make us stronger. We’re also proud to have a helpful and supportive culture, where we take time to celebrate accomplishments both large and small. And while we’re grounded in our rich history, we never stop searching for new approaches and tools; we were named the #1 Most Innovative Insights Firm in North America by the GRIT Report in 2019.

With offices around the world, our 500+ teammates work across a dozen business units, collaborating with clients in entertainment and media, pharmaceuticals, technology, consumer packaged goods and more. Our experienced leadership team offers stability and structure, while our commitment to innovation fosters groundbreaking initiatives that help us improve our research approaches—like our Pragmatic Brain Science teams, who explore new psychological frameworks to better understand customer motivations.

ABOUT THE ROLE

We are swimming in data, coming from many sources. The IPS Quantitative Operations team requires an experienced and all-purpose data architect to architect to help us deliver actionable data insights to our many clients.




In this role, you will:

Enable efficient market research and insights on various high value projects
Work with a variety of stakeholders to design, create, and manipulate data for researchers and our clients.
Develop data integrations, while oriented toward improving data reliability, efficiency and quality
Develop processes to shape data in meaningful ways




ABOUT YOU

The ideal candidate should be curious, independent, responsive, and articulate
Someone who is receptive to instruction and feedback but can work with ill-formed problems
Is interested in the core business of the company and seeks to identify the business and use implications of various solutions
Someone who is a tenacious problem-solver who seeks to identify core bottlenecks from both a technological as well as a process-oriented standpoint.
Experience using one of the following in a market research setting: SPSS, Unicom Intelligence (Dimensions), Python or R

WE OFFER

Competitive benefits package, including medical, dental, vision, 401k matching, paid time off, paid parking, casual dress, company-wide and team events
Centrally located offices in energizing cities around the country
Performance-based culture and endless opportunities for growth
Mentorship and formal training to help you grow, including courses from LRW University"
Research Square Company,Data Engineer,"North Carolina, United States
Remote","Posted by

Keith Houchin 2nd

Manager, Talent Acquisition at Research Square Company

Send InMail

About Research Square

Research Square Company, a five-time INC 5000 award winner, exists to make research communication faster, fairer, and more useful. Through our industry-leading preprint platform, Research Square, research promotion tools, and AJE’s comprehensive suite of manuscript preparation services, we are proud to have supported over 2.5 million authors in 192 countries since our founding in 2004. Across all sides of our business, our team of former researchers and publishing industry professionals truly understand the importance of sharing research results with the world. By helping researchers communicate their work more effectively, we accelerate the pace of global discovery and advancement.




Job Summary

The Data Engineer will play a key role in developing & supporting the company’s data & analytics capabilities. The successful candidate will have multifaceted skills & experience and have a strong desire to play an integral part in data architecture development and administration. This person will work closely with all parts of the IT team to advance strategic initiatives and support capabilities and adoption across the company. Our optimal candidate exhibits personal humility and strives to enable the success of their team in our collaborative and fast-moving work environment.




Essential Functions

Identify, evaluate, and recommend software technologies to achieve outstanding data warehouse performance and ETL functionality
Develop and maintain ETL pipelines and software tools to efficiently pre-process, modify, integrate, and archive large data collections
Work closely with data scientists, analysts, and engineers to ensure data quality and availability for reporting, analytical modeling, prototyping, and applications
Prepare data and build data pipelines for Machine Learning/AI use cases (e.g., text corpora for NLP modeling)
Implement solutions for data security, quality, and automation of processes
Create and maintain clear and well-organized documentation for database schemas and ETL pipelines
Any other duties as required




Education

Bachelor’s or master’s degree in Computer Science, Computer Engineer, or a technology-related field




Minimum Qualifications:

5+ years of experience as a SQL Developer, DBA, and/or Architect
Demonstrated ability to understand and articulate complex requirements for technical and non-technical colleagues
Experience designing, maintaining, and troubleshooting data warehouses and ‘big data’ ETL pipelines with responsibility for regular maintenance, bug fixes, and performance analysis required
Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement
Strong analytical skills related to working with unstructured datasets required
Strong knowledge of SQL with experience writing and optimizing queries against complex data models required
Proficiency with Python required; experience with other scripting languages a plus (e.g., R, Scala, Julia)
Experience with AWS services such as EC2, S3, RDS, Lambda, Glue, Athena, and/or Redshift required
Experience with UNIX/Linux including basic commands and shell scripting required
Familiarity with source control (Git) and Docker is a plus
Experience with GCP services such as Compute Engine, Cloud Storage, Cloud SQL, Bigtable, DataProc, and/or Dataflow is a plus
Experience using Big Data platforms (e.g., Hadoop, Spark, HBase, CouchDB, Hive, etc.) is a plus
Ability to work independently and in a remote team environment
Must be a creative and analytical thinker with strong problem-solving skills, able to go beyond current tools to deliver the best solutions to complex problems
Must be a team player (willing to set aside personal interests for the good of the team)
Must be goal-driven and a self-starter (can be given an objective and proactively identifies & executes the tasks required to achieve the objective)




Work Environment

Relocation is not required as this position can be remote-based.
This role can be based anywhere in the US.




Applicants must be currently authorized to work in the United States for any employer.




Working at Research Square Company

Our team embraces and fuels change, fights for simplicity invest in customers’ success, and applies a data-driven approach to continuously improve and magnify our impact. We have developed tools and services that have been adopted by major international publishers to improve the publishing experience for their authors.




We are a high-growth, family-friendly, and mission-driven company that regularly wins awards for our workplace culture, pace of growth, and innovations. Our organization is casual and flexible while also being stimulating and dynamic. We have a results-focused work environment.




Workplace Recognition

Sloan Award for Workplace Flexibility (2011, 2012)
When Work Works Award (2014, 2016, 2017)
NC Parenting Magazine’s Family Friendly 50 (2013, 2014)
Triangle Business Journal's Best Places to Work (2017, 2019, 2020)
NCBC Breastfeeding-Friendly Employer Award (2017)
Family Forward NC Featured Business (2019)




Research Square Company’s policy is to provide equal employment opportunity in all its employment practices without regard to race, color, religion, sex, national origin, citizenship, ancestry, marital status, protected veteran status, military status, age, individuals with disabilities, sexual orientation, or gender identity or expression or any other legally protected category. Applicants for US-based positions with Research Square must be legally authorized to work in the United States. Verification of employment eligibility will be required as a condition of hire.

Research Square supports individuals with disabilities and provides reasonable accommodations to job applicants. If you need assistance completing our online job application, email Recruitment@researchsquare.com. General inquiries, such as those regarding the status of a job application, will not receive a reply."
Apple,Data Science Engineer - Strategic Data Solutions,"Austin, TX
On-site","
Summary

Are you ready to apply your educational experience to real-world problems? Are you passionate about applying your data skills in a real-world tech environment?


Imagine what you could do here. At Apple, new ideas have a way of becoming extraordinary products, services, and customer experiences very quickly. Bring passion and dedication to your job and there's no telling what you could accomplish. Join Apple, and help us leave the world better than we found it. Apple’s Strategic Data Services (SDS) team is responsible for mitigating fraud, waste and abuse company-wide while optimizing and empowering our customers and internal partners.


SDS Data Science Engineering is building an environment to enable ground breaking data analysis over Petabytes of data. We work side-by-side with data scientists and implement scalable, easy-to-use systems and tools. We are seeking a customer-focused, passionate and driven Data Science Engineer with experience in building analytic tools and solutions.


This position is based in Austin, TX.


Key Qualifications

Mastery of one of Python, Java, Scala, C++ or equivalent language

3+ years of experience in software engineering/data science

Experience building data science or data analysis tools on Hadoop/cloud based systems

Some experience with Docker, Kubernetes, or cloud platform deployment

Experience with Relational databases and NoSQL databases

Demonstrated understanding of the full software development lifecycle

Excellent problem solving, critical thinking, and communication skills

Solid grasp of computer science fundamentals including data structures and algorithms

Solid ability to evaluate and apply new technologies in a short time

Self-motivated, proactive, and solution-oriented


Description

With the expansive data we have, our job is to build meaningful data relationships and engagement experiences for our internal customers. If you’re interested in being a part of a team that’s constantly learning and problem-solving, we’d love to talk with you. As a Data Science Engineer on the SDS team, you will work closely with Data Scientists and other Data Science Engineers to lead the design and implementation of systems and tools to support the fraud prevention efforts of SDS.


You will be:

• Developing and implementing production software for preventing fraud

• Responsible for system architecture design

• Working with external infrastructure teams to drive the development of infrastructure needs

• Innovating by recognizing opportunities for automation and tools improvements

• Responsible for developing and implementing process improvements to bring efficiency and stability to fraud analytics

• Responsible for technical leadership for a team of data scientists

• Lead the team to increase the level of maturity and skill in analytical software development

• Responsible for release engineering


Education & Experience

• BS or advanced degree in Computer Science, related field or equivalent experience


Role Number: 200353585

"
Amazon Web Services (AWS),"Data Engineer, AWS Econ Data","Seattle, WA","Job Summary

DESCRIPTION

AWS is looking for a Data Engineer to join the AWS Econ Data team, to be responsible for building and maintaining end to end data solutions enabling the AWS central economics team (ACE). The ACE team is the largest economics team in AWS tackling the most critical AWS decisions with econometrics methods and machine learning. The outputs directly influence the CEO, CFO and SVPs of AWS.

Econ Data team provides the data stewardship of the ACE. We are the experts of the enormous and complex AWS data. The team deeply understands the research and modeling needs and consolidates data from dozens of sources into highly curated data products. The team also owns the data infrastructure customized for modeling along with the research environment empowering all economists and scientists.

Key job responsibilities

As a data engineer of the team, you will own the most complex and strategically critical data models consolidating terabytes of data across AWS. You need to learn a wide array of technologies used in different data sources and develop scalable solutions to interact with these sources. Leveraging the AWS data technology and tools developed internally, you will design, implement and maintain a data infrastructure customized for research and ML models. You will also contribute to the research environment development enabling the scientists on the team.

About The Team

Inclusive Team Culture

Here at AWS, we embrace our differences. We are committed to furthering our culture of inclusion. We have ten employee-led affinity groups, reaching 40,000 employees in over 190 chapters globally. We have innovative benefit offerings, and host annual and ongoing learning experiences, including our Conversations on Race and Ethnicity (CORE) and AmazeCon (gender diversity) conferences. Amazon’s culture of inclusion is reinforced within our 16 Leadership Principles, which remind team members to seek diverse perspectives, learn and be curious, and earn trust.

Work/Life Balance

Our team puts a high value on work-life balance. It isn’t about how many hours you spend at home or at work; it’s about the flow you establish that brings energy to both parts of your life. We believe striking the right balance between your personal and professional life is critical to life-long happiness and fulfillment. We offer flexibility in working hours and encourage you to find your own balance between your work and personal lives.

Mentorship & Career Growth

Our team is dedicated to supporting new members. We have a broad mix of experience levels and tenures, and we’re building an environment that celebrates knowledge sharing and mentorship. We care about your career growth and strive to assign projects based on what will help each team member develop into a better-rounded professional and enable them to take on more complex tasks in the future.


Basic Qualifications

Bachelor’s degree in software engineering or a relevant quantitative discipline
5+ years of industry experience in software development, data engineering, business intelligence, data science, or related field
3+ years of experience developing and operating large-scale data structures for analytics and modeling: ETL/ELT processes; OLAP technologies; data modeling; SQL
2+ years of experiences using at least one massively parallel processing data technology such as Redshift (preferred), Teradata, Netezza, Spark or Hadoop based big data solution
Coding proficiency in at least one modern programming language (Python, Ruby, Java, etc)

Preferred Qualifications

Master`s degree in software engineering or a relevant quantitative discipline
2+ years of experiences working with core AWS data and analytics services. Understand of the applicability, limitations, and tradeoffs between a wide set of AWS database and analytics technologies.
5+ years of experience developing and operating large-scale data structures for analytics and modeling: ETL/ELT processes; OLAP technologies; data modeling; SQL
2+ years of experiences with research and modeling projects
Experience with R, Python, Weka, SAS, Matlab or other statistical/machine learning software.
Working knowledge of software development methodologies like Agile
Excellent oral and written communication skills including the ability to communicate effectively with both technical and non-technical stakeholders.

Amazon is committed to a diverse and inclusive workplace. Amazon is an equal opportunity employer and does not discriminate on the basis of race, national origin, gender, gender identity, sexual orientation, protected veteran status, disability, age, or other legally protected status. For individuals with disabilities who would like to request an accommodation, please visit https://www.amazon.jobs/en/disability/us.


Company - Amazon Dev Center U.S., Inc.

Job ID: A1981936"
Amazon Web Services (AWS),Data Engineer,"Seattle, WA","Job Summary

DESCRIPTION

As a Data Engineer you will enable data-driven decision making within the Amazon Web Services Data Center Infrastructure Operations organization. The Infrastructure Operations Team is responsible for planning, implementing, monitoring and continuously improving the global Amazon Data Center infrastructure. The team supports all aspects of the Data Center based organizations, including but not limited to : Safety, Security, maintenance, daily operations, logistics, engineering and equipment management.

You will be developing, implementing and maintaining the information data lake and utilizing insight platforms to enable decision support systems for the overall organization. You should have excellent business and communication skills, and be able to work with business owners to understand their data and reporting requirements.

Above all, you should be passionate about working with huge data sets and be someone who is able to bring data sets together to answer business questions and drive growth. You will build ETLs to ingest the data into the data warehouse and data lake, as well as end-user facing reporting applications. You will primarily support teams within the Infrastructure environment, but will also have opportunities to support teams in the overall Amazon Web Services community.

You will work with business customers and development teams to define analytics requirements and then deliver flexible, scalable, end-to-end solutions.

You will have an opportunity to work with big data and emerging technologies while driving business intelligence solutions end-to-end: business requirements, data modeling, ETL, metadata, reporting, and dashboarding. You should have expertise in the design, creation, management, and business use of large datasets.


Basic Qualifications

Bachelor’s Degree in Computer Science, Information Systems, Mathematics, Statistics, or related field
7+ years of experience in Data engineering
5+ years of experience with Data modeling, SQL, ETL , Data Warehousing and Datalakes
5+ years experience in writing SQL scripts Expert knowledge in an enterprise class RDBMS
Experience with scripting language such as Python, Perl, Ruby or Javascript
Excel in the design, creation, and management of very large datasets

Preferred Qualifications

Ability to balance and prioritize multiple conflicting requirements with high attention to detail.
Excellent verbal/written communication & data presentation skills, including ability to succinctly summarize key findings and effectively communicate with both business and technical teams.
Comfortable working in a Linux environment
Experience with MPP databases such as Redshift
Knowledge of AWS products and services
Exposure to predictive/advanced analytics and tools (such as R, SAS, Matlab)
Experience with Datalake development
Exposure to noSQL databases (such as DynamoDB, MongoDB)
Meets/exceeds Amazon’s leadership principles requirements for this role
Meets/exceeds Amazon’s functional/technical depth and complexity for this role

Amazon is committed to a diverse and inclusive workplace. Amazon is an equal opportunity employer and does not discriminate on the basis of race, national origin, gender, gender identity, sexual orientation, protected veteran status, disability, age, or other legally protected status. For individuals with disabilities who would like to request an accommodation, please visit https://www.amazon.jobs/en/disability/us.


Company - Amazon Data Services, Inc.

Job ID: A1979937"
NCSOFT,Data Engineer,"Newport Beach, CA
Hybrid","Posted by

Marta Willner 2nd

Senior Manager, Recruiting at NCSOFT

WHO WE ARE

NCSOFT is a premiere digital entertainment company and global publisher with worldwide locations and more than 4,000 employees focused on bringing extraordinary games to life for millions of fans around the world. Established in 1997 and headquartered in Seoul, South Korea, we quickly became a key leader in online games. Best known for critically acclaimed franchises including Lineage, Aion, Guild Wars, and Blade & Soul, NCSOFT is also one of the world’s top mobile developers with Lineage 2M occupying the #1 grossing revenue slot on Google Play. Our core goal is making people in this world happier by delivering games that entertain a globally connected audience has remained the same. Our culture is innovative, creative, collaborative and impactful, and we are passionate about creating the best gaming experiences for our players.




We are looking for a Data Engineer to gather and collect data, store it, doing batch processing or real-time processing on it, and serve it to data scientists and data analysts to easily query.




WHY JOIN THE BUSINESS INTELLIGENCE PLATFORM TEAM?

We are a passionate and positive team that is always working on new data engineering technology and exciting projects including new game launches.
We feel team members are most effective in working on what they are passionate about. We are all capable of working on any given project and often shift tasks based on preference.
We own the data platform, contributing to the design, implementation, and maintenance of these data services so we aren’t shy about experimenting with new tech. However, we take a measured approach in order to minimize introducing unnecessary technical debt.
We collaborate with Analytics, Game Development, Platform, and Publishing teams. Our data platform supports all kinds of data in an efficient environment that helps with analysis, customer care and company-wide data-driven decisions.




WHAT YOU'LL DO

Develop data processes for construction, mining, and modeling that are delivered to the technical and non-technical stakeholders
Create large data warehouses by running some ETL (Extract, Transform and Load) that is used for analysis by technical and non-technical stakeholders
Install continuous pipelines of huge pools of filtered information so that technical and non-technical stakeholders can pull relevant data sets for their analyses
Apply software engineering best practices to analytics processes, automation, and establish best practices for data modeling/processing




WHAT YOU’LL NEED TO BE SUCCESSFUL

BS in Computer Science or equivalent experience
3+ years of day-to-day working experience as data engineer or ETL developer
Advanced SQL skills with with data warehouses (RedShift/Snowflake)
Experience with data modeling and ETL/ELT (Airflow, Informatica)
Experience applying software engineering principles (testing, version control, code reviews, etc.) to scale analytical development
Strong communication with technical and non technical audiences"
IDEXX,Principal Data Engineer,"Maine, United States
Remote","Posted by

Elena Sudakova

Talent Acquisition, DEI, HR Policy, Global Mobility | she / her / hers

Send InMail

The Artificial Intelligence team in R&D is seeking a Principal Data Engineer to design and manage data pipelines and machine learning operations infrastructure and applications in support of high impact data science initiatives. You will join a team of talented data scientists, analysts, and data engineers driving cutting edge AI innovation in veterinary care. This is an exciting role that will interact with many of IDEXX’s core data streams, including new streams emerging from R&D innovation pipelines. If you enjoy complex data engineering challenges, consider this role!




In this role:

You will lead the design of data pipelines and machine learning operations workflows across diverse data sources.
You will collaborate closely with data scientists, analysts, data engineering teams, and other data consumers to define requirements and develop solutions.
You will work with modern machine learning and DevOps stacks, including H20, Kubernetes, AWS, and Spark.
You will keep up-to-date on the latest data engineering best practices and innovations.




What you need to succeed:

Experience with/in:

managing and maintaining all stages of the data pipeline lifecycle
building continuous integration and deployment (CI/CD) systems
building and deploying web applications and APIs
ML(Ops) offering of at least one major cloud provider (e.g. Sagemaker, Vertex)
Docker/containers and Kubernetes, Python and Pandas, R, RStudio, RStudio Connect, SQL, NoSQL databases and infrastructure-as-code tools (e.g. Terraform)

Proficiency with/in:

the design of data warehouse, data lake, and data lakehouse architectures
Git
big data platforms such as Hadoop and Spark
one or more major cloud providers (e.g. AWS)
stream-processing systems (e.g. Apache Kafka)

Familiarity with domain-driven design (DDD) and stream-processing systems (e.g. Apache Kafka) is a plus.




Why IDEXX

We’re proud of the work we do, because our work matters. An innovation leader in every industry we serve, we follow our Purpose and Guiding Principles to help pet owners worldwide keep their companion animals healthy and happy, to ensure safe drinking water for billions, and to help farmers protect livestock and poultry from diseases. We have customers in over 175 countries and a global workforce of over 9,000 talented people.




So, what does that mean for you? We enrich the livelihoods of our employees with a positive and respectful work culture that embraces challenges and encourages learning and discovery. At IDEXX, you will be supported by competitive compensation, incentives, and benefits while enjoying purposeful work that drives improvement.




Let’s pursue what matters together.




IDEXX values a diverse workforce and workplace and strongly encourages women, people of color, LGBTQ individuals, people with disabilities, members of ethnic minorities, foreign-born residents, and veterans to apply.

IDEXX is an equal opportunity employer. Applicants will not be discriminated against because of race, color, creed, sex, sexual orientation, gender identity or expression, age, religion, national origin, citizenship status, disability, ancestry, marital status, veteran status, medical condition, or any protected category prohibited by local, state, or federal laws."
Insight Global,Staff Data Engineer,"Santa Monica, CA","A top gaming publishing company is looking to hire multiple Data Engineers to one of their newly built teams. This team supports an advertisement revenue platform for one of Activisions biggest games ""Call of Duty"". This candidate will partner with Senior Engineers and Product Owners to build scalable data pipelines and services. They will work closely with the product team to understand business requirements and translate them into technical tasks.

Experience working with AWS or Google Cloud Platform (GCP)
Understanding of machine learning concepts
Experience in building data warehouse and data lake.
Knowledge of advertising platform.
Experience in working with machine Learning libraries
BA/BS in Computer Science or similar/relevant Degree
5-7 + years of hands on experience in software Design and Development
Advanced experience in Java Development
2 + years of experience working with relational databases such as (MySQL OR PostgreSQL OR MariaDB OR SQLite)
2+ years of experience in NOSQL databases (Bigtable OR Cassandra HBase)
Experience with Data Modeling
Exp with Data Processing
Experience developing ETL
Experience with distributed messaging systems like (Kafka and Rabbit)
Experience with distributed computing framework like (Apache Spark OR Flink)"
New York Mets,Lead Data Engineer,"New York City Metropolitan Area
Hybrid","Summary:

The New York Mets are seeking a Lead Data Engineer on the Data Engineering Technology team. This role requires hands on experience in ingesting, processing, warehousing, and distributing both structured and unstructured sources of baseball data. You will also work with your manager in actively supporting users and allocating/managing workload for the team. You will join a global team (NY and Poland) of data engineering/support professionals to provide best in class end to end data and engineering support to Baseball Operations, including but not limited to, Analytics and Systems teams. Prior experience in or knowledge of baseball is a plus but is not required.




Essential Duties & Responsibilities:

Ensure that all production data sources are ingested, processed, and distributed to applications and users at set frequencies including real time in game data
Onboard new datasets and technologies for trials
Troubleshoot and communicate any data ingestion and/or quality issues to all stakeholders
Design, code, test and roll out of new datasets, as well as enhancements and bug fixes for existing datasets
Support quantitative analysts in Baseball Analytics with productionization and maintenance of predictive models
Build and manage Data Model and Data Domains to keep data clean, accurate and well organized
Deploy and manage data quality solutions (e.g., scalable check framework, monitoring dashboard) to ensure accuracy, integrity, and proactive monitoring of information
Help maintain a data catalog for efficient data discovery
Help manage a global team of Data Engineering/Support professionals on data engineering and support requests/issues
Able to work flexible schedule during baseball season to ensure in game support for users
Coordinate with IT and Infrastructure team to ensure a robust cloud development and production environment




Qualifications:

BS degree in Computer Science or a related field
5+ years’ experience in data engineering and data operations/support roles
Prior experience in managing a team preferred
Experienced in Google Cloud Platform (GCP)
Technical skills (Python, SQL, Linux) in working with ingesting, processing, and distributing large scale (both structured and unstructured) data sets
Experience building data structures and data pipelines in GCP using Dataflow
Experience in using modern Software Development Life Cycle (SDLC) and DevOps tools from development to production e.g., Terraform, Cloud Build, BitBucket
Ability to provide prompt support and resolution of data issues
Experience in implementing scalable data quality solutions
Knowledge of data engineering frameworks - Cloud Composer/Airflow, Dataflow, Pub/Sub, etc
Ability to deliver superior customer experience through continuous process improvement
Strong analytical skills and ability to work well in a collaborative and fast paced environment




The above information is intended to describe the general nature, type, and level of work to be performed. The information is not intended to be an exhaustive or complete list of all responsibilities, duties, and skills required for this position. Nothing in this job description restricts management’s right to assign or reassign duties and responsibilities to this job at any time. The individual selected may perform other related duties as assigned or requested.




The New York Mets recognize the importance of a diverse workforce and value the unique qualities individuals of various backgrounds and experiences can offer to the Organization. Our continued success depends heavily on the quality of our workforce. The Organization is committed to providing employees with the opportunity to develop to their fullest potential."
Ingram Micro,Associate Data Engineer,"Irvine, CA
Hybrid","Posted by

Jeannie Jones 2nd

Executive Search - Passionate about Recruiting the TOP Executive and Technical Professionals for World Class Teams.

Send InMail
Description

Ingram Micro is an integral part of the technology and commerce ecosystems, helping our partners grow and thrive through the creation and delivery of Information Technology, Lifecycle Management, e-Commerce Logistics, and Cloud solutions. With $49 billion in revenue and the ability to reach 90% of the global population, we have become the world’s largest technology distributor with operations in 59 countries and more than 35,000 associates.

This position is in the Irvine, California corporate headquarters and has the opportunity for hybrid work with up to two days remote per week.

The Data Engineering team is a part of the Global IT Organization and is responsible for managing data driven solutions and applications. The team is primarily responsible for supporting the Global Business Intelligence data warehouse and providing ETL and Infrastructure support for the applications and processes that improve profitability, margin, and revenue through advanced analytics.

We are looking for a talented and motivated SQL/SSIS Developer whose primary function will be to design, develop, implement, and maintain SQL database programming components of the Global Data Warehouse. Main components are the SSIS packages, procedures, SQL Agent and Control-m scheduled jobs. The successful candidate will have strong verbal and written communications skills, demonstrate strong analytical thinking, thrive in uncertain environment, and proficient with SQL and SSIS. Additionally, the ability to understand and translate business requirements into successful delivery of projects is required.

Your Role

Design, develop, deploy, maintain Microsoft SQL SSIS packages and procedures.
Operational support for the Global BI Data Warehouse ETL processes including on-call support every 5th week.
Design, develop, deploy, maintain operational reports.
Support and create ETL process using data integration points such as ERP, SAP, and CRM applications.
Provide application support for Global BI dependent applications.
Partner with data science and analytical teams to provide documentation and assistance in sourcing data within the Data Warehouse.

What You Bring To The Role

Bachelor’s Degree in Computer Science, Engineering, Science and Math or related discipline required, or an equivalent combination of education and experience.
Design and development with knowledge of DW architecture, data modeling, data normalization and ETL processes.
SQL knowledge and experience having worked with relational databases, managing medium-large data sets, query authoring, and complex stored procedures.
Proficiency interacting with database and file storage systems like SQL, Oracle, and Redshift.
Ability to work closely with advanced level business team to understand requirements and deliver high quality output.
Competence and comfort with Transact-SQL (T-SQL) and Microsoft Database Administration.
Competence in Microsoft Schema management (DDL).
Competence in design, develop, deploy, maintain operational reports.
Patience and perseverance to overcome challenges.
Gather requirements accurately and prioritize tasks and document development details.
Knowledge and hands-on experience working with Microsoft SQL and SSIS.
Understanding of relational databases.

Knowledge That’s a Plus

Performance tuning for complex SQL queries
Python programming
Experience working with AWS or Google Cloud

This is not a complete listing of the job duties. It’s a representation of the things you will be doing, and you may not perform all these duties.

Please be prepared to pass a drug test and successfully pass a pre-employment (post offer) background check that includes verification of vaccination status.

Ingram Micro requires all new associates to be fully vaccinated against COVID-19. Therefore, this position requires applicants to submit proof, prior to start date, that the successful applicant is fully vaccinated against COVID-19. Ingram Micro will comply with applicable laws regarding the reasonable accommodation of individuals with disabilities and/or sincerely held religious beliefs. Applicants will be notified of the requirements of Ingram Micro’s COVID-19 policy and process for verification of vaccination status prior to the start of employment.

Ingram Micro believes there is no place in our society for social injustice, discrimination, or racism. As a company we do not – and will not – tolerate these actions.

Ingram Micro Inc. is committed to creating a diverse environment and is proud to be an equal opportunity employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, gender, gender identity or expression, sexual orientation, national origin, genetics, disability, age, veteran status, or any other protected category under applicable law.

"
Ruggable,Data Engineer,"Los Angeles, CA","About Ruggable:

Ruggable is a Los Angeles-based, venture-backed, rapidly growing e-commerce startup that is disrupting the $7B rug industry and revolutionizing the market for residential rugs. Our patented 2-Piece Rug System allows you to remove the top layer of your rug and wash it in your home washing machine, offering an affordable, stylish and convenient solution for young families, pet owners, and busy individuals. We're a small, entrepreneurial team with big ambitions. We have a strong brand, established traction, and amazing advisors fueling our steady growth. This is an opportunity to get in at the ground floor and solve for complex challenges, while charting your own career.

Job Summary:

As a Data Engineer you will be a key part of a team whose goal is to build and maintain foundational data infrastructure essential to driving Ruggable’s revenue growth and accelerating user acquisition. You will work closely with technology leadership, data engineers, data analysts, data scientists, and engineering teams to build best-in-class data pipelines and processes that stitch together complex sets of data stores and drive actionable insights.

What You’ll Do: 

 Bridge gap between business requirements and ETL logic by troubleshooting data discrepancies and implementing scalable solutions 
 Design, build and maintain critical data pipelines to ensure highly accurate and reliable business reporting 
 Successful track record of monitoring daily execution, diagnosing and logging issues, and fixing business critical pipelines to ensure SLAs are met with internal stakeholders 
 Make data model and ETL code improvements to automate pipeline efficiency and data quality 
 Own data import/export pipelines and incorporate into existing workflows to enable reporting and optimization efforts 
 Contribute to marketing data insights strategy and development 


What You’ll Need to Have:

Required: 

 3+ years experience in a data engineering / BI role supporting Marketing or Sales organizations (ecommerce, digital, tech is a plus) 
 2+ years experience with Python, SQL, UNIX/Linux shell scripting 
 Experience working with cloud based data warehouse (Amazon Redshift, Big Query) 
 Experience with cloud (AWS) data pipelines 
 Experience working with Agile/Kanban methodologies 
 Understanding of structured, semi-structured and unstructured file formats and the ability to parse them with Python (preferred) or other programming language 
 Strong attention to detail 
 Strong business intuition and ability to understand complex business systems, data architecture, and software design patterns 
 Excellent communication skills, particularly when explaining technical matters to less technical co-workers 


Preferred: 

 Experience with DBT 
 Experience with Confluence & Jira 
 Experience with Big Data platforms and architecture (S3/Data Lake, Hive, Spark, AWS Athena) 
 Experience with ETL scheduling technologies with dependency checking, such as Airflow 
 Experience with Kafka, AWS Kinesis, Spark-Streaming 
 Experience with Machine Learning (ML) and productionalizing of ML models 


At Ruggable, we offer competitive compensation and benefits packages. Ruggable is an Equal Employment Opportunity employer. We proudly recruit and hire a diverse workforce and are committed to creating an inclusive environment for all employees.

If you are based in California, we encourage you to read this important information for California residents linked here.

To all recruitment agencies: Ruggable does not accept unsolicited agency resumes. Please do not forward resumes to our jobs alias, Ruggable employees or any other company destination. Ruggable is not responsible for any fees related to unsolicited resumes."
Cisco,Data Engineer,"United States
Remote","What You Are

You are a business and strategic thinker who can easily marry business concepts to data and technical execution. You are unafraid and welcome complex problems and the ability to showcase your solution to senior leadership. You are passionate about data and analytics and strive to be data driven in all aspects of what you do. The ideal candidate will have extensive experience in data engineering and analysis, data wrangling, and helping translate business requirements into a data execution strategy.

Who You’ll Work With

The Data Engineer will be an integral role within the Sales & Marketing Analytics team. Our team’s mission is to deliver a singular and unified analytics experience, providing visibility and insight into our Sales & Marketing teams. Ultimately, this team will be integral in helping identify, position, and sell more Cisco products and services to our customers aiding in our shift to recurring revenue. This role provides the opportunity to work with senior leadership on a highly visible and strategic programs. The role will also provide exposure to other stakeholder teams such as Sales, Marketing, Ops, and IT. This role will execute in a matrixed environment and will partner closely with other business leaders, product owners, analysts, and process architects.

What You’ll Do

In the role of a Data Engineer, you will be focusing on building new data pipelines for acquiring, modeling, and integrating enterprise data with the SMX Analytics data foundation. You will

Design, solution, and build data products to be consumed by front end architectures such as Tableau.
Use a variety of tools to extract, transform and analyze enterprise data sets
Report your findings with data visualizations that are easy to understand
Communicate insights and provide solutions that have proven results.
Have a toolkit of technical skills enabling data exploration, dynamic analysis, and sophisticated analysis
Participate in agile ceremonies and support the iterative development of Analytics products
Participate in grooming/prep discussions to provide input on time estimates for backlogged features.
Work with POs and BAs to assess cross-functional dependencies on dataset that need to be acquired.
Develop and maintain internal product documentation. This may include but not limited data pipeline documentation and ERDs
Oversee data quality including validation automation, cleansing/scrubbing data


We are seeking

7+ years in a data analyst/engineer role or similar role with technical critical thinking capabilities
Experience in Cisco and Sales/Marketing highly preferred as ideal candidate will have working knowledge of the Cisco’s Enterprise Data Foundation and relevant data architectures supporting this domain.
Bachelor's Degree in CS, Engineering, a related field or equivalent work experience
Deep technical, analytical, interpersonal, communication and writing skills.
Working experience with On Prem (Teradata, Oracle, HANA) and Cloud DBs (Snowflake, AWS, etc.)
Data Visualization Tools Tableau (ideal), PowerBI


Why Cisco

#WeAreCisco, where each person is unique, but we bring our talents to work as a team and make a difference. Here’s how we do it.

We embrace digital, and help our customers implement change in their digital businesses. Some may think we’re “old” (30 years strong) and only about hardware, but we’re also a software company. And a security company. A blockchain company. An AI/Machine Learning company. We even invented an intuitive network that adapts, predicts, learns and protects. No other company can do what we do – you can’t put us in a box.

But “Digital Transformation” is an empty buzz phrase without a culture that allows for innovation, creativity, and yes, even failure (if you learn from it.)

Day to day, we focus on the give and take. We give our best and give our egos a break and we give of ourselves (because giving back is built into our DNA.) We take accountability, we take bold steps, and we take difference to heart! Because without diversity of thought and a commitment to equality for all, there is no moving forward."
Amazon Web Services (AWS),"Infrastructure Data Engineer, Datacenter Security","Seattle, WA","Job Summary

DESCRIPTION

Job Summary

Come join the Amazon Web Services (AWS) Data Center Infrastructure Security Team in our mission to change the way we manage data center enterprise asset security and process excellence on behalf of our worldwide customers.

If you are passionate about driving meaningful change and operational performance with data and insights, this is the right role for you. Your work will directly enable leaders across software, hardware and infrastructure operations to ensure all networking and server components are managed in a responsible, secure and repeatable way. You will be driving business intelligence solutions end-to-end: business requirements, data modeling, ETL, metadata, reporting, and dashboard development. Your self-service analytics tools will be leveraged across our entire global footprint. You will work closely with business planning and finance teams to achieve business objectives and communicate insights to key stakeholders. You will implement best practices in reporting and analysis, including data integrity, visualization design, and documentation; and drive results in a fast-paced, ambiguous environment.

Key job responsibilities

Work directly with customers to understand the business problems at hand and propose impactful analyses and solutions. Ensure customers’ problems are solved and sustained.
Design, develop and manage scalable solutions for new and existing metrics, reports and dashboards to inform business decisions.
Write high quality SQL to retrieve and analyze data from relational databases (ex. Redshift, MySQL)
Build ETLs to ingest the data into the data warehouse and datalake, as well as end-user facing reporting applications.
Create user friendly repeatable reporting solutions to minimize ad hoc customer requests, which creates team thrash and a poor customer experience.
Interface with leadership and management to drive performance behaviors
Invent new ways to define and measure program effectiveness and organizational productivity.


Basic Qualifications

Bachelor's Degree in Mathematics/Business Analytics/Statistics/Computer Science or related discipline
3+ years of relevant experience in one of the following areas: Business intelligence, data engineering, database engineering, or business analytics.
3+ years experience with data modeling, SQL, ETL, Data Warehousing and Datalakes
3+ years experience with data visualization tools such as Tableau, Power BI, QlikView, Quicksight, etc.
3+ years of experience writing complex, optimized SQL queries.

Preferred Qualifications

Master's degree in Mathematics/Business Analytics/Statistics/Computer Science or related discipline
Experience in an analytical roles in an e-commerce, technology company, investment banking, or product management
Understanding of A/B testing methodology, advanced statistical/predictive modeling, and optimization techniques
Experience building self-service, interactive, scalable data solutions
Experience in building large scale distributed data processing pipelines
Experience designing and building new reporting from the ground up
Ability to explore new datasets and find potential problems
Experience with enterprise-class Business Intelligence tools such as Microstrategy, PowerBI, Tableau, Oracle BI, Pentaho, AWS Quicksight, etc.
Experience with data presentation skills to summarize key findings and communicate with both business and technical teams.

Amazon is committed to a diverse and inclusive workplace. Amazon is an equal opportunity employer and does not discriminate on the basis of race, national origin, gender, gender identity, sexual orientation, protected veteran status, disability, age, or other legally protected status. For individuals with disabilities who would like to request an accommodation, please visit https://www.amazon.jobs/en/disability/us.


Company - Amazon Data Services, Inc.

Job ID: A1977730"
Motion Recruitment,"Data Engineer / $210,000 per year","Los Angeles, CA","They were recently voted as one of Inc Magazine's Best Workplaces in 2021 and 2020. They are on a mission to democratize TV advertising and to help businesses and brands, such as Calm, Daily Harvest, and Rothy’s, to grow their business through broadcast and streaming TV. Data and analytics company focused on buying and measuring ads across TV and streaming platforms. They have built their own proprietary tools that allow businesses to buy, traffic, measure, and scale TV campaigns real time.

Their team includes founders and leaders from Google, Microsoft, Stripe, Shazam and Facebook. They are building out their product, engineering, and data science teams rapidly as they accelerate our mission to automate the complex landscape of managing and measuring television advertising. Their long-term goal is to make marketing on TV available to businesses of any size.

They are currently looking for an experienced Data Engineer who will design, build, and deploy data pipelines and analytics infrastructure to enable our customers to measure and optimize TV advertising campaigns. This will involve cross-functional collaboration with data scientists, business analysts, and product managers to understand the company’s data-related needs and to formulate appropriate solutions.

Requirements

3+ Senior Data Engineer
AWS (redshift – warehouse, S3 – data lakes, Lambda – ETL)
Just moved to Airflow and early days of Spark
Needs to be proficient with Python
Working along the DS team so plus is ML tools, data bricks experience
Large data but not huge amounts
3+ Senior Backend Engineer – Python

The Offer

$210,000 + equity

You Will Receive The Following Benefits

Medical Insurance & Health Savings Account (HSA)
401(k)
Paid Sick Time Leave
Pre-tax Commuter Benefit
10% bonus
Stock Options

Applicants must be currently authorized to work in the United States on a full-time basis now and in the future.

Posted By: Lauren Hassouneh"
Amazon Web Services (AWS),"Infrastructure Data Engineer, Datacenter Security","Herndon, VA","Job Summary

DESCRIPTION

Job Summary

Come join the Amazon Web Services (AWS) Data Center Infrastructure Security Team in our mission to change the way we manage data center enterprise asset security and process excellence on behalf of our worldwide customers.

If you are passionate about driving meaningful change and operational performance with data and insights, this is the right role for you. Your work will directly enable leaders across software, hardware and infrastructure operations to ensure all networking and server components are managed in a responsible, secure and repeatable way. You will be driving business intelligence solutions end-to-end: business requirements, data modeling, ETL, metadata, reporting, and dashboard development. Your self-service analytics tools will be leveraged across our entire global footprint. You will work closely with business planning and finance teams to achieve business objectives and communicate insights to key stakeholders. You will implement best practices in reporting and analysis, including data integrity, visualization design, and documentation; and drive results in a fast-paced, ambiguous environment.

Key job responsibilities

Work directly with customers to understand the business problems at hand and propose impactful analyses and solutions. Ensure customers’ problems are solved and sustained.
Design, develop and manage scalable solutions for new and existing metrics, reports and dashboards to inform business decisions.
Write high quality SQL to retrieve and analyze data from relational databases (ex. Redshift, MySQL)
Build ETLs to ingest the data into the data warehouse and datalake, as well as end-user facing reporting applications.
Create user friendly repeatable reporting solutions to minimize ad hoc customer requests, which creates team thrash and a poor customer experience.
Interface with leadership and management to drive performance behaviors
Invent new ways to define and measure program effectiveness and organizational productivity.


Basic Qualifications

Bachelor's Degree in Mathematics/Business Analytics/Statistics/Computer Science or related discipline
3+ years of relevant experience in one of the following areas: Business intelligence, data engineering, database engineering, or business analytics.
3+ years experience with data modeling, SQL, ETL, Data Warehousing and Datalakes
3+ years experience with data visualization tools such as Tableau, Power BI, QlikView, Quicksight, etc.
3+ years of experience writing complex, optimized SQL queries.

Preferred Qualifications

Master's degree in Mathematics/Business Analytics/Statistics/Computer Science or related discipline
Experience in an analytical roles in an e-commerce, technology company, investment banking, or product management
Understanding of A/B testing methodology, advanced statistical/predictive modeling, and optimization techniques
Experience building self-service, interactive, scalable data solutions
Experience in building large scale distributed data processing pipelines
Experience designing and building new reporting from the ground up
Ability to explore new datasets and find potential problems
Experience with enterprise-class Business Intelligence tools such as Microstrategy, PowerBI, Tableau, Oracle BI, Pentaho, AWS Quicksight, etc.
Experience with data presentation skills to summarize key findings and communicate with both business and technical teams.

Amazon is committed to a diverse and inclusive workplace. Amazon is an equal opportunity employer and does not discriminate on the basis of race, national origin, gender, gender identity, sexual orientation, protected veteran status, disability, age, or other legally protected status. For individuals with disabilities who would like to request an accommodation, please visit https://www.amazon.jobs/en/disability/us.


Company - Amazon Data Services, Inc.

Job ID: A1977728"
Reveel,Senior Data Engineer,"Irvine, CA
Hybrid","Posted by

Chad Beville

Co-Founder at Reveel

Send InMail

We are looking for a Data Engineer to help build our data infrastructure.




A key part of our application involves data processing to support analytics and machine learning workflows. This person should be familiar with data modeling, ETL pipelines, as well as have experience with systems integration, concurrency, distributed systems, eventually consistent distributed databases, and building fault tolerant systems.




This person should have experience in the AWS ecosystem (AWS EMR, AWS Redshift, AWS Sagemaker, AWS Kinesis Data Streams, AWS Glue, etc). Experience working with both SQL (e.g. PostgreSQL) and NoSQL (e.g. DynamoDB) databases is also required. Databricks experience is a plus, as we are moving to this platform for ELT.




This role is fairly autonomous. A successful applicant should be comfortable using his or her skill, intuition, and creativity to accomplish a task without detailed direction on how it should be accomplished. Although this applicant will need to be competent and self-sufficient, they will work closely with a team that is full of friendly people that want everyone to succeed. This person should be comfortable working in 2-week sprint cycles.




Minimum Requirements:

Experience as a backend / systems developer

US citizenship or permanent residence




The applicant should be skilled in the following technologies:

SQL (PostgreSQL preferred)
Git
Python
RabbitMQ
AWS S3
AWS Lambda
AWS DynamoDBSQL (PostgreSQL preferred)
AWS DynamoDB
AWS Glue

Bonus if skilled in the following technologies:

Databricks
Clojure
RabbitMQ
Heroku
JavaScript
AWS Textract
AWS CloudFormation
AWS EMR
AWS Redshift
AWS Kinesis Data Streams




In recent years, we’ve pioneered first-in-industry shipping data analytics software, so that our clients can understand the factors affecting their shipping spend. No other company is helping small and medium sized businesses reduce shipping cost increases with technology like Reveel. The logic behind shipping costs is insane and very hard to understand! We make that easy for our customers by creating amazing software.




Since 2006, Reveel® has been dedicated to providing Shipping Intelligence® for the smartest business decisions possible. Founded by former DHL sales executives, Reveel was created to level the playing field. Over the past decade-plus, our zero-risk services have saved our clients millions of dollars."
Cedars-Sinai,Data Engineer,"Los Angeles, CA
On-site","Grow your career at Cedars-Sinai!

The Enterprise Information Services (EIS) team at Cedars-Sinai understands that true clinical transformation and the optimization of a clinical information systems implementation is fueled through the alignment of people, processes, and technologies.

Why work here?

Beyond an outstanding benefit package, we take pride in hiring the best, most committed employees. Our staff reflects the culturally and ethnically diverse community we serve. They are proof of our dedication to creating a multifaceted, inclusive environment that fuels innovation and the standard of patient care we strive for.

What Will You Be Doing In This Role

The Data Engineer is responsible for application development supporting business objectives while providing moderate to complex level experience in software development lifecycle phases from concept and design to testing. Analyses, designs, and builds component-based applications including introduction of an application layer, modeling techniques, component and object-oriented design, sophisticated algorithmic coding, and detailed approaches to application integration. Works on new and existing applications. Performs hands-on coding and assists in architecting solutions.

Works closely with application teams to understand workflow, documentation standards and potential gaps. Makes recommendations for closing gaps.
Author moderately complex and performant SQL statements based on end user specifications.
Triages intake of data requests and seeks understanding of business need.
Designs, develops, and supports applications using python.
Scopes, implements, tests, and deploys new features and versions of core applications, databases, and utilities."
Spectrum Health,Data Engineer,"Grand Rapids, MI
Hybrid","Posted by

Vinay Kumar Pendam

Manager, Information Services at Spectrum Health

We are looking for a couple of savvy Data Engineers to join our Enterprise Information Management team. They must be self-directed and comfortable supporting the data needs of multiple teams, systems and products. The right candidate will be excited by the prospect of optimizing or even re-designing our company’s data architecture to support our next generation of products and data initiatives.

For Job Summary, Functions, and Qualifications - check out the following link. https://spectrumhealth.wd5.myworkdayjobs.com/en-US/SpectrumHealthCareers/job/Application-Development-Analyst_R37365
"
"Solera, Inc.",Data Engineer,"United States
Remote","Posted by

Jonna English 2nd

**Almost as good as Liam Neeson, at finding missing talent.**

Send InMail

Data Engineer II - Virtual USA




Who We Are




Solera is a global leader in data and software services that strives to transform every touchpoint of the vehicle lifecycle into a connected digital experience. In addition, we provide products and services to protect life’s other most important assets: our homes and digital identities. Today, Solera processes over 300 million digital transactions annually for approximately 235,000 partners and customers in more than 90 countries. Our 6,500 team members foster an uncommon, innovative culture and are dedicated to successfully bringing the future to bear today through cognitive answers, insights, algorithms and automation. For more information, please visit solera.com.




The Role




Data Engineer II




What You’ll Do




Collaborate with data engineers, business analysts and product managers
Participate in daily scrum and other agile ceremonies
Collaborate with product managers to develop SOWs for new customers
Model, wrangle, extract, transform, load and query data based on requirements from data product team
Manage small to medium sized data projects
Develop reports using Tableau and/or Looker
Engage with customers to gather requirements and troubleshoot issues
Manage your code using GitHub
Assist with the development of cloud-native data products in AWS
Work with big data




What You’ll Bring




1+ years hands on development experience with SQL, ETL, and some reporting tool like Tableau, Looker, or Power BI
Bachelors degree in Computer Science or Computer Engineering, Mathematics, Physics, or its professional equivalent
Analytical and problem solving skills and an understanding of data movement strategies
Exceptional communication skills - both verbal and written - with the ability to collaborate with other developers, and members of all other teams
Experience writing SQL queries
Experience using ETL tools
Some experience with Jira/Confluence, & GitHub
Logistics industry experience huge plus!




It is impossible to list every requirement for, or responsibility of, any position. Similarly, we cannot identify all the skills a position may require since job responsibilities and the Company’s needs may change over time. Therefore, the above job description is not comprehensive or exhaustive. The Company reserves the right to adjust, add to or eliminate any aspect of the above description. The Company also retains the right to require all employees to undertake additional or different job responsibilities when necessary to meet business needs."
Amazon Web Services (AWS),Sr. Data & ML Engineer,"Los Angeles, CA","Description

At Amazon Web Services (AWS), we’re hiring highly technical Data and Machine Learning engineers to collaborate with our customers and partners on key engagements. Our consultants will develop and deliver proof-of-concept projects, technical workshops, and support implementation projects. These professional services engagements will focus on customer solutions such as Machine Learning, Data and Analytics, HPC and more.

In this role, you will work with our partners, customers and focus on our AWS offerings such Amazon Kinesis, AWS Glue, Amazon Redshift, Amazon EMR, Amazon Athena, Amazon SageMaker and more. You will help our customers and partners to remove the constraints that prevent them from leveraging their data to develop business insights.

AWS Professional Services engage in a wide variety of projects for customers and partners, providing collective experience from across the AWS customer base and are obsessed about customer success. Our team collaborates across the entire AWS organization to bring access to product and service teams, to get the right solution delivered and drive feature innovation based upon customer needs.

You will also have the opportunity to create white papers, writing blogs, build demos and other reusable collateral that can be used by our customers. Most importantly, you will work closely with our Solution Architects, Data Scientists and Service Engineering teams.

The ideal candidate will have extensive experience with design, development and operations that leverages deep knowledge in the use of services like Amazon Kinesis, Apache Kafka, Apache Spark, Amazon Sagemaker, Amazon EMR, NoSQL technologies and other 3rd parties.

Inclusive Team Culture

Here at AWS, we embrace our differences. We are committed to furthering our culture of inclusion. We have thirteen employee-led affinity groups, reaching 85,000 employees in over 190 chapters globally. We have innovative benefit offerings, and host annual and ongoing learning experiences, including our Conversations on Race and Ethnicity (CORE) and AmazeCon (gender diversity) conferences. Amazon’s culture of inclusion is reinforced within our 16 Leadership Principles, which remind team members to seek diverse perspectives, learn and be curious, and earn trust.

Work/Life Balance

Our team puts a high value on work-life harmony. Striking a healthy balance between your personal and professional life is crucial to your happiness and success here. We are a customer-obsessed organization—leaders start with the customer and work backwards. They work vigorously to earn and keep customer trust. As such, this is a customer facing role in a hybrid delivery model. Project engagements include remote delivery methods and onsite engagement that will include travel to customer locations as needed.

Mentorship & Career Growth

Our team is dedicated to supporting new members. We have a broad mix of experience levels and tenures, and we’re building an environment that celebrates knowledge sharing and mentorship. We care about your career growth and strive to assign projects based on what will help each team member develop into a better-rounded professional and enable them to take on more complex tasks in the future.

This is a customer facing role. You will be required to travel to client locations and deliver professional services when needed.


Basic Qualifications

Bachelor’s degree in Computer Science, Engineering, Mathematics or a related field or equivalent professional or military experience
8+ years of experience of Data platform implementation
3+ years of hands-on experience in implementation and performance tuning of Kinesis, Kafka, Spark or similar implementations
Hands on experience with building data or machine learning pipeline
Experience with one or more relevant tools (Flink, Spark, Sqoop, Flume, Kafka, Amazon Kinesis)
Experience developing software code in one or more programming languages (Java, JavaScript, Python, etc)
Current experience with hands-on implementation

Preferred Qualifications

Masters or PhD in Computer Science, Physics, Engineering or Math.
Familiar with Machine learning concepts
Hands on experience working on large-scale data science/data analytics projects
Hands-on experience with technologies such as AWS, Hadoop, Spark, Spark SQL, MLib or Storm/Samza.
Experience Implementing AWS services in a variety of distributed computing, enterprise environments.
Experience with at least one of the modern distributed Machine Learning and Deep Learning frameworks such as TensorFlow, PyTorch, MxNet Caffe, and Keras.
Experience building large-scale machine-learning infrastructure that have been successfully delivered to customers.
Experience defining system architectures and exploring technical feasibility trade-offs.
3+ years experiences developing cloud software services and an understanding of design for scalability, performance and reliability.
Ability to prototype and evaluate applications and interaction methodologies.
Experience with AWS technology stack.
Written and verbal technical communication skills with an ability to present complex technical information in a clear and concise manner to a variety of audiences.

Amazon is committed to a diverse and inclusive workplace. Amazon is an equal opportunity employer and does not discriminate on the basis of race, national origin, gender, gender identity, sexual orientation, protected veteran status, disability, age, or other legally protected status. For individuals with disabilities who would like to request an accommodation, please visit https://www.amazon.jobs/en/disability/us.

Pursuant to the San Francisco Fair Chance Ordinance, we will consider for employment qualified applicants with arrest and conviction records.

Pursuant to the Los Angeles Fair Chance Ordinance, we will consider for employment qualified applicants with arrest and conviction records.


Company - Amazon Web Services, Inc.

Job ID: A1414200"
Amazon Web Services (AWS),"Infrastructure Data Engineer, Datacenter Security","San Francisco, CA","Job Summary

DESCRIPTION

Job Summary

Come join the Amazon Web Services (AWS) Data Center Infrastructure Security Team in our mission to change the way we manage data center enterprise asset security and process excellence on behalf of our worldwide customers.

If you are passionate about driving meaningful change and operational performance with data and insights, this is the right role for you. Your work will directly enable leaders across software, hardware and infrastructure operations to ensure all networking and server components are managed in a responsible, secure and repeatable way. You will be driving business intelligence solutions end-to-end: business requirements, data modeling, ETL, metadata, reporting, and dashboard development. Your self-service analytics tools will be leveraged across our entire global footprint. You will work closely with business planning and finance teams to achieve business objectives and communicate insights to key stakeholders. You will implement best practices in reporting and analysis, including data integrity, visualization design, and documentation; and drive results in a fast-paced, ambiguous environment.

Key job responsibilities

Work directly with customers to understand the business problems at hand and propose impactful analyses and solutions. Ensure customers’ problems are solved and sustained.
Design, develop and manage scalable solutions for new and existing metrics, reports and dashboards to inform business decisions.
Write high quality SQL to retrieve and analyze data from relational databases (ex. Redshift, MySQL)
Build ETLs to ingest the data into the data warehouse and datalake, as well as end-user facing reporting applications.
Create user friendly repeatable reporting solutions to minimize ad hoc customer requests, which creates team thrash and a poor customer experience.
Interface with leadership and management to drive performance behaviors
Invent new ways to define and measure program effectiveness and organizational productivity.


Basic Qualifications

Bachelor's Degree in Mathematics/Business Analytics/Statistics/Computer Science or related discipline
3+ years of relevant experience in one of the following areas: Business intelligence, data engineering, database engineering, or business analytics.
3+ years experience with data modeling, SQL, ETL, Data Warehousing and Datalakes
3+ years experience with data visualization tools such as Tableau, Power BI, QlikView, Quicksight, etc.
3+ years of experience writing complex, optimized SQL queries.

Preferred Qualifications

Master's degree in Mathematics/Business Analytics/Statistics/Computer Science or related discipline
Experience in an analytical roles in an e-commerce, technology company, investment banking, or product management
Understanding of A/B testing methodology, advanced statistical/predictive modeling, and optimization techniques
Experience building self-service, interactive, scalable data solutions
Experience in building large scale distributed data processing pipelines
Experience designing and building new reporting from the ground up
Ability to explore new datasets and find potential problems
Experience with enterprise-class Business Intelligence tools such as Microstrategy, PowerBI, Tableau, Oracle BI, Pentaho, AWS Quicksight, etc.
Experience with data presentation skills to summarize key findings and communicate with both business and technical teams.

Amazon is committed to a diverse and inclusive workplace. Amazon is an equal opportunity employer and does not discriminate on the basis of race, national origin, gender, gender identity, sexual orientation, protected veteran status, disability, age, or other legally protected status. For individuals with disabilities who would like to request an accommodation, please visit https://www.amazon.jobs/en/disability/us.

Pursuant to the San Francisco Fair Chance Ordinance, we will consider for employment qualified applicants with arrest and conviction records


Company - Amazon Data Services, Inc.

Job ID: A1977729"
Amazon Web Services (AWS),"Infrastructure Data Engineer, Datacenter Security","Houston, TX","Job Summary

DESCRIPTION

Job Summary

Come join the Amazon Web Services (AWS) Data Center Infrastructure Security Team in our mission to change the way we manage data center enterprise asset security and process excellence on behalf of our worldwide customers.

If you are passionate about driving meaningful change and operational performance with data and insights, this is the right role for you. Your work will directly enable leaders across software, hardware and infrastructure operations to ensure all networking and server components are managed in a responsible, secure and repeatable way. You will be driving business intelligence solutions end-to-end: business requirements, data modeling, ETL, metadata, reporting, and dashboard development. Your self-service analytics tools will be leveraged across our entire global footprint. You will work closely with business planning and finance teams to achieve business objectives and communicate insights to key stakeholders. You will implement best practices in reporting and analysis, including data integrity, visualization design, and documentation; and drive results in a fast-paced, ambiguous environment.

Key job responsibilities

Work directly with customers to understand the business problems at hand and propose impactful analyses and solutions. Ensure customers’ problems are solved and sustained.
Design, develop and manage scalable solutions for new and existing metrics, reports and dashboards to inform business decisions.
Write high quality SQL to retrieve and analyze data from relational databases (ex. Redshift, MySQL)
Build ETLs to ingest the data into the data warehouse and datalake, as well as end-user facing reporting applications.
Create user friendly repeatable reporting solutions to minimize ad hoc customer requests, which creates team thrash and a poor customer experience.
Interface with leadership and management to drive performance behaviors
Invent new ways to define and measure program effectiveness and organizational productivity.


Basic Qualifications

Bachelor's Degree in Mathematics/Business Analytics/Statistics/Computer Science or related discipline
3+ years of relevant experience in one of the following areas: Business intelligence, data engineering, database engineering, or business analytics.
3+ years experience with data modeling, SQL, ETL, Data Warehousing and Datalakes
3+ years experience with data visualization tools such as Tableau, Power BI, QlikView, Quicksight, etc.
3+ years of experience writing complex, optimized SQL queries.

Preferred Qualifications

Master's degree in Mathematics/Business Analytics/Statistics/Computer Science or related discipline
Experience in an analytical roles in an e-commerce, technology company, investment banking, or product management
Understanding of A/B testing methodology, advanced statistical/predictive modeling, and optimization techniques
Experience building self-service, interactive, scalable data solutions
Experience in building large scale distributed data processing pipelines
Experience designing and building new reporting from the ground up
Ability to explore new datasets and find potential problems
Experience with enterprise-class Business Intelligence tools such as Microstrategy, PowerBI, Tableau, Oracle BI, Pentaho, AWS Quicksight, etc.
Experience with data presentation skills to summarize key findings and communicate with both business and technical teams.

Amazon is committed to a diverse and inclusive workplace. Amazon is an equal opportunity employer and does not discriminate on the basis of race, national origin, gender, gender identity, sexual orientation, protected veteran status, disability, age, or other legally protected status. For individuals with disabilities who would like to request an accommodation, please visit https://www.amazon.jobs/en/disability/us.


Company - Amazon Data Services, Inc.

Job ID: A1977727"
Homesite Insurance,Data Engineer II - Remote,"Los Angeles Metropolitan Area
Remote","Posted by

Tom Brenneman

Talent Acquisition Manager: Enterprise IT & Product COE

Send InMail

Homesite Insurance was founded in 1997 and was one of the first companies to enable customers to purchase home insurance directly online, during a single visit. Since then, we've continued to innovate rapidly to meet the needs of our customers and their changing expectations.

One thing that's stayed the same since our founding: our commitment to our customers, partners and employees.

Join us on our journey as we continue to grow into a powerful contender in the field of insurance.

Homesite Insurance was founded in 1997 and was one of the first companies to enable customers to purchase home insurance directly online, during a single visit. Since then, we've continued to innovate rapidly to meet the needs of our customers and their changing expectations.

One thing that's stayed the same since our founding: our commitment to our customers, partners and employees.

Join us on our journey as we continue to grow into a powerful contender in the field of insurance.

Homesite is an innovative national property and casualty insurance company with a fast-paced environment. We're proud to be part of the American Family Mutual Insurance Company and the home for the Enterprise’s Property Center of Excellence. We support multiple products sold through a number of operating companies and distribution methods, including offerings in partnership with other large national brands. We seek a person possessing analytical rigor and strong technical skills with a desire to build a successful career in Data Engineering. The primary role will be to develop cloud-based data solutions intended to support the data needs of statistical predictive modeling projects that grow profits through the development of state of the art pricing and product plans.

Responsibilities

· Develop, maintain, and enhance data tooling to enable data integration and availability.

· Collaborate with the Modeling and Data Science teams to provide non-production dev/ops and robust monitoring of model performance.

· Assist in the implementation of a cloud data strategy including tooling, governance, and documentation.

· Develop, maintain, and enhance databases throughout the organization to support analytics projects.

· Understand corporate data structure to be able to draw data from transactional data tables existing in the company.

· Support the acquisition of external data sets, interpreting data layouts, structures, fields and values to incorporate new data into the core analytics data base.

· Analyze databases for performance optimization, including data normalization, indexing, and memory management.

· Clearly communicate complex findings to colleagues and external customers.

· Monitor the results of statistical models through the use of dashboards and ad hoc analyses.

Minimum Requirements

· Bachelor’s degree or higher in Computer Science, Engineering, Information Systems, Statistics, Machine Learning, Applied Mathematics, Physics or other quantitative discipline.

· Three or more years of programming or data engineering experience.

· Intermediate software development skills utilizing an Object-Oriented Approach with one or more of the following languages in: Python, Java/Kotlin, Go

· Familiarity with implementation of data engineering tooling such as Apache Spark, Apache Airflow, and/or cloud provider-specific tooling.

· Familiarity with pipeline and application deployment to various compute approaches including serverless, container orchestration, and cloud-based virtual machines.

· Intermediate SQL knowledge, including advanced knowledge of one of more of the following: Microsoft SQL Server, PostgreSQL, Google BigQuery, Amazon Redshift or similar

· Familiarity with Linux/Unix shell.

· Experience with at least one of the major cloud platforms: Google Cloud Platform, Amazon Web Services, and/or Microsoft Azure

· Moderate level of comfort working with Git.

· Motivated individual with strong analytic, problem solving, and troubleshooting skills

Preferred Qualifications

· Master’s degree or higher in Computer Science, Engineering, Statistics, Machine Learning, Applied Mathematics, Physics or other quantitative discipline.

· Familiarity with statical computing approaches (R, Pandas/NumPy, Mathematica, and such)

· Experience with administrative responsibilities on Windows Server environments.

· Experience with NoSQL approaches such as DynamoDB, MongoDB, and/or Google BigTable

· Familiarly with data lake approaches such as Amazon Athena/S3, Delta Lake, and/or Google Cloud Dataflow/Cloud Storage.

· Knowledge of data visualization approaches such as Tableau, D3.js, Looker, and/or Power BI.

· Experience with MLOps tooling such as DataRobot, KubeFlow, AWS Sagemaker, MlFlow, H20, GCP AI Platform, and such.

· Knowledge of statistical software languages/packages such as R, Python, DataRobot, Tableau, Mathematica, and Spark."
Grammarly,"Data Engineer, Data Platform","San Francisco County, CA
Remote","Grammarly offers a remote-first hybrid working model . Team members can work primarily remotely. Teams will meet in person every quarter in one of Grammarly’s hubs, currently in San Francisco, Vancouver, New York, and Kyiv. To ensure that teams are able to overlap in their working hours and to meet face-to-face when needed, all team members need to live within three time zones of their direct team.

Grammarly team members who will be collaborating at our San Francisco hub must be based in the United States.

The opportunity

Grammarly empowers people to thrive and connect whenever and wherever they communicate. Every day, 30 million people and 30,000 teams around the world use our AI-powered writing assistant. All of this begins with our team collaborating in a values-driven and learning-oriented environment.

To achieve our ambitious goals, we’re looking for a Data Engineer to join our Data Platform team. The Data Platform team is responsible for all aspects of the data lifecycle here at Grammarly.

As a rapidly growing company, we offer opportunities to join a team and learn from experienced peers, as well as opportunities to build product offerings and teams from the ground up. Within our growing engineering organization, there is plenty of room for ownership and direct impact on users.

Grammarly’s engineers and researchers have the freedom to innovate and uncover breakthroughs—and, in turn, influence our product roadmap. The complexity of our technical challenges is growing rapidly as we scale our interfaces, algorithms, and infrastructure. Read more about our stack or hear from our team on our technical blog .

Your impact

As a Data Engineer, you will create engineering and analytical efficiencies as the owner of our company-wide data lake. Our cutting-edge data lake is the central hub for all data producers and data consumers, so your impact will span all of Grammarly. You will have the opportunity to design and code highly optimized algorithms and datasets.

Data engineers are responsible for making disparate data centrally available to Grammarly’s team members while ensuring query efficiency and efficacy. The technical requirements of this role include a deep understanding of the querying engine of Spark, the performance (read/write) optimization of Delta tables, data modeling, data transformation at scale, Python or Scala coding abilities, and advanced SQL skills. The backbone of a Data Engineer at Grammarly’s success will be communication, stakeholder management, and a passion for data.

We invite you to share your knowledge, experience, and goals with us to help us find the best team match for you. Tell us your superpower!

In This Role, You Will

Start building and pushing code in your first week and ship meaningful features in your first few months.
Build algorithms that allow for the quick and efficient retrieval of large amounts of data.
Build datasets that create fact-based insight, influence operational decision-making, and provide data-driven business solutions.
Have the opportunity to mentor new hires.
Shape and build a technical and data-driven culture at Grammarly.
We’re looking for someone who

Embodies our EAGER values—is ethical, adaptable, gritty, empathetic, and remarkable.
Has at least 2 years of experience using large-scale data-analytics engines like Spark.
Displays excellent software-engineering fundamentals, including knowledge of algorithms and data structures.
Demonstrates perseverance when faced with tough technical issues.
Has some experience with AWS or other cloud offerings (GCP, Azure, etc.).
Has experience building, deploying, and debugging production systems at scale.
Cares about the end-user experience and strives to ensure high quality.
Support for you, professionally and personally

Professional growth : We hire people we trust, and we give team members autonomy to do their best work. We also support professional development with training, coaching, and regular feedback.
A connected team : Grammarly builds a product that helps people connect, and we apply this mindset to our own team. We have a highly collaborative culture supported by our EAGER values. We also take time to celebrate our colleagues and accomplishments with global, local, and team-specific events and programs.
Comprehensive benefits : Grammarly offers all team members competitive pay along with a benefits package encompassing superior health care (including mental health benefits). We also offer support to set up a home office, ample and defined time off, gym and recreation stipends, 401(k) matching, and more.
For Colorado-based employment : The salary range for this position is $101,000–$217,000/year; however, base pay offered may vary considerably depending on job-related knowledge, skills, and experience. The compensation package includes a wide range of medical, dental, vision, financial, and other benefits, as well as equity.
We encourage you to apply

At Grammarly, we value our differences, and we encourage all—especially those whose identities are traditionally underrepresented in tech organizations—to apply. We do not discriminate on the basis of race, religion, color, gender expression or identity, sexual orientation, national origin, citizenship, age, marital status, veteran status, disability status, or any other characteristic protected by law. Grammarly will consider qualified applicants with criminal histories in a manner consistent with the San Francisco Fair Chance Ordinance. Grammarly is an equal opportunity employer and participant in the U.S. Federal E-Verify program.

Please note that Grammarly’s COVID-19 vaccination policy requires that all team members in North America be vaccinated against COVID-19 to meet in person for Grammarly business or to work from a North America hub location. It is expected that this will be a requirement for this role. Qualified candidates in North America who cannot be vaccinated for medical reasons or because of a sincerely held religious belief may request a reasonable accommodation to this policy. For Ukraine, this policy requires team members to be vaccinated or produce a daily negative COVID-19 test administered at the Kyiv hub to work from the hub or attend in-person meetings.

"
American Family Insurance,Cloud Data Engineer III (R27171),"Greater Boston
Hybrid","Posted by

David Binder 2nd

Connecting High Tech Talent to Dream Jobs in Data Science/Engineering, Actuarial & Analytics.

At American Family Insurance, we believe people are an organization’s most valuable asset, and their ideas and experiences matter. From our CEO to our agency force, we’re committed to growing a diverse and inclusive culture that empowers innovation that will inspire, protect, and restore our customers’ dreams in ways never imagined.




American Family Insurance is driven by our customers and employees. That’s why we provide more than just a job – we provide opportunity. Whether you’re already part of our team in search of a new challenge or new to our company and ready for what’s next, you’re in the right place. Every dream is a journey that starts with a single step. Start your journey right here. Join our team. Bring your dreams.

Job ID:

R27171 Cloud Data Engineer III (open to remote) (Open)Compensation may vary based on the job level and your geographic work location.

Compensation Minimum:$103,500Compensation Maximum:$165,700Summary:

This is a team of BUILDERS--all cloud data engineering, end-to-end from concept to implementation and deployment. Skill set includes: AWS and GCP with strong emphasis on fundamentals (Python, OOP coding practices, SQL, Big Data, data engineering, need to be a strong coder). ""Full-stack"" sensibility is a big plus as employees take responsibility for multiple layers of a solution. The team has a well documented training path, constant communication and mutual support. Engineers will learn architecture over time.




The Cloud Data Engineer is a specialized role participating in designing and implementing systems on Public Cloud infrastructure to deliver more analytical and business value from a wide range of data sources. You will work with the team to design and develop high-performance, resilient, automated data pipelines, streams, and applications, adapting technologies for ingesting, transforming, classifying, cleansing and exposing data using creative design to meet objectives. Your broad experience with data management technologies will enable you to match the right technologies to the required schemas and workloads. Our focus in on the AWS and GCP platforms, with a strong serverless bias. We rely heavily on Python, PySpark, BigQuery and related technologies, and work in an Agile, DevOps team culture. We expect you to bring an array of specialized skills noted below, and to lead by learning.




Job Description:

Primary Accountabilities




Build and Maintain serverless data pipelines in terabyte scale using AWS and GCP services – AWS Glue, PySpark and Python, AWS Redshift, AWS S3, AWS Lambda and Step Functions, AWS Athena, AWS DynamoDB, GCP BigQuery, GCP Cloud Composer, GCP Cloud Functions, Google Cloud Storage and others
Integrate new data sources from enterprise sources and external vendors using a variety of ingestion patterns including streams, SQL ingestion, file and API.
Maintain and provide support for the existing data pipelines using the above-noted technologies
Work to develop and enhance the data architecture of the new environment, including recommending optimal schemas, storage layers and database engines including relational, graph, columnar, and document-based, according to requirements
Develop real-time/near real-time data ingestion from a range of data integration sources, including business systems, external vendors and partner and enterprise sources
Provision and use machine-learning-based data wrangling tools like Trifacta to cleanse and reshape 3rd party data to make suitable for use.
Participate in a DevOps culture by developing deployment code for applications and pipeline services
Develop and implement data quality rules and logic across integrated data sources.
Serve as internal subject matter expert and coach to train team members in the use of distributed computing frameworks and big-data services and tools, including AWS and GCP services and projects

Travel Requirements




This position requires travel up to 10% of the time.

Specialized Knowledge & Skills Requirements




Four years working with datasets with very high volume of records or objects
Expert level programming experience in Python and SQL
Two years working with Spark or other distributed computing frameworks (may include: Hadoop, Cloudera)
Four years with relational databases (typical examples include: PostgreSQL Microsoft SQL Server, MySQL, Oracle)
Two years with AWS services including S3, Lambda, Redshift, Athena, S3
One year working with Google Cloud Platform (GCP) services, which may include any combination of: BigQuery, Cloud Storage, Cloud Functions, Cloud Composer, Pub/Sub and others (this may be via POC or academic study, though professional experience is preferred)
Some knowledge of AWS services: DynamoDB, Step Functions
Experience with contemporary data file formats like Apache Parquet and Avro, preferably with compression codecs, like Snappy and BZip.
Experience analyzing data for data quality and supporting the use of data in an enterprise setting.

Desired Experience and Skills




Streaming technologies (e.g.: Amazon Kinesis, Kafka)
Graph Database experience (e.g.: Neo4j, Neptune)
Distributed SQL query engines (e.g.: Athena, Redshift Spectrum, Presto)
Experience with caching and search engines (e.g.: ElasticSearch, Redis)
ML experience, especially with Amazon Sagemaker, DataRobot, AutoML
IAC coding tools, including CDK, Terraform, Cloudformation, Cloud Build

Education and Licenses




Master's degree in computer science, mathematics, engineering, or equivalent combination of education and experience.

Additional Job Information:

Although currently remote, we prefer candidates to be located in the Boston, MA area as in future state would have teams in the office 1x per week. Some consideration may be given for candidates in the Eastern/Central timezones.

Offer to selected candidate will be made contingent on the results of applicable background checks.

Offer to selected candidate is contingent on signing a non-disclosure agreement for proprietary information, trade secrets, and inventions.

When you work at American Family you can expect benefits that support your physical, emotional, and financial wellbeing. You will have access to comprehensive medical, dental, vision and wellbeing benefits that enable you to take care of your health. We also offer a competitive 401(k) contribution, a pension plan, an annual incentive, and a paid-time off program. In addition, our student loan repayment program and paid-family leave are available to support our employees and their families. Interns and contingent workers are not eligible for American Family Enterprise benefits.

We also consider qualified applicants with criminal histories, consistent with applicable federal, state and local law.

Stay connected: Join Our Enterprise Talent Community!"
Roblox,Senior Data Engineer - Growth Acquisition & Activation,"San Mateo, CA
On-site","Every day, tens of millions of people from around the world come to Roblox to play, learn, work, and socialize in immersive digital experiences created by the community. Our vision is to build a platform that enables shared experiences among billions of users. This is what’s known as the metaverse: a persistent space where anyone can do just about anything they can imagine, from anywhere in the world and on any device. Join us and you’ll usher in a new category of human interaction while solving exceptional challenges that you won’t find anywhere else.

As the lead data engineer on the Growth Acquisition & Activation team, you will build and manage the data pipelines that power our user acquisition and retention efforts for our next generation of Roblox users. You will work with partners from Growth Marketing and data science to build a scalable set of pipelines that form the basis of our paid user acquisition efforts. You will translate business requirements into performant data models for Multi-Terabyte data sets. You will also establish the ETLs and backend systems to help personalize the first time user experience for our new users. This role reports to the Engineering Manager on the growth team.

You Have

Proficient in SQL and at least one programming language (Python is ideal)
Experienced Data Engineer
5+ years of professional experience working with scalable ETL pipelines on industry standard ETL orchestration tools (eg - Airflow, Luigi, AWS Step Functions)
Experience working in the Hadoop Data Ecosystem for data processing.
Experience with at least one major cloud's suite of offerings (AWS, GCP, Azure)
(Preferred) Experience with growth marketing programs or user acquisition channel level expertise.
Familiarity with growth marketing metrics, KPI's, and dashboards
Understanding of different marketing attribution logics with experience making recommendations to enhance models.

You Will

Partner with growth marketing, data science, engineering and our external partners to collect data requirements needed to establish foundational data pipelines.
Architect ETL pipelines to grow and extend our user acquisition efforts.
Scale data flows that improve acquisition and first time user experience on the platform
Shape the next generation of patterns and technologies used at Roblox
Manage writing unit and integration tests
Identify and resolve performance issues
Participate in building the world's best user generated gaming platform

You’ll Love

Industry-leading compensation package
Excellent medical, dental, and vision coverage
A rewarding 401k program
Flexible vacation policy
Roflex - Flexible and supportive work policy
Roblox Admin badge for your avatar
At Roblox HQ:
Free catered lunches
Onsite fitness center and fitness program credit
Annual CalTrain Go Pass"
Edward Jones,"Data Engineer, Technology","Los Angeles, CA
Remote","60717BR

About-Us

At Edward Jones, we help clients achieve their serious, long-term financial goals by understanding their needs and implementing tailored solutions. To ensure a personal client experience, we have located our 14,000+ branch offices where our more than 7 million clients live and work.

A typical branch office has one financial advisor who meets with clients face-to-face and one branch office administrator who enhances the team's ability to build deep relationships with clients. Headquarters associates in St. Louis and Tempe provide support and expertise to help U.S. and Canada branch teams deliver an ideal client experience. Edward Jones currently has more branch offices than any other financial services firm, and we continue to grow to meet the needs of long-term individual investors.

Job-Overview

At Edward Jones we are developing a next-generation analytics architecture to support our growing business. As part of the Data Architecture and Data Design team, you will be challenged daily to find innovative solutions to improve the speed and consumption of our information. This will also bring you into contact with a diverse array of stakeholders ranging from IT to our front line business organizations, and you will be working on a data platform that establishes the foundational layer for Edward Jones analytics for years to come.

Responsibility Summary

Analyze data requirement and mapping documents to determine the best solution for data onboarding
Work across stakeholders including data stewards, business analysts, and data modelers to clarify and confirm data requirements
Design and implement the ODS data platform required for optimal extraction, transformation, and loading (ETL) of data from a wide variety of data sources
Create and maintain optimal data pipeline architecture capable of ingesting structured and unstructured data in near real-time
Assemble large, complex data sets that meet functional and technical requirements
Identify, design, and implement internal process improvements: Automating manual processes, optimizing data delivery, redesigning infrastructure for greater scalability, etc.
Develop data flows that can leverage both on premise and cloud architectures
Build KPIs that utilize the data pipeline to provide insights into user adoption, operational efficiency and other key business performance metrics
Implement data control and exception handling including data reconciliation
Work with stakeholders including the business analytics teams and IS architecture teams to assist with data-related technical issues and support their data infrastructure needs

Skills-Requirements

Requirements:
Advanced working SQL knowledge and experience working with relational databases, query authoring (SQL), as well as experience with complex Stored Procedures for data processing.
Experience building and optimizing data pipelines, architectures and datasets
Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement
Build processes supporting data transformation, data structures, metadata, dependency and workload management
A successful history of manipulating, processing and extracting value from large disconnected datasets
Working knowledge of message queuing, stream processing, and highly scalable ‘big data’ data stores
Experience supporting and working with cross-functional teams in a dynamic environment
5+ years' experience building large scale operational data stores in Oracle or equivalent relational database
Understanding of data domains such as client, account, investments, transactions, holdings etc. are highly desired
Candidates should also have 3+ years of experience using the following software/tools:
Hadoop, Spark, Kafka, etc.
Experience with relational SQL and NoSQL databases, including Oracle, MongoDB, or Azure SQL
Experience with AWS/Azure cloud services
Experience with stream-processing systems: Kafka, Spark-Streaming, etc.
Experience with object-oriented/object function scripting languages: Python, Java, etc.
Experience with IICS (Informatica Intelligent Cloud Services) or similar ETL tools

Awards-Accolades
FORTUNE 2020 – Edward Jones was named No. 7 on the 2020 FORTUNE 100 Best Companies to Work For® list.
From FORTUNE ©2020 FORTUNE Media IP Limited. All rights reserved. Used under license. FORTUNE and FORTUNE 100 Best Companies to Work For are registered trademarks of Fortune Media IP Limited and are used under license. FORTUNE and Fortune Media IP Limited are not affiliated with, and do not endorse products or services of, Edward Jones Investments.
Edward Jones Ranked No. 6 of the 100 best workplaces for millennials by Great Place to Work and FORTUNE Magazine.
2020 Corporate Equality Index – Edward Jones joins the ranks of 680 major U.S businesses that received top marks in the HRC Foundations 18th Annual Scorecard on LBGTQ Workplace Equality
Edward Jones named a top company for training, ranking No. 25 on Training magazine's 2020 Training Top 125 list – the highest ranking among the financial-services industry!
Financial services firm Edward Jones has been named one of the 2019 Best Workplaces for Parents by Great Place to Work and FORTUNE magazine. The firm ranked No. 5 on the list of 50 companies, up two spots from last year.
The Best Workplaces for Parents list is one of a series of rankings by Great Place to Work and FORTUNE based on employee feedback from Great Place to Work-Certified™ organizations. Edward Jones ranked No. 7 on the FORTUNE 100 Best Companies to Work for in 2019 list, the firm’s 20th appearance on the list. The firm also was ranked the No. 1 Best Workplace in Financial Services & Insurance and the No. 6 Best Workplace for Millennials by Great Place to Work and FORTUNE.

EEO

Edward Jones does not discriminate on the basis of race, color, gender, religion, national origin, age, disability, sexual orientation, pregnancy, veteran status, genetic information or any other basis prohibited by applicable law."
American Family Insurance,Cloud Data Engineer II (R27172),"Greater Boston
Hybrid","Posted by

David Binder 2nd

Connecting High Tech Talent to Dream Jobs in Data Science/Engineering, Actuarial & Analytics.

Send InMail

At American Family Insurance, we believe people are an organization’s most valuable asset, and their ideas and experiences matter. From our CEO to our agency force, we’re committed to growing a diverse and inclusive culture that empowers innovation that will inspire, protect, and restore our customers’ dreams in ways never imagined.




American Family Insurance is driven by our customers and employees. That’s why we provide more than just a job – we provide opportunity. Whether you’re already part of our team in search of a new challenge or new to our company and ready for what’s next, you’re in the right place. Every dream is a journey that starts with a single step. Start your journey right here. Join our team. Bring your dreams.

Job ID:

R27172 Cloud Data Engineer II (open to remote) (Open)Compensation may vary based on the job level and your geographic work location.

Compensation Minimum:$90,200Compensation Maximum:$144,500Summary:

This is a team of BUILDERS--all cloud data engineering, end-to-end from concept to implementation and deployment. Skill set includes: AWS and GCP with strong emphasis on fundamentals (Python, OOP coding practices, SQL, Big Data, data engineering, need to be a strong coder). ""Full-stack"" sensibility is a big plus as employees take responsibility for multiple layers of a solution. The team has a well documented training path, constant communication and mutual support. Engineers will learn architecture over time.




The Cloud Data Engineer is a specialized role participating in designing and implementing systems on Public Cloud infrastructure to deliver more analytical and business value from a wide range of data sources. You will work with the team to design and develop high-performance, resilient, automated data pipelines, streams, and applications, adapting technologies for ingesting, transforming, classifying, cleansing and exposing data using creative design to meet objectives. Your skills and education in data management technologies will enable you to match the right technologies to the required schemas and workloads. Our focus is on the AWS and GCP platforms, with a strong serverless bias. We rely heavily on Python, PySpark, BigQuery and related technologies, and work in an Agile, DevOps team culture. We expect you to bring an array of specialized skills noted below, and to come prepared to learn rapidly to build on the foundation of your basic skills and education in this field.




Job Description:

Primary Accountabilities




Build and Maintain serverless data pipelines in terabyte scale using AWS and GCP services – AWS Glue, PySpark and Python, AWS Redshift, AWS S3, AWS Lambda and Step Functions, AWS Athena, AWS DynamoDB, GCP BigQuery, GCP Cloud Composer, GCP Cloud Functions, Google Cloud Storage and others
Integrate new data sources from enterprise sources and external vendors using a variety of ingestion patterns including streams, SQL ingestion, file and API.
Maintain and provide support for the existing data pipelines using the above-noted technologies
Work to develop and enhance the data architecture of the new environment, including recommending optimal schemas, storage layers and database engines including relational, graph, columnar, and document-based, according to requirements
Develop real-time/near real-time data ingestion from a range of data integration sources, including business systems, external vendors and partner and enterprise sources
Provision and use machine-learning-based data wrangling tools like Trifacta to cleanse and reshape 3rd party data to make suitable for use.
Participate in a DevOps culture by developing deployment code for applications and pipeline services
Develop and implement data quality rules and logic across integrated data sources.
Serve as internal subject matter expert and coach to train team members in the use of distributed computing frameworks and big-data services and tools, including AWS and GCP services and projects

Travel Requirements




This position requires travel up to 10% of the time.

Specialized Knowledge & Skills Requirements




Some exposure to working with datasets with very high volume of records or objects
Intermediate level programming experience in Python and SQL
One year working with Spark or other distributed computing frameworks (may include: Hadoop, Cloudera)
Two years with relational databases (typical examples include: PostgreSQL Microsoft SQL Server, MySQL, Oracle)
Some exposure to AWS services including S3, Lambda, one or more AWS database technologies including Redshift, DynamoDB or Athena
Some exposure to AWS services: DynamoDB, Step Functions
Experience with contemporary data file formats like Apache Parquet and Avro, preferably with compression codecs, like Snappy and BZip.
Experience analyzing data for data quality and supporting the use of data in an enterprise setting.

Desired Experience & Skills




Some exposure to Machine Learning tools and practices, including DataRobot, Sagemaker or others
Some exposure to Google Cloud Platform (GCP) services, which may include any combination of: BigQuery, Cloud Storage, Cloud Functions, Cloud Composer, Pub/Sub and others (this may be via POC or academic study, though practical experience is preferred)
Streaming technologies (e.g.: Amazon Kinesis, Kafka)
Graph Database experience (e.g.: Neo4j, Neptune)
Distributed SQL query engines (e.g.: Athena, Redshift Spectrum, Presto)
Experience with caching and search engines (e.g.: ElasticSearch, Redis)
ML experience, especially with Amazon Sagemaker, DataRobot, AutoML
IAC coding tools, including CDK, Terraform, Cloudformation, Cloud Build

Education and Licenses




Bachelor’s degree in computer science, mathematics, engineering, or equivalent combination of education and experience.

Additional Job Information:

Although currently remote, we prefer candidates to be located in the Boston, MA area as in future state would have teams in the office 1x per week. Some consideration may be given for candidates in the Eastern/Central timezones.

Offer to selected candidate will be made contingent on the results of applicable background checks.

Offer to selected candidate is contingent on signing a non-disclosure agreement for proprietary information, trade secrets, and inventions.

When you work at American Family you can expect benefits that support your physical, emotional, and financial wellbeing. You will have access to comprehensive medical, dental, vision and wellbeing benefits that enable you to take care of your health. We also offer a competitive 401(k) contribution, a pension plan, an annual incentive, and a paid-time off program. In addition, our student loan repayment program and paid-family leave are available to support our employees and their families. Interns and contingent workers are not eligible for American Family Enterprise benefits.

We also consider qualified applicants with criminal histories, consistent with applicable federal, state and local law.

Stay connected: Join Our Enterprise Talent Community!"
Dice,Data Engineer - Python,"South San Francisco, CA
On-site","Dice is the leading career destination for tech experts at every stage of their careers. Our client, Advanced Software Talent, is seeking the following. Apply via Dice today!

Direct W2 employees only! No 3rd party agencies! Mainly remote work, some onsite work required. Description Summary Data Engineer with responsibility to configure non-GMP computer systems used by Manufacturing Science Technology (MSAT) department staff to organize manufacturing process information and to assist with bioprocess monitoring and analysis. This employee should be passionate about the intersection of data and life sciences. They should actively apply both data engineering and software development skills to solve problems. Job Responsibilities Work closely with MSAT staff to understand workflows and questions that originate in manufacturing and laboratory environments. Work as part of a larger data engineering team to develop analysis computer systems and tools with the overall goal to improve E2E operational performance (e.g., capacity to be freed up, quality, yield, productivity, lead times exact KPIs) at various manufacturing sites across the globe. Standard data engineering tasks obtain data extracts and define secure data exchange approaches, perform data quality checks and contribute to the data pipeline. Acquire, ingest, and process data from multiple sources and systems into Big Data platforms. Collaborate with data scientists to map data fields to hypotheses and curate, wrangle, and prepare data for use in their advanced analytical models. Skills Preferred 1 year of work experience in current good manufacturing practice biopharmaceutical production setting (process development andor manufacturing technical support). Some knowledge of statistical process control and data analysis techniques The ideal candidate will have a general knowledge of the underlying scientific principles applied to the development and manufacture of biopharmaceuticals. They will have a keen interest in learning bioprocess operations. Skills required Proficiency in data analysis techniques and a willingness to learn new techniques is desired. Experience with the following technologies Distributed Processing (Spark, Hadoop, EMR), traditional RDBMS (MS SQL Server, Oracle, MySQL, PostgreSQL), MPP (AWS Redshift, Teradata), NoSQL (MongoDB, DynamoDB, Cassandra, Neo4J, Titan) Ability to work across structured, semi-structured, and unstructured data, extracting information and identifying linkages across disparate data sets Experience and interest in Cloud platforms Experience in traditional data warehousing and deploying ETL processes with Python. Familiarity with agile ways of working e.g. in SCRUM-teams. Experience translating scientific or engineering workflows into automated systems. 3 years of work experience with Python, C, Javascript, or other programming language. 1 years of work experience with SQL and data manipulation."
Dice,AWS Data Engineer,"United States
Remote","Dice is the leading career destination for tech experts at every stage of their careers. Our client, Derex Technologies Inc., is seeking the following. Apply via Dice today!

Hi, Please find below Job description and send me your Updated Resume, AWS Data Engineer Long term Contract Job Description Senior Developer with at least 8+ years experience in data and atleast 4+ years of experience working with AWS Data platform Experience of developing on AWS Expensive experience on Spark Very experienced on Python or Scala Very experienced on SQL Strong ownership and can work independently"
Dice,Spark Data Engineer,"San Francisco, CA
On-site","Dice is the leading career destination for tech experts at every stage of their careers. Our client, Reliable Software Resources, is seeking the following. Apply via Dice today!

This is Srikanth from Reliable Software. I hope you and your friends and family are Good during the COVID crisis. We have a position with one of our clients. Below are a few details pertaining to the job. Please take a look at it and let me know if you would like to be considered for the opportunity. Please share with me your updated resume. Job Title Spark Data Engineer Location San Francisco, CA (hybrid oniste i.e tuesday thursday) Duration 12+ months Required Skills Spark Developer proficient in Python and SQL.. Qualifications At least 3+ years of experience in developing large scale data processingdata storagedata distribution systems At least 3+ years of experience on working with large Hadoop projects using Spark and Python and working with Spark DataFrame, Dataset APIs with SparkSQL as well as RDDs and Scala function literals and closures. Hands-on experience with Azure Data Factory, Databricks, and Azure Synapse Experience with ELTETL development, patterns and tooling, experience with ETL tools (Informatica, Talend) preferred. Experience with Azure Data and cloud environments including ADLS2, PowerBi, and Synapse Analytics Experience with SQL including Postgres, MySQL RDBMS platforms Experience with Linux (RHEL or Centos preferred) environments Experience with various IDE and code repositories as well as unit testing frameworks. Experience with code build tools such as Maven. Fundamental knowledge of distributed data processing systems and storage mechanisms. Ability to produce high quality work products under pressure and within deadlines with specific references Strong communication and collaborative skills At least 5+ years of working with large multi-vendor environment with multiple teams and people as a part of the project At least 5+ years of working with a complex Big Data environment 5+ years of experience with JIRAGitHubGit and other code management toolsets Educational Qualifications Required - Bachelorrsquos degree in Computer Science, Information Technology, Computer Engineering or closely related or equivalent. Preferred - Masterrsquos degree in Management Information Systems (MIS), Computer Science, Big Data or Analytics or equivalent Travel Open to travel based up on the nature of the engagement Thanks Regards Srikanth Donkani Sr. Talent Acquisition Specialist (w) 312-448-6138 (E) srikanth.drsrit.com www.rsrit.com 2260 Haggerty Road, Suite 285 Northville, MI 48167 Equal Employment Opportunity Reliable Software employment does not discriminate on the basis of race, religion, gender, sexual orientation, age or any other basis as covered by federal, state, or local law. Employment decisions are based solely on qualifications, merit and business needs."
Deluxe,Senior Data Engineer,"Atlanta, GA
Hybrid","Posted by

Paul Miller

Talent Acquisition Consultant at Deluxe

Send InMail

Req ID 220313WD-8

Location Atlanta, Georgia




Deluxe is a trusted, technology-enabled solutions provider for enterprises, small businesses and financial institutions offering a range of solutions to help customers manage and grow their businesses. Approximately 4.8 million small business customers access Deluxe's wide range of products and services, including incorporation services, logo design, website development and hosting, email marketing, social media, search engine optimization, and payroll services along with customized checks and forms. For our approximately 4,600 financial institution customers, Deluxe offers industry-leading programs in data analytics, customer acquisition and treasury management solutions, fraud prevention and profitability solutions, as well as checks. Deluxe is also a leading provider of checks and accessories sold directly to consumers.




The Senior Data Engineer, equipped with strong ETL and AWS experience will drive data ingestion and management of data pipelines for the Data Driven Marketing business. This role requires a high level of collaboration with Product Management, Data science and Compliance organizations. This person will participate in data asset management, regulatory compliance, internal and external audits associated with supplier and/or customer data.




Job Responsibilities

Manage all data pipelines execution, management, monitoring and issues resolution.
Work with Data Engineering to implement data pipelines.
Maintain and document data flow specifications for all supplier and/or customer data.
Lead and participate in data acquisition initiatives.
Participate in product management activities to meet the data needs associated with new products or services




Basic Qualifications:

Education and Experience: Bachelor’s and 3 years or HS/GED and 7 years

Expert in Microsoft ETL data ingestion framework specifically SQL Server, SSIS DevOps.
Expert in SQL server development and management tools.
Expert in file management, data storage management and operations on premise and cloud.
Solid understanding of data warehousing and archiving tools.
Solid understanding of Apache Spark operations and monitoring.
Solid understanding of high-volume data processing frameworks on premise and Cloud.
Familiarity with data pipeline management frameworks on cloud (AWS, Azure, Google)
Familiarity with Python, R scripting and IDEs like Jupitar notebook, RStudio.




Preferred Qualifications:




Education: Bachelors Degree in Computer Science or related field




Experience: 5 years of data ingestion and pipeline management

Familiarity with open-source data wrangling tools and technologies.
Familiarity with data access control and management tools.
Familiarity with Apache Hadoop, Hive, Impala etc.
Familiarity with NoSQL databases like MongoDB, Graph databases like Neo4j, Amazon Neptune etc.
Expertise in data pipeline management on any 1 Cloud (Amazon AWS, Microsoft Azure, Google Cloud).
Worked in data supply chain management for a large data aggregator.
Worked in data driven marketing businesses.




Deluxe Corporation is an Equal Opportunity / Affirmative Action employer:

All qualified applicants will receive consideration for employment without regard to race, color, creed, religion, disability, sex, age, ethnic or national origin, marital status, sexual orientation, gender identity or presentation, pregnancy, genetics, veteran status or any other status protected by state or federal law.




EOE/Minorities/Females/Vet/Disability

Please view the electronic EEO is the Law Poster which serves to inform you of your equal employment opportunity protections as part of the application process.




Reasonable Accommodation for Job Seekers with a Disability: If you require reasonable accommodation in completing this application, interviewing, completing any pre-employment testing, or otherwise participating in the employee selection process, please direct your inquiries to deluxecareers@deluxe.com.




Department: IT

Time Type: Full time

Work Status: Permanent"
Deloitte,Cloud Data Engineer-LOCATION OPEN,"San Diego, CA","Are you an experienced, passionate pioneer in technology; are solutions your focus, a roll-up-your-sleeves individual who thrives in a daily collaborative environment, a think-tank who can share new ideas with your colleagues - without the extensive demands of travel? If so, consider an opportunity with our US Delivery Center - we are breaking the mold of a typical Delivery Center.

Our US Delivery Centers have been growing since 2014 with significant, continued growth on the horizon. Interested? Read more about our opportunity below ...

Work You'll Do/Responsibilities

Work with the team to evaluate business needs and priorities, liaise with key business partners and address team needs related to data systems and management.
Translate business requirements into technical specifications; establish and define details, definitions, and requirements of applications, components and enhancements.
Participate in project planning; identifying milestones, deliverables and resource requirements; tracks activities and task execution.
Generate design, development, test plans, detailed functional specifications documents, user interface design, and process flow charts for execution of programming.
Develop data pipelines / APIs using Python, SQL, potentially Spark and AWS, Azure or GCP Methods.
Use an analytical, data-driven approach to drive a deep understanding of fast changing business.
Build large-scale batch and real-time data pipelines with data processing frameworks in AWS, Azure or GCP cloud platform.

The Team

From our centers, we work with Deloitte consultants to design, develop and build solutions to help clients reimagine, reshape and rewire the competitive fabric of entire industries. Our centers house a multitude of specialists, ranging from systems designers, architects and integrators - to creative digital experts - to cyber risk and human capital professionals. All work together on diverse projects from advanced preconfigured solutions and methodologies, to brand-building and campaign management.

We are a unique blend of skills and experiences, yet we underline the value of each individual, providing customized career paths, fostering innovation and knowledge development with a focus on quality. The US Delivery Center supports a collaborative team culture where we work and live close to home with limited travel.

Qualifications Required

3+ years of experience in data engineering with an emphasis on data analytics and reporting.
3+ years of experience with at least one of the following cloud platforms: Microsoft Azure, Amazon Web Services (AWS), Google Cloud Platform (GCP), others.
3+ years of experience in SQL, data transformations, statistical analysis, and troubleshooting across more than one Database Platform (Cassandra, MySQL, Snowflake, PostgreSQL, Redshift, Azure Synapse, etc.).
3+ years of experience in the design and build of data extraction, transformation, and loading processes by writing custom data pipelines.
3+ years of experience with one or more of the follow scripting languages: Python, SQL, Scala, PySPark and/or other.
3+ years of experience designing and building solutions utilizing various Cloud services such as EC2, S3, EMR, Kinesis, RDS, Redshift/Spectrum, Lambda, Glue, Athena, API gateway, Streaming services etc.
Bachelor's degree or equivalent work experience.
Must live a commutable distance to one of the following cities: Atlanta, GA; Austin, TX; Boston, MA; Charlotte, NC; Chicago, IL; Cincinnati, OH; Cleveland, OH; Dallas, TX; Detroit, MI; Gilbert, AZ; Houston, TX; Indianapolis, IN; Kansas City, MO; Lake Mary, FL; Mechanicsburg, PA; Miami, FL; Minneapolis, MN; Nashville, TN; Philadelphia, PA; Phoenix, AZ; Pittsburgh, PA; Sacramento, CA; St. Louis, MO; San Diego, CA; Seattle, WA; Tallahassee, FL; Tampa, FL; or be willing to relocate to one of the following USDC locations: Gilbert, AZ; Lake Mary, FL; Mechanicsburg, PA.
Limited Immigration sponsorship may be available.
Ability to travel up to 15% (While 15% of travel is a requirement of the role, due to COVID-19, non-essential travel has been suspended until further notice.)

Preferred

AWS, Azure and/or Google Cloud Platform Certification.
Master's degree or higher.
Expertise in one or more programming languages, preferably Scala, PySpark and/or Python.
Experience working with either a Map Reduce or an MPP system on any size/scale.
Experience working with agile development methodologies such as Sprint and Scrum."
Dice,Spark Data Engineer,"San Francisco, CA
On-site","Dice is the leading career destination for tech experts at every stage of their careers. Our client, Reliable Software Resources, is seeking the following. Apply via Dice today!

Qualifications At least 3+ years of experience in developing large scale data processingdata storagedata distribution systems At least 3+ years of experience on working with large Hadoop projects using Spark and Python and working with Spark DataFrame, Dataset APIs with SparkSQL as well as RDDs and Scala function literals and closures. Hands-on experience with Azure Data Factory, Databricks, and Azure Synapse Experience with ELTETL development, patterns and tooling, experience with ETL tools (Informatica, Talend) preferred. Experience with Azure Data and cloud environments including ADLS2, PowerBi, and Synapse Analytics Experience with SQL including Postgres, MySQL RDBMS platforms Experience with Linux (RHEL or Centos preferred) environments Experience with various IDE and code repositories as well as unit testing frameworks. Experience with code build tools such as Maven. Fundamental knowledge of distributed data processing systems and storage mechanisms. Ability to produce high quality work products under pressure and within deadlines with specific references Strong communication and collaborative skills At least 5+ years of working with large multi-vendor environment with multiple teams and people as a part of the project At least 5+ years of working with a complex Big Data environment 5+ years of experience with JIRAGitHubGit and other code management toolsets"
Tomorrow.io,Data Engineer,"United States
Remote","Posted by

Doron Mualem Feler 2nd

Talent Acquisition Specialist at Tomorrow.io

Send InMail

tomorrow.io's R&D Team is a mixture of scientists and engineers committed to generating the best and most novel data and models across all times: historical, real-time and forecast. The story just begins when the data hits our ingest and post-processing services. Every product that the user sees is the result of a pipeline of algorithms that needs to be run quickly and continuously. We are the team that builds the architecture behind the data and the models, to prepare the weather analyses, for the Product and Engineering team to serve to the masses.




We’re looking for a Data Engineer that will help us accelerate data science models to business value and make sure we are delivering the best weather forecast in the world.




As a Data Engineer at Tomorrow.io, you’ll develop and maintain weather applications data pipelines in an operations environment. You will also use your strong data engineering background to enable rapid experimentation of modeling and to help operationalize machine learning (ML) pipelines.




What you bring:

At least 3 years of experience as a software or data engineer, preference towards MLOps - building and deploying ML models in production environments
Experience with a cloud environment, e.g. GCP, AWS, Azure
Strong experience with data pipelining technologies (Airflow, Apache Beam/Dataflow, Kubeflow)
Experience with containerized Python development using Docker
Proficient SQL with hands on experience with databases (Postgres) and data warehouses (BigQuery, RedShift, Snowflake)
Excellent verbal and written communication skills




You might also have:

Experience with ML libraries and technologies in Python (TFX, PyTorch)
Experience with geoscientific/geospatial data




So if big data is your middle name and you have a passion for translating innovative ML models into robustly-engineered ML solutions, then this team is for you.




If you have reached this point and you are super excited but not sure you check all the boxes - we still want to speak with you! Your passion is priceless. Other things can be learned.




Proof of eligibility to work in the United States lawfully must be provided.

Anticipated salary range is $100K-$150K, subject to local market and a candidate’s skills and experience. Health, leave and other benefits included.

___________________________________




About Tomorrow.io:




Tomorrow.io is the world’s leading Weather Intelligence Platform™. Fully customizable to any industry impacted by the weather, customers around the world including Uber, Delta, Ford, National Grid and more use Tomorrow.io to dramatically improve operational efficiency.




Tomorrow.io was built from the ground up to help teams predict the business impact of weather, streamline team communication and action plans, improve productivity, and optimize profit margins.




Space: In case you have not heard, we are also going to space with our Operation Tomorrow Space initiative. We are building the first-of-its-kind proprietary satellites equipped with radar, and launching them into space to improve weather forecasting technology for everyone on Earth.




How we roll: We work in an “one office” environment. We believe that magic happens when people work together. Together also includes Zoom meetings, flexible hours and unlimited vacation days. Your success is achieved by your impact and deliveries and not by the hours you put in. We believe in transparency and directness, putting work before ego and empathy. We grow fast and move faster but we always see people first. Each person has their own career growth path for we believe that the only way for the company to grow is if you grow."
Dice,Data Engineer,"Cupertino, CA
On-site","Dice is the leading career destination for tech experts at every stage of their careers. Our client, Collabera, is seeking the following. Apply via Dice today!

Develop analytics based solutions that produce quantitative and qualitative business insights. Work with partners as necessary to integrate systems and data quickly and effectively, regardless of technical challenges or business environments. Design, Build Test the data streaming batch processing data pipeline as per the business requirements Build data pipelines using Spark, Java, Hadoop, Kafka as the primary skillset Must understand and contribute to architecture for distributed systems Collaborate with the team to use best practices to deliver and address any dependencies Provide regular status update including any risksissues to the project lead Proactive in learning new skills and communicating ideas articulately Please provide a summary of the total hands-on experience in the following areas (1) Spark (SparkQL, Spark Streaming v2.4x, Spark Structured Streaming v3.x) (2) Java as used to program Spark (3) building batch streaming data pipelines based on multiple Kafka topics (4) using Python to script portions of a data pipeline java,apache kafka,AWS bigdata,spark"
Dice,Data Engineer -- REMOTE,"United States
Remote","Dice is the leading career destination for tech experts at every stage of their careers. Our client, iBrain Technologies Inc., is seeking the following. Apply via Dice today!

Hi, Hope you are doing well. I have a position of Data Engineer for remote. Please find the JD and share yourconsultant profile. Data Engineer Pleasanton, CA (Remote until Covid) Job Description Need 10+ years' experience candidates. Look for someone who has strong experience with SQL, Python, AWS And Spark Regards, Suresh iBrain Technologies, INC. A Certified Minority Business Enterprise Email mailtoEmail Phone Direct www.ibrain-tech.com httpwww.ibrain-tech.com"
Twilio,Data Engineer (L3) - Email Platform,"Irvine, CA","Because you belong at Twilio

The Who, What, Why And Where

Twilio is growing rapidly and seeking a Software Engineer L3, experienced with Data Analytics, to join the Email Platform BU. This person will be a part of a team of talented engineers who build end-to-end data processing and persistence solutions that deliver high performance services at a very large scale.

This position is ideal for a self-learner and self-starter individual who can operate independently to solve common coding problems, and has a good understanding and experience working in a software development life cycle.

Who?

Twilio Is Looking For Software Engineer Who Lives The Twilio Magic And Has

Building a platform requires people from a variety of engineering backgrounds.

4+ years of software development experience at a fast paced, software delivery company
Hands-on experience in SQL and object oriented programming languages.
Experience working in the areas of data ingestion, data analytics, or distributed databases.
Understanding of cloud technologies and experience in at least one cloud platform: AWS, GCP, or Azure.
Experience building and operating distributed systems at high scale.
Track record of delivering systems with operational excellence; dependable, reliable, highly available.
Highly effective collaboration skills - you work with your team towards common goals.
Self-driven - you take complete ownership of your tasks and champion efforts to get your tasks to completion.
Experience with Containers/Kubernetes and Docker.
Degree in Computer Science or similar experience

What?

As a Software Engineer, you will you will live the Twilio Magic values:

BE INCLUSIVE, EMPOWER OTHERS: Work in a small, high-impact, diverse team.
DRAW THE OWL: Collaborate with the team to tackle complex problems in distributed computing
BE AN OWNER: Build and operate your team's services in a distributed production environment in the DevOps model; Ensure quality by writing unit, integration, and load tests.
DON’T SETTLE, RUTHLESSLY PRIORITIZE: Constantly look for areas of improvement in processes, system and optimize. Be an advocate for balancing tech debt and feature development
WEAR THE CUSTOMER’S SHOES: Your team will ship reliable features at scale that delight your customers. You will get to know your customers and walk in their shoes. How will you listen to your customers’ challenges, see opportunities, craft solutions, and deliver the right value at the right time?
NO SHENANIGANS: Ensure customer trust and service reliability remain at the forefront of our priorities
WRITE IT DOWN: Craft clear and concise documentation
BE BOLD: Be creative, take your own initiative and solve some of our most challenging problems

The Twilio SendGrid Email Platform is responsible for delivering about 3 billion emails a day. As an engineer on the Email Platform team you will have hands-on impact in building and operating our high-scale email system that continues to grow at a substantial rate year-over-year.

Twilio is a company that is empowering the world’s developers with modern communication in order to build better applications. Twilio is truly unique; we are a company committed to your growth, your learning, your development, and your entire employee experience. We only win when our employees succeed and we're dedicated to helping you develop your strengths. We invest in weeks dedicated to tackling hard problems and creating your own ideas. We have a cultural foundation built on diversity, inclusion, and innovation and we want you and your ideas to thrive at Twilio.

Where?

This position will be located in our Irvine, CA office. Around the world, Twilio offers benefits and perks to support the physical, financial, and emotional well being of you and your loved ones. No matter where you are based, you will experience a company that believes in small teams for maximum impact; seeks well-rounded talent to ensure a full perspective on our customers’ experience, understands that this is a marathon, not a sprint; that continuously and purposefully builds an inclusive culture that empowers everyone to do their best work and be the best version of themselves.

About Us

Millions of developers around the world have used Twilio to unlock the magic of communications to improve any human experience. Twilio has democratized communications channels like voice, text, chat, video and email by virtualizing the world’s communications infrastructure through APIs that are simple enough for any developer to use, yet robust enough to power the world’s most demanding applications. By making communications a part of every software developer’s toolkit, Twilio is enabling innovators across every industry — from emerging leaders to the world’s largest organizations — to reinvent how companies engage with their customers.

Please note this role is open to candidates outside of Colorado as well. The information below is provided for those hired in Colorado only.
If you are a Colorado applicant:
The estimated pay range for this role, based in Colorado, is $132,320 - $181,940

An overview of Twilio’s benefits offered is listed below:

Benefits

At the time of this posting, this role is eligible to participate in the following benefits, which Twilio reserves the right to modify at any time for any reason in accordance with applicable law

Twilio is committed to delivering a comprehensive benefits program that provides support needed for you and your loved ones. It’s likely that you don’t think about benefits every day; however, they are an important component of your total compensation, and we want you to understand the options available to you so that you can make the most of your benefit dollars.

Healthcare Insurance and Leave

Prescription Drug
Dental
Vision
Flexible Spending and Health Savings Accounts
Leave programs for all of life’s moments: maternity, parental/bonding, as well medical leave to care for yourself or a loved one

Financial Benefits

Short and Long Term Disability Insurance
Life and Accidental Death & Dismemberment Insurance
401(k) Retirement Savings Plan with a match

Reimbursement Programs & Stipends

$65 per month work-from-home stipend
Up to $50 per month for wellness expenses and activities
Up to $30 per month to use towards books/eBooks

#LIRemote"
American Family Insurance,"Data Engineer II or III - SQL, Python, Insurance, Cloud (GCP/AWS) - (open to remote)","Wisconsin, United States
Remote","Posted by

David Binder 2nd

Connecting High Tech Talent to Dream Jobs in Data Science/Engineering, Actuarial & Analytics.

Send InMail

At American Family Insurance, we believe people are an organization’s most valuable asset, and their ideas and experiences matter. From our CEO to our agency force, we’re committed to growing a diverse and inclusive culture that empowers innovation that will inspire, protect, and restore our customers’ dreams in ways never imagined.




American Family Insurance is driven by our customers and employees. That’s why we provide more than just a job – we provide opportunity. Whether you’re already part of our team in search of a new challenge or new to our company and ready for what’s next, you’re in the right place. Every dream is a journey that starts with a single step. Start your journey right here. Join our team. Bring your dreams.

Job ID:

R27086 Data Engineer II or III - SQL, Python, Insurance, Cloud (GCP/AWS) - (open to remote) (Open)Compensation may vary based on the job level and your geographic work location.

Compensation Minimum:$103,500Compensation Maximum:$165,700Summary:

The Property data Engineering team is part of our Enterprise Data Office, and this team supports Property data needs for the Enterprise. This team brings data together from our different operating companies, enable data consumption with row and column level security and serves multiple functions within the Enterprise and operating companies. The work primarily falls in two buckets: adding new data at raw & conformed layer, maintain and improve current pipelines, enhancing the efficiency of them to better serve the needs of the business and be able to respond to question from business partners. Top candidates will have great passion for data, deep understanding of driving business value from data and demonstrated experience in engineering data using programming language like Python, extensively used SQL and GCP skills a plus.




Job Family Summary

Determines and builds the technical solution(s) to allow unstructured data to be structured and used by Data Scientists. Seeks to understand the data being worked with as its often unstructured data sets. Often are data gurus who prepare data for all stages of the modeling process including exploration, training, testing, and deployment.




Job Profile Summary

As a Data Engineer, you’ll work on collecting, storing, processing and building Business Intelligence and Analytics applications within our big data platform. Presently, our team is constructing an enterprise data lake to enable analysts and scientists to self-service data at scale across American Family’s operating companies. We’re leveraging technologies like Python, Big Query, Composer and other cloud native tools to curate high-quality data sets. You’ll also be responsible for integrating these applications with the architecture used across the organization. Adjacent responsibilities include establishing best practices with respect to data integration, data governance, data visualization, schema design, performance and reliability of data processing systems, supporting data quality, and enabling convenient access to data for our scientists and business users.




Job Description:

Job Level Summary




Requires specialized depth and/or breadth of expertise in own job discipline or field
Leads others to solve complex problems
Works independently, with guidance in only the most complex situations
May lead functional teams or projects

Primary Accountabilities




Perform exploratory data analysis to determine which questions can be answered effectively with a given dataset. Ability to analyze new (possibly unstructured) data sources to determine what additional value they may bring and how to effectively make use of them.
Design and develop highly scalable and extensible data pipelines from internal and external sources.
Work on cross-functional teams to design, develop, and deploy data-driven applications and products, particularly within the space of data science.
Lead in prototyping emerging technologies involving data ingestion and transformation, distributed file systems, databases and frameworks.
Work with our data science team on applying improvements to their machine learning algorithms and platforms.
Design, build, and maintain tools to increase the productivity of application development and client facing teams.
Partner with business analyst to define, develop, and automate data quality checks.
Review developed solutions to solve specific business problems. Optimize queries, data models, and storage formats to support common usage patterns.
Design and develop big data applications and data visualization tools.
Code structure moves beyond procedural ad-hoc workflows and exhibits modularity and comprehensive test cases. Develops code at a high-level of abstraction – writes, maintains, and reuses utilities/libraries across projects.
Influences strategy related to processes and workflows across their division.
Participates in mentoring junior colleagues.

Travel Requirements




This position requires travel up to 10% of the time.

Education and Licenses




Bachelor’s degree in computer science or related field, or equivalent combination of education and experience.

Specialized Knowledge & Skills Requirements




In-depth knowledge of SQL or NoSQL and experience using a variety of data stores (e.g. RDBMS, analytic database, scalable document stores)
Extensive hands-on Python programming experience, with an emphasis towards building ETL workflows and data-driven solutions. Able to employ design patterns and generalize code to address common use cases. Capable of authoring robust, high quality, reusable code and contributing to the division’s inventory of libraries.
Expertise in big data batch computing tools with demonstrated experience developing distributed data processing solutions.
Applied knowledge of cloud computing (GCP,AWS,Azure).
Solid data understanding and business acumen in the data rich industries like insurance or financial
Applied knowledge of data modeling principles (e.g. dimensional modeling and star schemas).
Strong understanding of database internals, such as indexes, binary logging, and transactions.
Experience using tools for infrastructure-as-code (e.g. Terraform, Docker, CloudFormation, etc.)
Experience with software engineering tools and workflows (i.e. GitLab, Jenkins, CI/CD).
Practical experience authoring and consuming web services.

Additional Job Information:




There are multiple positions available.
Candidates may be considered at different levels depending on qualifications.
Offer to selected candidate will be made contingent on the results of applicable background checks.
Offer to selected candidate is contingent on signing a non-disclosure agreement for proprietary information, trade secrets, and inventions.
We are open to hiring candidates to work remotely (anywhere in the USA).

#LI-Remote

When you work at American Family you can expect benefits that support your physical, emotional, and financial wellbeing. You will have access to comprehensive medical, dental, vision and wellbeing benefits that enable you to take care of your health. We also offer a competitive 401(k) contribution, a pension plan, an annual incentive, and a paid-time off program. In addition, our student loan repayment program and paid-family leave are available to support our employees and their families. Interns and contingent workers are not eligible for American Family Enterprise benefits.

We also consider qualified applicants with criminal histories, consistent with applicable federal, state and local law.

Stay connected: Join Our Enterprise Talent Community!"
Edward Jones,"Data Engineer, Technology","Los Angeles, CA
Remote","60717BR

Job-Overview

At Edward Jones we are developing a next-generation analytics architecture to support our growing business. As part of the Data Architecture and Data Design team, you will be challenged daily to find innovative solutions to improve the speed and consumption of our information. This will also bring you into contact with a diverse array of stakeholders ranging from IT to our front line business organizations, and you will be working on a data platform that establishes the foundational layer for Edward Jones analytics for years to come.

Responsibility Summary

Analyze data requirement and mapping documents to determine the best solution for data onboarding
Work across stakeholders including data stewards, business analysts, and data modelers to clarify and confirm data requirements
Design and implement the ODS data platform required for optimal extraction, transformation, and loading (ETL) of data from a wide variety of data sources
Create and maintain optimal data pipeline architecture capable of ingesting structured and unstructured data in near real-time
Assemble large, complex data sets that meet functional and technical requirements
Identify, design, and implement internal process improvements: Automating manual processes, optimizing data delivery, redesigning infrastructure for greater scalability, etc.
Develop data flows that can leverage both on premise and cloud architectures
Build KPIs that utilize the data pipeline to provide insights into user adoption, operational efficiency and other key business performance metrics
Implement data control and exception handling including data reconciliation
Work with stakeholders including the business analytics teams and IS architecture teams to assist with data-related technical issues and support their data infrastructure needs

Skills-Requirements

Requirements:
Advanced working SQL knowledge and experience working with relational databases, query authoring (SQL), as well as experience with complex Stored Procedures for data processing.
Experience building and optimizing data pipelines, architectures and datasets
Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement
Build processes supporting data transformation, data structures, metadata, dependency and workload management
A successful history of manipulating, processing and extracting value from large disconnected datasets
Working knowledge of message queuing, stream processing, and highly scalable ‘big data’ data stores
Experience supporting and working with cross-functional teams in a dynamic environment
5+ years' experience building large scale operational data stores in Oracle or equivalent relational database
Understanding of data domains such as client, account, investments, transactions, holdings etc. are highly desired
Candidates should also have 3+ years of experience using the following software/tools:
Hadoop, Spark, Kafka, etc.
Experience with relational SQL and NoSQL databases, including Oracle, MongoDB, or Azure SQL
Experience with AWS/Azure cloud services
Experience with stream-processing systems: Kafka, Spark-Streaming, etc.
Experience with object-oriented/object function scripting languages: Python, Java, etc.
Experience with IICS (Informatica Intelligent Cloud Services) or similar ETL tools

EEO

Edward Jones does not discriminate on the basis of race, color, gender, religion, national origin, age, disability, sexual orientation, pregnancy, veteran status, genetic information or any other basis prohibited by applicable law."
Amazon Web Services (AWS),Sr. Data & ML Engineer,"San Diego, CA","Description

At Amazon Web Services (AWS), we’re hiring highly technical Data and Machine Learning engineers to collaborate with our customers and partners on key engagements. Our consultants will develop and deliver proof-of-concept projects, technical workshops, and support implementation projects. These professional services engagements will focus on customer solutions such as Machine Learning, Data and Analytics, HPC and more.

In this role, you will work with our partners, customers and focus on our AWS offerings such Amazon Kinesis, AWS Glue, Amazon Redshift, Amazon EMR, Amazon Athena, Amazon SageMaker and more. You will help our customers and partners to remove the constraints that prevent them from leveraging their data to develop business insights.

AWS Professional Services engage in a wide variety of projects for customers and partners, providing collective experience from across the AWS customer base and are obsessed about customer success. Our team collaborates across the entire AWS organization to bring access to product and service teams, to get the right solution delivered and drive feature innovation based upon customer needs.

You will also have the opportunity to create white papers, writing blogs, build demos and other reusable collateral that can be used by our customers. Most importantly, you will work closely with our Solution Architects, Data Scientists and Service Engineering teams.

The ideal candidate will have extensive experience with design, development and operations that leverages deep knowledge in the use of services like Amazon Kinesis, Apache Kafka, Apache Spark, Amazon Sagemaker, Amazon EMR, NoSQL technologies and other 3rd parties.

Inclusive Team Culture

Here at AWS, we embrace our differences. We are committed to furthering our culture of inclusion. We have thirteen employee-led affinity groups, reaching 85,000 employees in over 190 chapters globally. We have innovative benefit offerings, and host annual and ongoing learning experiences, including our Conversations on Race and Ethnicity (CORE) and AmazeCon (gender diversity) conferences. Amazon’s culture of inclusion is reinforced within our 16 Leadership Principles, which remind team members to seek diverse perspectives, learn and be curious, and earn trust.

Work/Life Balance

Our team puts a high value on work-life harmony. Striking a healthy balance between your personal and professional life is crucial to your happiness and success here. We are a customer-obsessed organization—leaders start with the customer and work backwards. They work vigorously to earn and keep customer trust. As such, this is a customer facing role in a hybrid delivery model. Project engagements include remote delivery methods and onsite engagement that will include travel to customer locations as needed.

Mentorship & Career Growth

Our team is dedicated to supporting new members. We have a broad mix of experience levels and tenures, and we’re building an environment that celebrates knowledge sharing and mentorship. We care about your career growth and strive to assign projects based on what will help each team member develop into a better-rounded professional and enable them to take on more complex tasks in the future.

This is a customer facing role. You will be required to travel to client locations and deliver professional services when needed.


Basic Qualifications

Bachelor’s degree in Computer Science, Engineering, Mathematics or a related field or equivalent professional or military experience
8+ years of experience of Data platform implementation
3+ years of hands-on experience in implementation and performance tuning of Kinesis, Kafka, Spark or similar implementations
Hands on experience with building data or machine learning pipeline
Experience with one or more relevant tools (Flink, Spark, Sqoop, Flume, Kafka, Amazon Kinesis)
Experience developing software code in one or more programming languages (Java, JavaScript, Python, etc)
Current experience with hands-on implementation

Preferred Qualifications

Masters or PhD in Computer Science, Physics, Engineering or Math.
Familiar with Machine learning concepts
Hands on experience working on large-scale data science/data analytics projects
Hands-on experience with technologies such as AWS, Hadoop, Spark, Spark SQL, MLib or Storm/Samza.
Experience Implementing AWS services in a variety of distributed computing, enterprise environments.
Experience with at least one of the modern distributed Machine Learning and Deep Learning frameworks such as TensorFlow, PyTorch, MxNet Caffe, and Keras.
Experience building large-scale machine-learning infrastructure that have been successfully delivered to customers.
Experience defining system architectures and exploring technical feasibility trade-offs.
3+ years experiences developing cloud software services and an understanding of design for scalability, performance and reliability.
Ability to prototype and evaluate applications and interaction methodologies.
Experience with AWS technology stack.
Written and verbal technical communication skills with an ability to present complex technical information in a clear and concise manner to a variety of audiences.

Amazon is committed to a diverse and inclusive workplace. Amazon is an equal opportunity employer and does not discriminate on the basis of race, national origin, gender, gender identity, sexual orientation, protected veteran status, disability, age, or other legally protected status. For individuals with disabilities who would like to request an accommodation, please visit https://www.amazon.jobs/en/disability/us.


Company - Amazon Web Services, Inc.

Job ID: A1414205"
Dice,Data Engineer,"New York, NY
On-site","Dice is the leading career destination for tech experts at every stage of their careers. Our client, Ascendex, is seeking the following. Apply via Dice today!

Data Engineer Overview A leading cryptocurrency financial platform built by a group of Wall Street quant trading veterans is seeking an experienced Data Engineer to join our New York office. As a member of our data engineering team, yoursquoll shape the way we approach data at AscendEx by using your engineering, analytical and communication skills to work with teams across the business. You are passionate about challenging data projects and tasks. You are excited to work with a group of data engineers and analysts and use data to contribute to our business decisions. About AscendEX AscendEX is a global cryptocurrency exchange with a comprehensive product suite including spot, margin, and futures trading, wallet services, and staking support for over 150 blockchain projects such as bitcoin, ether, and ripple. Launched in 2018, AscendEX services over 1 million retail and institutional clients globally with a highly liquid trading platform and secure custody solutions. AscendEX has emerged as a leading platform by ROI on its ldquoinitial exchange offeringsrdquo by supporting some of the industryrsquos most innovative projects from the DeFi ecosystem, such as Thorchain, xDai Stake, and Serum. AscendEX users receive exclusive access to token airdrops and the ability to purchase tokens at the earliest possible stage. To learn more about how AscendEX is leveraging best practices from both Wall Street and the cryptocurrency ecosystem to bring the best altcoins to its users, please visit www.AscendEX.com. Key Responsibilities Build and maintain data dictionary and process documentation Develop and tune SQL objects, queries and store procedures Work closely with business team, product team and account team to build reporting solutions Create test plans for new product and perform data validation Partner with Core Engineering team to perform root cause analysis, resolve production and data issues Cooperate with engineers, project managers and business leaders to deliver insights to the business Research new data technologies Design, build and develop more sustainable, reliable and scalable data pipline Education and Skills Bachelor's degree in computer science, engineering, or related field required 1+ years experience in data engineering or related data warehouse technologies 1+ years experience in designing, developing and maintaining ETL process Advanced skill with python is a must Advanced skill with SQL is a must Strong problem solving skills Experience with one or more databases (RDS, Aurora, Redshift, etc) are a plus Experience with one or more project management tools (Jira, Clickup, Jenkings, etc) are a plus Professional demeanor and communication skills with ability to work in rapidly growing, pressurized environment Dedicated team player Knowledge of or fluent in Mandarin a plus Prior experience in cryptocurrency highly desired Equal Employment Opportunity AscendEX provides equal employment opportunities to all employees and applicants for employment and prohibits discrimination and harassment of any type without regard to race, color, religion, age, sex, national origin, disability status, genetics, protected veteran status, sexual orientation, gender identity or expression, or any other characteristic protected by federal, state or local laws."
Dice,Data Engineer,"Redmond, WA
On-site","Dice is the leading career destination for tech experts at every stage of their careers. Our client, ENFEC, is seeking the following. Apply via Dice today!

JD for Role Azure Data Engineer As a Data Engineer on the Data Science and Platform team, you will Develop and maintain data pipelines including solutions for data collection, management and usage. Work closely with Data Scientists to optimize and reengineer model code to be modular, efficient and scalable, and to deploy models to production. Maintain and enhance our data and computation platform. Develop and implement solutions for data quality validation and continuous improvement. Manage, execute and monitor weekly and monthly production operations resolve and escalate production issues as appropriate. Partner with business stakeholders to understand business and technical requirements, plan and execute projects, and communicate status, risks and issues. Perform root cause analysis of system and data issues and develop solutions as required. Qualifications Required BABS in business, computer science or other technical field or 5+ years of appropriate work experience Proficient in Azure SQL Data Warehouse, T-SQL and database architecture. Implement data warehouses and analysis services as part of a modern data warehouse (MDW) design Experience working with cloud-based technologies, including relational databases, data warehouse, big data (Spark), orchestrationdata pipeline tools, data frames concepts, Spark Streaming and Spark SQL Experience with Azure Analytics stack, Azure Data Lake, Azure Databricks, Azure Data Factory, Azure Cloud, Kafta. Experience with ETL data pipelining concept. Proficient in Power BI Experience working in software engineering, and can demonstrate best practices for project management, quality control, and product development. Proven track record of collaborative development in an agile team environment. Experience with Python, Scala. Expertise in Pyspark or Scala is a must. Hands on experience of writing notebooks in Databricks using python for complex data aggregations, transformations, schema operations."
Rapid7,Data Engineer,"Boston, MA","Rapid7 (Nasdaq: RPD) is advancing security with visibility, analytics, and automation delivered through our Insight cloud. Our solutions simplify the complex, allowing security teams to work more effectively with IT and development to reduce vulnerabilities, monitor for malicious behavior, investigate and shut down attacks, and automate routine tasks. Over 9,300 customers rely on Rapid7 technology, services, and research to improve security outcomes and securely advance their organization. For more information, visit our website, check out our blog, or follow us on LinkedIn.

The Opportunity

Rapid7 seeks a Data Engineer to build and maintain data infrastructure within Rapid7's Data Engineering team's data platform. You will be responsible for deploying data pipelines and machine learning models in the cloud, implementing DevOps practices and developing data models within Snowflake. You will assist in architecting a new modern architecture working closely with cloud based tools such as Fivetran, Snowflake, and Airflow. The ability to conceptualize and create user-friendly self-service solutions is critical to be successful in this role. Our business is evolving quickly and we need you to think long term, but deliver incrementally.

The ideal candidate has hands-on experience performing Data Engineering and/or DevOps work in a cloud environment, and has worked closely with databases and data pipelines. It's critical that you are able to translate business objectives into data required to support key analyses. You will collaborate with a creative, analytical and data-driven team to bring a single source of truth and self-service analytics to the entire company.

In The Role You Will

Build and maintain the pipelines and applications that ingest, analyze and store Rapid7's enterprise data
Lead the entire software lifecycle including hands-on development, code reviews, testing, deployment, and documentation for batch ETL's.
Productionize data and machine learning pipelines with docker containerization and clustering tools (ECS/Kubernetes)
Build an environment that enables data scientists to easily develop and productionize Python, R and Spark code on top of a Snowflake data warehouse
Perform data engineering projects within Snowflake such as developing data pipelines, data models and metadata management solutions
Collaborate with stakeholders in product, business and IT to deliver data products
Work closely with leadership to drive adoption of the latest DevOps and DataOps trends and technologies.
Partner with the IT, Infrastructure and engineering teams on integration efforts between systems that impact data & Analytics

In Return You Will Bring

1+ years of experience with a major cloud provider (preferably AWS) including hands on experience deploying code in cloud environments using tools such as Docker, Kubernetes, EC2, Terraform, etc
More than 1 year of experience working with a modern cloud data warehouse (preferably Snowflake) and SQL
More than 1 year of experience with orchestration tooling (preferably Airflow)
Proficiency in one programming language such as Python, Java, Scala etc.
Experience with a CI/CD tool such as Github Actions and AWS Code Pipeline
Working knowledge of data architecture, data warehousing, and metadata management
BS or MS in Computer Science, Analytics, Statistics, Informatics, Information Systems or another quantitative field. or equivalent experience and certifications will be considered"
Meta,"Data Engineer, Analytics","Los Angeles, CA","Every month, billions of people leverage Meta products to connect with friends and loved ones from across the world. On the Data Engineering Team, our mission is to support these products both internally and externally by delivering the best data foundation that drives impact through informed decision making. As a highly collaborative organization, our data engineers work cross-functionally with software engineering, data science, and product management to optimize growth, strategy, and experience for our 3 billion plus users, as well as our internal employee community. In this role, you will see a direct correlation between your work, company growth, and user satisfaction. Beyond this, you will work with some of the brightest minds in the industry, and you'll have a unique opportunity to solve some of the most interesting data challenges with efficiency and integrity, at a scale few companies can match. As we continue to expand and create, we have a lot of exciting work ahead of us!

Data Engineer, Analytics Responsibilities:


Conceptualize and own the data architecture for multiple large-scale projects, while evaluating design and operational cost-benefit tradeoffs within systems
Create and contribute to frameworks that improve the efficacy of logging data, while working with data infrastructure to triage issues and resolve
Collaborate with engineers, product managers, and data scientists to understand data needs, representing key data insights in a meaningful way
Define and manage SLA for all data sets in allocated areas of ownership
Determine and implement the security model based on privacy requirements, confirm safeguards are followed, address data quality issues, and evolve governance processes within allocated areas of ownership
Design, build, and launch collections of sophisticated data models and visualizations that support multiple use cases across different products or domains
Solve our most challenging data integration problems, utilizing optimal ETL patterns, frameworks, query techniques, sourcing from structured and unstructured data sources
Assist in owning existing processes running in production, optimizing complex code through advanced algorithmic concepts
Optimize pipelines, dashboards, frameworks, and systems to facilitate easier development of data artifacts
Influence product and cross-functional teams to identify data opportunities to drive impact
Mentor team members by giving/receiving actionable feedback


Minimum Qualifications:


Bachelor's degree in Computer Science, Computer Engineering, relevant technical field, or equivalent practical experience.
4+ years of work experience in data engineering (a minimum of 2+ years with a Ph.D)
Experience with SQL, ETL, data modeling, and at least one programming language (e.g., Python, C++, C#, Scala, etc.)


Preferred Qualifications:


Master's or Ph.D degree in a STEM field
Experience with one or more of the following: data processing automation, data quality, data warehousing, data governance, business intelligence, data visualization, data privacy
Experience working with terabyte to petabyte scale data


Facebook is proud to be an Equal Opportunity and Affirmative Action employer. We do not discriminate based upon race, religion, color, national origin, sex (including pregnancy, childbirth, or related medical conditions), sexual orientation, gender, gender identity, gender expression, transgender status, sexual stereotypes, age, status as a protected veteran, status as an individual with a disability, or other applicable legally protected characteristics. We also consider qualified applicants with criminal histories, consistent with applicable federal, state and local law.Facebook is committed to providing reasonable accommodations for candidates with disabilities in our recruiting process. If you need any assistance or accommodations due to a disability, please let us know at accommodations-ext@fb.com."
Allied World,Data Engineer,"New York, NY","

Job Objectives:

Allied World is seeking an individual to contribute integration efforts of underwriting and claims platforms with our corporate Enterprise Data Warehouse (EDW). The successful candidate will work within our Agile environment as part of a Scrum team to develop new, and support pre-existing Extract, Transform & Load (ETL) processes. We look for people who can work effectively within a team as well as take the lead on new initiatives.

Duties and Responsibilities:


Work with EDW & Architecture groups to define and implement integration of new systems and processes.
Participate in peer code reviews and adhere to best practice development methodologies.
Assemble large, complex data sets that meet functional / non-functional business requirements
Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.


Desired Skills and Experience


Requirements:


Must be a self-starter with a great attitude, not afraid to take the initiative, and work with little supervision
Preference given for holders of a University Degree or Diploma in Computer Science or related field.
Preferred experience working within an Agile – Scrum/Kanban environment
Minimum 5 years ETL experience, real world experience with SSIS
Minimum 5 years of experience with SQL server
Extensive knowledge and experience using concepts including but not limited to
Stored Procedures/TSQL
Query Performance Tuning
Data Warehousing & OLAP
Source control tools & concepts (CI/CD, Git, Bitbucket, Github)
Practical experience with property & casualty insurance/reinsurance and multi-currency systems is an asset
Ability to analyze and write business and technical specifications and interact with our business users effectively
Experience with AWS-based data services will be plus: Athena, Glue, RedShift, RDS, S3, etc.
Ability to work independently while understanding the necessity for communicating and coordinating work efforts with other employees and organizations



Our Business

Allied World Assurance Company Holdings, Ltd, through its subsidiaries, is a global provider of insurance and reinsurance solutions. We operate under the brand Allied World and have supported clients, cedents and trading partners with thoughtful service and meaningful coverages since 2001. We are a subsidiary of Fairfax Financial Holdings, Limited and benefit from a strong capital base and a worldwide network of affiliated entities that allow us to think and respond in non-traditional ways.

Our generous benefits package includes: Health, Dental and Disability Insurance, a company match 401k plan, and Group Term Life Insurance. Allied World is an Equal Opportunity and Affirmative Action Employer. All qualified applicants will be considered for employment without regard to an individual's race, color, national origin, religion or belief, sex, gender, gender identity, age, genetic information, marital or civil partnership status, family status, sexual orientation , military, veteran status, disability or any other characteristic protected by law.

To learn more, visit awac.com, or follow us on Facebook at facebook.com/alliedworld and LinkedIn at linkedin.com/company/allied-world.

"
Cognite,Data Engineer,"Austin, TX
On-site","Do you see how data can be used, modeled and visualized in new ways to improve decisions in industrial engineering, but you experience that the tools and data availability is insufficient to create impact? If you want to change that, and take part in forming what the future of the industry will look like you should join our Cognite and become a part of the team responsible for delivering Cognite’s cutting edge industry solutions to our customers!

As a Data Engineer, located in Austin, TX, you will design, develop and implement data infrastructure and best-in-class pipelines that collect, connect, centralize and curate data from various internal and external data sources. You will ensure that architectures support the needs of the business, and recommend ways to improve data reliability, efficiency. You are an experienced engineer with a passion for software development, hands-on in designing, implementing, and delivering features for flagship products.

What You'll Do

Partner with Solution Architects to understand client requirements and define queries with subject matter experts
Develop custom extractors using backend technologies and languages i.e Python, Spark, Rest APIs
Customize existing extractors i.e. database extractor using SQL, event streaming using Kafka and deploy using Docker
Create custom data models for data discovery, mapping, and cleansing
Collaborate with product development to turn customer needs into potential product offerings
Prototype data visualization and dashboards

Who You Are

3+ years of experience in a Data intense role
Experience in O&G, Power & Utilities and/or Manufacturing is required
BS or MS degree in computer science or related field
Loves to code, passion for coding, and enjoys sharing that knowledge with others
Strong understanding of data analysis or data science
Experience working with data technologies, such as: ETL, SQL, Python
Ability to work on both internal and external client-facing projects and communicate with key stakeholders
Ability to travel onsite to meet with and engage with clients -- we don’t build solutions in isolation.
Role based in Houston, TX or (Austin, TX w/ travel to Houston)

What Makes Us Great

An opportunity to make an impact on the industrial future and be part of disruptive and groundbreaking global projects
High level of autonomy, ability to influence decisions and to learn from mistakes
Work along side a driven, engaging team with in-depth software expertise and industry experience
Opportunity to join Together@Cognite for social, community, and diversity initiatives
Focus on agility and speed, openness, togetherness, impact, and obligation to speak up
Join a team that truly lives their values and brings their whole selves to Cognite --> watch some of our Cognite Voices Katrine Tjølsen , Petter Reistad .

Perks & Benefits

Competitive Compensation + 401(k) with employer matching
Health, Dental, Vision & Disability Coverages with premiums fully covered for employees and all dependents
Unlimited PTO + flexibility to enjoy it
Paid Parental Leave Program
Learning & Development Stipends
Global Mobility & Exchange Program
Company Paid Friday Lunch via DoorDash + Fully Stocked Fridges

Cognite is a global industrial SaaS company that was established with one clear vision: to rapidly empower industrial companies with contextualized, trustworthy, and accessible data to help drive the full-scale digital transformation of asset-heavy industries around the world. Our core Industrial DataOps platform, Cognite Data Fusion™, enables industrial data and domain users to collaborate quickly and safely to develop, operationalize, and scale industrial AI solutions and applications to deliver both profitability and sustainability. Visit us at www.cognite.com and follow us on Twitter @CogniteData or LinkedIn: https://www.linkedin.com/company/cognitedata

Equal Opportunity

Cognite is committed to creating a diverse and inclusive environment at work and is proud to be an equal opportunity employer. All qualified applicants will receive the same level of consideration for employment; everyone we hire will receive the same level of consideration for training, compensation, and promotion.

We ask for gender as part of our application because we want to ensure equal assessment in the recruitment process. Your answer will help us reach this commitment! However, the question about gender is optional and your choice not to answer will not affect the assessment of your application in any way."
Brightline,Senior Data Engineer,"San Francisco Bay Area
Remote","Who We Are and What We’re About:

Brightline is on a mission to build a bright future for every child, bringing extraordinary behavioral health care to children and families.




We’re in the middle of a behavioral health crisis that we can’t ignore—especially not when it comes to our children. 1 in every 5 children has a behavioral health condition, yet up to 80% still get little or no help. Far too many families face insurmountable barriers when their child really needs help, and parents are too often left feeling alone and without the support they need. We can do so much better. That’s why we’re here!




When you join Brightline, you’re joining a team that has an unstoppable drive to change the lives of families across the country. We’re a team of thoughtful problem-solvers, expert and kind clinicians, and experienced operators and technologists. Above all, we’re committed to helping each other succeed, learn, and grow—all while bringing care to families when, where, and how they need it.




No matter what you’re looking for in your next role, we’re pretty confident that you’ll #FindItAtBrightline.




As an early Data Engineer at Brightline, here's where you’ll make an impact:

Lead data architecture strategy in order to meet company, client, and member information needs
Collaborate with tech leadership on data integrity for our key product and company health KPIs
Partner closely with Data Science and other engineering teams to build world class infrastructure that empowers data customers from around the company to confidently build accurate dashboards, run data research projects, instrument features, and support outcome studies
Define and iterate on our data models and pipelining to best support engineering, product, and business goals




About you:

Have 3+ years of data engineering experience
Are curious to learn new things and also exercise judgment about when to use well-established patterns and tools
Are a self-starter and see projects through from inception to completion with minimal oversight, utilizing communication and collaboration as enablers
Care deeply about product experience and quality
Have experience and enjoy working cross-functionally and partnering with stakeholders across departments of varying levels
Have experience driving technical ideas and communicating clearly with technical as well as non-technical audiences, at all levels of details
Have experience in analytics engineering such as designing and implementing ETL, BI, reverse ETL, DW, compute (spark, dask, etc)




Benefits + Perks:

Remote first and focused on community—we have Bright Spots with employees across the country (check out more info below) + generous work-from-home stipend
Competitive compensation & benefits packages because we know the work you do is hard and we recognize how valuable you are, including an unlimited PTO policy + 14 paid holidays, Health, Vision, Dental, 401k, and stock options
Collaborate with diverse members across teams with weekly group learning opportunities and team breakfasts (and offsite retreats in the future!)
Dedicated time for your health and wellness, including group workouts and meditations hosted by Brightline employees
With your help, we’ll build diverse & equitable programs and experiences




Brightline is a nationally distributed team with many of our team members located throughout the country, some of those being: New York, Boston, Seattle, Portland, Atlanta, Houston, Denver, Minneapolis, San Diego, and Los Angeles. The Brightline office is headquartered in the SF Bay Area, but we consider all local and remote (U.S.) candidates.

At Brightline, we believe that collaborative, diverse, and empathetic tech & product teams can create transformational products to change the lives of families across the country. In a conscious effort to create a diverse and inclusive environment for our employees and the families we serve, we celebrate our individual differences and walks of life. Brightline is an equal opportunity employer and encourages all applicants from every background and life experience to apply.




#li-hybrid #li-remote"
"KellyMitchell Group, Inc",Data Engineer,"United States
Remote","KellyMitchell matches the best IT and business talent with premier organizations nationwide. Our clients, ranging from Fortune 500 corporations to rapidly growing high-tech companies, are exceptionally served by our 1500+ IT and business consultants. Our industry is growing rapidly, and now is a great time to launch your career with the KellyMitchell team.




Data Engineer




Duties:

Use analytics to identify opportunities in dev processes. For example: running natural language clustering on Azure Dev Ops bug reports, community reports, forum, and support tickets. Augment data sets leveraging behavioral and non-behavior data.
Dashboarding and building of KPIs, specifically using PowerBI and Azure Monitors, Azure Data Explorer and Jarvis preferred.
Leverages deep insights to make sure we are automating the right measures and KPIs.
Work with development teams to ensure the right prioritization of bugs. Being able to understand player impact.
Excellent project management and communication. Will closely partner with the platform team.
Tools: SQL, Databricks, Natural Language Processing, statistics, Python.




Desired Skills/Experience:

3+ years of experience in data science role.
Strong understanding of software engineering systems required.
Experience analyzing data using Python, R, or other statistical languages.
Proficient with SQL-like languages - Hive, Spark, or Databricks experience is a bonus.
Experience using and evaluating machine learning models on real-world data.
Experience building dashboards and reports using tools like PowerBI, Azure Monitors, Azure Data Explorer, or Jarvis
Excellent project management and communication skills, both written and verbal.
Deliver business impact on ambiguous projects with incomplete or dirty data.
Able to work on a collaborative and diverse team."
Navio Inc,Data Engineer / Data Science SME for Data Pathway Curriculum Design,"San Francisco Bay Area
Remote","Job Title- Data Engineer / Data Science SME for Data Pathway Curriculum Design




Job Type- Contract W2 OR C2C




Job Location- Remote




Salary Range- $80 to $100/hour (Salary range is depend upon the experience level)




Client- Cisco




A subject matter expert (SME) is needed in Big Data with experience in data engineering, data science or deep experience as a data analyst to help define a learning curriculum for workforce development in Data.

Responsibilities will include:

Completing / helping to complete a market scan and analysis of existing curricula in market to help inform differentiation.
Analysis of existing organizational courseware in big data and analytics.
Competency mapping for a data pathway leading to certification that is aligned with globally recognized job frameworks.
Definition of competencies associated with the data learning pathway.
Definition of appropriate modules and courses for learners, leading to certification.
Recommendation of how data competencies may be used in other technical pathways such as networking, cybersecurity, cloud, IoT and other topics.
Recommendations on how to incorporate best in class online labs, practice, and simulation(s).
Recommendations for production roadmap in collaboration with product management to obtain the largest impact in shortest time.
Collaborate with instructional designers, learning developers and other members of the learning engineering team to develop the pathway courseware including all materials.

Experience:

Demonstrated experience and knowledge in subject area.
Experience in curriculum design and development preferred.


"
Dice,Lead Data Engineer,"San Francisco, CA
On-site","Dice is the leading career destination for tech experts at every stage of their careers. Our client, Salesforce, is seeking the following. Apply via Dice today!

Salesforce.com seeks a Lead Data Engineer at our San Francisco, CA office located at 415 Mission Street, 94105. The individual could also be placed at 123 Mission Street, 350 Mission Street, and 50 Fremont Street. The permanent position may be offered at any of these locations in San Francisco. 1 position is available on this team. Job Duties Own technical solution design and work on the technical architecture and implementation of data acquisition and integration projects, both batch and real time. Define the overall solution architecture needed to implement a layered data stack that ensures a high level of data quality and timely insights. Craft technical solutions and assemble design artifacts, including functional design documents, data flow diagrams, and data models, and communicate with product owners and analysts to clarify requirements. Build data pipelines, data processing tools, and data technologies in open source and proprietary products like Oracle, Hadoop, Pig, Hive, HBase, Spark, Salesforce API, and Talend big data fabric. Serve as a subject matter expert and mentor for Hadoop ecosystem products, ETL design, and other related big data and programming technologies. Identify performance and data quality problems and drive the team to remediate them, working to identify incomplete data, improve the overall quality of data, and integrate data from several data sources. Advocate architectural and code improvements to the team to improve execution speed and reliability. Design and develop tailored data structures in database and Hadoop and create functioning ETL prototypes to assemble production-ready data flows in order to address quickly changing business needs. Support Data Science research by designing, developing, and maintaining all parts of the Big Data pipeline for reporting, statistical and machine learning, and computational requirements. Perform data profiling, complex sampling, statistical testing, and testing of reliability on data. Articulate pros and cons of various technologies and platforms in open source and proprietary products and execute proof of concept on new technology and tools to help the organization pick the best tools and solutions. Minimum Requirements Masterrsquos degree (or its foreign degree equivalent) in Computer Science, Engineering (any field), or a related quantitative discipline, and six (6) years of experience in the field of software engineering or systems analysis or six (6) years of experience in the job offered OR Bachelorrsquos degree (or its foreign degree equivalent) in Computer Science, Engineering (any field), or a related quantitative discipline, and eight (8) years of progressively responsible experience in the field of software engineering or systems analysis or eight (8) years of experience in the job offered. Training will not be provided to applicants. Special Skill Requirements (1) Big data processing (2-3 years) (2) Big data storage (2-3 years) (3) big data processing design patterns (2-3 years) (4) data modeling (2-3 years) (5) Hadoop (2-3 years) (6) Spark (2-3 years) (7) Pyspark (2-3 years) (8) SQL (2-3 years) (9) Git or Stash (2-3 years). Any suitable combination of education, training andor experience is acceptable. Education, experience and criminal background checks will be conducted. Salary 219,975.00 per annum. Hours 40 hours per week M-F, 900 a.m. to 500 p.m. Interested applicants should send resumes to the following address Recruitment and Employment Office, SALESFORCE.COM, Attn Job Ref SAL33531, P.O. Box 56625, Atlanta, GA 30343."
Nike,Data Engineer,"Beaverton, OR","Nike has embraced big data technologies to enable data-driven decisions. We’re looking to expand our Data Engineering team to keep pace. As a Data Engineer, you will work with a variety of talented Nike teammates and be a driving force for building first-class solutions for Nike Technology and its business partners, working on development projects related to supply chain, commerce, consumer behavior and web analytics among others.

Role responsibilities:

Chip in to Design and implement data products and features in collaboration with product owners, data analysts, and business partners using Agile / Scrum methodology
Contribute to overall architecture, frameworks and patterns for processing and storing large data volumes
Contribute to evaluation of new technologies/tools/frameworks centered around high-volume data processing
Translate product backlog items into logical units of work in engineering
Implement distributed data processing pipelines using tools and languages prevalent in the big data ecosystem
Build utilities, user defined functions, libraries, and frameworks to better enable data flow patterns
Work with engineering leads and other teams to ensure quality solutions are implemented, and engineering best practices are defined and followed
Build and incorporate automated unit tests and participate in integration testing efforts
Utilize and advance continuous integration and deployment frameworks
Troubleshoot data issues and perform root cause analysis
Work across teams to resolve operational & performance issues


The following qualifications and technical skills will position you well for this role:

Bachelor’s degree or higher in Computer Science, or related technical discipline; or equivalent combination of education and experience and training
2+ years of experience in large-scale software development, 1+ years of big data experience
Programming experience, Python or Scala preferred
Experience working with Hadoop and related processing frameworks such as Spark, Hive, etc
Experience with messaging/streaming/complex event processing tooling and frameworks
Experience with data warehousing concepts, SQL and SQL Analytical functions
Experience with workflow orchestration tools like Apache Airflow
Experience with source code control tools like Github or Bitbucket
Ability to communicate effectively, both verbally and written, with team members
Interest in and ability to quickly pick up new languages, technologies, and frameworks
Experience in Agile/Scrum application development


The following skills and experience are also relevant to our overall environment, and nice to have:

Experience with Java
Experience working in a public cloud environment, particularly AWS
Experience with cloud warehouse tools like Snowflake
Experience working with NoSQL data stores such as HBase, DynamoDB, etc
Experience building RESTful API’s to enable data consumption
Experience with build tools such as Terraform or CloudFormation and automation tools such as Jenkins or Circle CI
Experience with practices like Continuous Development, Continuous Integration and Automated Testing


These are the characteristics that we strive for in our own work. We would love to hear from candidates who embody the same:

Desire to work collaboratively with your teammates to come up with the best solution to a problem
Demonstrated experience and ability to deliver results on multiple projects in a fast-paced, agile environment
Excellent problem-solving and interpersonal communication skills
Strong desire to learn and share knowledge with others"
Motion Recruitment,Data Engineer / Python/Spark/AWS,"Santa Clara, CA","One of the top companies in the education industry is seeking talented Data Engineers to add to their roster. They are well-known for their student media learning platform, offering services to a myriad of college students including flashcards, prep material, and supplemental lessons. As a Data Engineer at this company, you will be building templates, frameworks, and tools that other engineers will use to build out services. This role utilizes Python with AWS as their cloud service. If you are seeking a new role that involves a large amount of scale and are looking for challenge, this is the role for you. Required Skills & Experience

Highly skilled in Python
Previous usage of tools such as Spark and Kafka
SQL or other related database experience
Past work in an AWS environment
Desired Skills & Experience

Experience with Data Bricks is a big plus but not required
What You Will Be Doing

Tech Breakdown

Coding: 80%
Design: 20%
The Offer

Competitive Salary: Up to $200,000 a year

You Will Receive The Following Benefits

Fully remote availability
Flex Hours
PTO
Health Care Benefits

Applicants must be currently authorized to work in the United States on a full-time basis now and in the future.

This position does not offer sponsorship.

Posted By: Jamison Diep"
Flock Freight,Staff Data Engineer,"San Diego, CA
On-site","What drives you? Maybe you want to make the world a better place. Maybe you want to advance your career and work with the brightest minds in freight technology. Maybe you want to tackle new, exciting challenges while disrupting an industry. If this sounds like you, you’ve come to the right place.

Interviewing is a two-way street, so let’s get to know each other. Here’s a little about us:

We’re a unicorn. Following our recent Series D funding round, our valuation has surpassed $1 billion
We’re growing fast — like, speed-of-light fast. In the past year, we’ve tripled our workforce and added thousands of square feet in Encinitas and Chicago
We’ve got support. Our investors include SoftBank, Susquehanna, Eden Capital, Volvo Group Venture Capital, SignalFire, GLP Capital Partners, and GV (formerly Google Ventures)
Efficiency is in our DNA. Our mission is to reinvent North America’s $2 trillion freight industry by relentlessly eliminating waste and inefficiency
Algorithms drive our sustainable shared truckload solution. Behind our carbon neutral shared truckload service is world-class tech (and a lot of sweat)
We’re taking names. Find us on the CNBC Disruptor 50, SDCE 100, SDBJ Diversity & Social Responsibility, and Fast Company World Changing Ideas lists
We walk the walk. We make a positive impact in everything we do; our B Corporation certification is proof. We were the first B Corp in our industry 
We offer a hybrid work environment, and we are intentional about doing it right. Knowing you have a life outside of work, we won’t ask you to do your job from the office five days a week


About the Role:

The Staff Data Engineer will be primarily responsible for driving the Data Platform roadmap of the Infrastructure team. The primary mission of this platform is to continue to democratize data access at Flock Freight in a unified way via Big Data technologies. The primary stakeholder of these products will be the Technology and Analytics teams consisting of analysts, data scientists and engineers. A successful engineer will collaborate closely with these teams to build a scalable and performant data platform and tooling to help scale the business.

Responsibilities: 

Define Data platform roadmap in collaboration with Technology leadership and business stakeholders
Define data needs from data science, analytics and product management into information to design relevant data platforms and tools to service these needs
Design, develop and maintain scalable data pipelines and Data warehouse to support continuing increases in data volume and complexity
Design and implement production-grade data pipelines utilizing big data and distributed streaming technologies like Apache Kafka
Improve data models that feed business intelligence tools, increasing data accessibility and fostering educated decision making across the organization
Take ownership end-to-end, from defining the data architecture to implementing data models, pipelines, monitoring and quality assurance frameworks
Write complex SQL queries and tune them for efficiency to quickly transform data into timely and actionable insights


Qualifications: 

5+ years of experience in data engineering and a proven track record of solving data problems at scale leveraging distributed data systems
A good understanding of designing and building highly scalable Data warehouses, Data pipelines, Data Modelling, designing Data Lakes and Big Data Analytics using Nosql, and programming languages (like Python, Java, Scala)
Expert in SQL and optimizing complex queries
Proficient in programming in Python
Experience working with Snowflake, Redshift, PostgreSQL and/or other DBMS platforms
Experience working with dbt
Excellent communication, collaborative demeanor and ability to work in distributed, multi-functional teams with the ability to articulate a point of view.


What’s in it for you?

Unlimited paid time off: Take the time you need to recharge, stay healthy, and do your best work
Ownership: As a thank-you for your dedication, we’ll give you the chance to build equity at Flock Freight with stock options
Retirement plan: Plan for your future with a 401(k) account, which includes sustainable investing options
Insurance: Health, dental, vision, and life insurance with HMO, PPO, and premium PPO options — plus an elective flexible spending account
HealthiestYou: Connect with a doctor virtually — 24 hours a day, 365 days a year
Commuter benefits: Save on commuting costs with our commuter benefit fund and reimbursements for low-emission transportation purchases.
Paid parental leave: Take up to eight weeks of paid parental or paid pregnancy disability leave — we’ll coordinate any extra state or insurance benefits for you!
Fun stuff: Cold brew and kombucha on tap, intramural sports, surf club, free lunches — need we say more?


We’re changing how freight moves for the better, and are having fun doing it. You with us? Visit flockfreight.com/careers to join the Flock.

Committed to creating an environment that’s fair and inclusive, Flock Freight fills open positions based on qualifications, merit, and business need. We value the skills of people from various backgrounds and are proud to hire, promote, and retain talent from a diverse candidate pool. Diverse perspectives are central to innovation at Flock Freight and make our team better.

Flock Freight has zero tolerance for behavior that negatively impacts marginalized groups, including women, people of color, veterans, immigrants, and members of the LGBTQIA+ community. We invite people of all identities to join the Flock! 

To ensure safety for all of our employees, Flock Freight's offices are fully vaccinated worksites. If you are local to either our San Diego or Chicago office, please know that we will require proof of vaccination to work at these sites. This vaccination policy applies to all Flock Freight offices and sponsored work events."
IBM,Data Engineer- Industrial,"Raleigh, NC","492882BR

Introduction

At IBM, work is more than a job - it's a calling: To build. To design. To code. To consult. To think along with clients and sell. To make markets. To invent. To collaborate. Not just to do something better, but to attempt things you've never thought possible. Are you ready to lead in this new era of technology and solve some of the world's most challenging problems? If so, lets talk.

Your Role and Responsibilities

As an IBM Data Engineer, you will directly help clients transform their business and solve complex problems. You will be involved in the transformation by doing the critical work of getting the data in a state where and it be effectively used by the client for its business, analytics and AI. You will do this at scale with understanding of the needs of the various stakeholders. You will be responsible for implementing solutions that are cutting edge and utilizing best practices with solid documentation, unit testing, performance testing, capacity planning, monitoring, alerting and governing.

If you are hired into a Colorado work location, the anticipated compensation range for the position $63,300 to $172,500 is based on a full-time schedule. Your ultimate compensation may vary depending on your job-related skills and experience. For part time roles, the compensation will be adjusted appropriately.

POST COVID you are expected to travel up to 75% (4 days a week) of the time to meet our client needs across the US.

Benefits

Health Insurance. Paid time off. Corporate Holidays. Sick leave. Family planning. Financial Guidance. Competitive 401K. Training and Learning. We continue to expand our benefits and programs, offering some of the best support, guidance and coverage for a diverse employee population.

http://www-01.ibm.com/employment/us/benefits/

CAREER GROWTH

At Our Core, We Are Committed To Believing And Investing In Our Workforce Through

Our goal is to be essential to the world, which starts with our people. Company-wide we kicked off an internal talent strategy program called Go Organic.

Skill Development: Helping our employees grow their foundational skills

Finding the Dream Career at IBM: Navigating our company with the potential for many careers by channeling an employee’s strengths and career aspirations

Diversity of People: Diversity of thought driving collective innovation

In 2015, Go Organic filled approximately 50% of our open positions with internal talent that were promoted into the role.

CORPORATE CITIZENSHIP

With an employee population of 375,000 in over 170 countries, amazingly we connect, collaborate, and care. IBMers drive a corporate culture of shared responsibility. We love grand challenges and everyday improvements for our company and for the world. We care about each other, our clients, and the communities we live, work, and play in!

http://www.ibm.com/ibm/responsibility/initiatives.html

http://www.ibm.com/ibm/responsibility/corporateservicecorps

Required Technical and Professional Expertise


BS in Data Science, Machine Learning, Statistics, or AI
3+ years of proficiency in Python
3+ years of experience with applying deep learning and machine learning techniques to solve problems
3+ years of Experience with common Python libraries used by data scientists (e.g., NumPy, Pandas, SciPy, scikit-learn, matplotlib, Seaborn, etc.) and deep learning libraries (pytorch or Tesnorflow)
Experience working with structured and unstructured data
Experience building end-to-end data pipelines and deploying in the Cloud (AWS or Azure or GCP)
Demonstrated ability to think strategically about business, product, and technical challenges in an enterprise environment
Experience in Agile development
Excellent oral and written communication skills
Ability to collaborate in a team environment
Prior experience working with clients in the industrial space
Experience with advanced analytics


Preferred Technical And Professional Expertise


Master’s degree in quantitative Field
Demonstrated leadership abilities, with team leader or managerial experience preferred
Proficiency with dealing with big data (Spark)
Hands-on experience deploying analytical models to solve business problems
Experience working in a consulting or services environment


About Business Unit

IBM Consulting is IBM’s consulting and global professional services business, with market leading capabilities in business and technology transformation. With deep expertise in many industries, we offer strategy, experience, technology, and operations services to many of the most innovative and valuable companies in the world. Our people are focused on accelerating our clients’ businesses through the power of collaboration. We believe in the power of technology responsibly used to help people, partners and the planet.

This job requires you to be fully COVID-19 vaccinated prior to your start date, where legally permissible. Proof of vaccination status will be required. If you are unable to be vaccinated due to medical, pregnancy or religious reasons, we offer accommodations in accordance with applicable law.

Your Life @ IBM

Are you craving to learn more? Prepared to solve some of the world's most unique challenges? And ready to shape the future for millions of people? If so, then it's time to join us, express your individuality, unleash your curiosity and discover new possibilities.

Every IBMer, and potential ones like yourself, has a voice, carves their own path, and uses their expertise to help co-create and add to our story. Together, we have the power to make meaningful change – to alter the fabric of our clients, of society and IBM itself, to create a truly positive impact and make the world work better for everyone.

It's time to define your career.

About IBM

IBM’s greatest invention is the IBMer. We believe that through the application of intelligence, reason and science, we can improve business, society and the human condition, bringing the power of an open hybrid cloud and AI strategy to life for our clients and partners around the world.

Restlessly reinventing since 1911, we are not only one of the largest corporate organizations in the world, we’re also one of the biggest technology and consulting employers, with many of the Fortune 50 companies relying on the IBM Cloud to run their business.

At IBM, we pride ourselves on being an early adopter of artificial intelligence, quantum computing and blockchain. Now it’s time for you to join us on our journey to being a responsible technology innovator and a force for good in the world.

Location Statement

Benefits

In addition to a competitive benefits program consisting of medical and life insurance, retirement plans, and time off, eligible employees may also have access to

IBM offers a wide range of resources for eligible IBMers to thrive both inside and outside of work.


12 weeks of paid parental bonding leave. Family care options are also available to support eligible employees during COVID-19.
World-class training and educational resources on our personalized, AI-driven learning platform. IBM's learning culture supports your restless attitude to grow your skills and build the depth and scale of knowledge needed to achieve your career goals.
Well-being programs to support mental and physical health.
Financial programs that empower you to plan, save, and manage your money (including expert financial counseling, 401(k), IBM stock discount, etc.).
Select educational reimbursement opportunities.
Diverse and inclusive employee resource groups where you can network and connect with IBMers across the globe.
Giving and volunteer programs to benefit charitable organizations and local communities.
Discounts on retail products, services, and experiences.


We consider qualified applicants with criminal histories, consistent with applicable law.

Being You @ IBM

IBM is committed to creating a diverse environment and is proud to be an equal opportunity employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, gender, gender identity or expression, sexual orientation, national origin, genetics, pregnancy, disability, age, veteran status, or other characteristics. IBM is also committed to compliance with all fair employment practices regarding citizenship and immigration status."
Deloitte,Data Warehousing Engineer - Alteryx / Python / R,"Washington, DC","In this age of disruption, organizations need to navigate the future with confidence by tapping into the power of data analytics, robotics, and cognitive technologies such as Artificial Intelligence (AI).

Our Strategy & Analytics portfolio helps clients leverage rigorous analytical capabilities and a pragmatic mindset to solve the most complex of problems. By joining our team, you will play a key role in helping to our clients uncover hidden relationships from vast troves of data and transforming the Government and Public Services marketplace.

Work you'll do

Data mining using state-of-the-art methods.
Enhance data collection procedures.
Develop data migration and wrangling scripts.
Process, cleanse, and verify the integrity of data.
Use visualization tools to develop dashboards.
Create automated anomaly detection systems.

The team

Deloitte's Government and Public Services (GPS) practice - our people, ideas, technology and outcomes-is designed for impact.

Serving federal, state, & local government clients as well as public higher education institutions, our team of over 15,000+ professionals brings fresh perspective to help clients anticipate disruption, reimagine the possible, and fulfill their mission promise.

The GPS Analytics and Cognitive (A&C) offering is responsible for developing advanced analytics products and applying data visualization and statistical programming tools to enterprise data in order to advance and enable the key mission outcomes for our clients.

Our team supports all phases of analytic work product development, from the identification of key business questions through data collection and ETL, and from performing analyses and using a wide range of statistical, machine learning, and applied mathematical techniques to delivery insights to decision-makers.

Our practitioners give special attention to the interplay between data and the business processes that produce it and the decision-makers that consume insights.

Qualifications

2+ years of military or post-grad experience
2+ years working in data wrangling, Extract-Transform-Load (ETL) functions, data modeling and building data pipelines
2+ years using open-source and COTS middleware solutions e.g., Alteryx, R, Python.
Active Secret Clearance (or Higher)
Ability to work primarily remote w/ onsite needs in Downtown DC

How You'll Grow

At Deloitte, our professional development plan focuses on helping people at every level of their career to identify and use their strengths to do their best work every day. From entry-level employees to senior leaders, we believe there's always room to learn.

We offer opportunities to help sharpen skills in addition to hands-on experience in the global, fast-changing business world. From on-the-job learning experiences to formal development programs, our professionals have a variety of opportunities to continue to grow throughout their career."
Deloitte,Journeyman Data Engineer,"Tampa, FL","In this age of disruption, organizations need to navigate the future with confidence by tapping into the power of data analytics, robotics, and cognitive technologies such as Artificial Intelligence (AI). Our Strategy & Analytics portfolio helps clients leverage rigorous analytical capabilities and a pragmatic mindset to solve the most complex of problems. By joining our team, you will play a key role in helping to our clients uncover hidden relationships from vast troves of data and transforming the Government and Public Services marketplace.

Work you'll do

Translate business requirements into technical specifications; establish and define details, definitions, and requirements of applications, components and enhancements
Generate design, development, test plans, detailed functional specifications documents, user interface design, and process flow charts for execution of programming
Build large-scale batch and real-time data pipelines with data processing frameworks in AWS
Develop data pipelines / APIs using Python, SQL, potentially Spark and AWS
Use an analytical, data-driven approach to drive a deep understanding of fast changing business
Participate in project planning; identifying milestones, deliverables and resource requirements; tracks activities and task execution

The team

Deloitte's Government and Public Services (GPS) practice - our people, ideas, technology and outcomes-is designed for impact. Serving federal, state, & local government clients as well as public higher education institutions, our team of over 15,000+ professionals brings fresh perspective to help clients anticipate disruption, reimagine the possible, and fulfill their mission promise.

The GPS Analytics and Cognitive (A&C) offering is responsible for developing advanced analytics products and applying data visualization and statistical programming tools to enterprise data in order to advance and enable the key mission outcomes for our clients. Our team supports all phases of analytic work product development, from the identification of key business questions through data collection and ETL, and from performing analyses and using a wide range of statistical, machine learning, and applied mathematical techniques to delivery insights to decision-makers. Our practitioners give special attention to the interplay between data and the business processes that produce it and the decision-makers that consume insights.

Qualifications

Required:

Bachelor's degree required
Must be legally authorized to work in the United States without the need for employer sponsorship, now or at any time in the future
2+ years of relevant technology consulting or industry experience to include experience in Information delivery, Analytics and Business Intelligence based on data
2+ years experience in Python and/or R
2+ years experience SQL
2+ years of hands on experience with data core modernization and data ingestion
Experience engineering data pipelines, ETL jobs and logical data models that serve business analytics and visualization tools
Experience building algorithm, automation and deployment code
Experience working with Streaming data sources and building data analytic applications from event driven data sources
Experience Optimizing Data Loads, Data Extractions, Queries and Stored Procedures

Preferred:

TS/SCI s ecurity clearance
Local to Tampa
Experience with containerization
Working with senior level leadership as they work through requirements to build a pipeline
Experience with Agile

How you'll grow

At Deloitte, our professional development plan focuses on helping people at every level of their career to identify and use their strengths to do their best work every day. From entry-level employees to senior leaders, we believe there's always room to learn. We offer opportunities to help sharpen skills in addition to hands-on experience in the global, fast-changing business world. From on-the-job learning experiences to formal development programs, our professionals have a variety of opportunities to continue to grow throughout their career."
Stax,Mid-Level Data Engineer,Baton Rouge Metropolitan Area,"Stax is a home-grown start-up revolutionising the AWS cloud experience. Our aim is to provide a platform that automates and streamlines critical tasks to provide a secure and efficient developer experience. Stax comprises of a team of over 40 cloud artisans passionate about the quality outcomes our product delivers. With over 40 active customers across three continents, our customers range from fast food, to luxury, to ASX top 20. We've cultivated an environment based on creativity. When people who care about their craft are given the freedom to explore possibilities without limits, amazing things happen. There are no cool cliques, just a hard-working team generating ideas and devising solutions in a creative and collaborative workspace.

Our Stax tribe are going through a period of massive growth and we're pretty chuffed to be announced as one of the Top 3 fastest growing companies across Australia by CRN Australia and Schneider Electric.

We're looking for Mid-level Data Engineer to join our Cost engineering team on a permanent basis. You'll be an engineer with data application experience that can work in a fast-moving environment to help face into our roadmap.

Our cost product is a big data pipeline, processing hundreds of millions of rows and terabytes of data every day. The successful engineer will bring their love of Software and DevOps skills and Data expertise to continue to deliver value while setting us up for growth in the future. They will be joining an established cross-functional development team that's agile, fun, and driven to build high quality product.

This is a great opportunity for someone to tackle complex data problems, in an environment full of smart and talented people to learn from

As Part Of The Team You Will

Develop and maintain scalable data pipelines and lakes to support continuing increases in data volume and complexity
Collaborate and engage in product decisions to improve the data models and processes that feed our platform
Write unit/integration tests, contributes to engineering wiki, and documents work
Work closely with other teams across the business to develop and our long-term data strategy
Work with our SRE team to monitor and improve our new and existing data pipelines to ensure they are reliable
Perform data analysis required to troubleshoot data related issues and assist in the resolution of data issues

We Are Looking For Someone Who

Has 3+ years’ experience in a Data Engineering role – programming in python is preferred
Has a software engineering background
Has experience running and maintaining production system
Experience working with SQL and no-SQL technologies
Is proactive and a ‘can do’ approach to the problem solving
Experience working in an agile environment – including planning, estimating, and delivering in iterative chunks
Experience working with AWS and AWS specific technologies like Redshift is highly regarded

Our values reflect the way we work. We’re a casual, inclusive bunch, with team members from a variety of backgrounds collaborating as a team to overcome challenges. Everyone is given space to learn and develop their skills and knowledge. We support each other in all ventures, whether attaining a new AWS certification or trying their hand at baking sourdough or brewing beer. We create remarkable experiences for our customers and we treat others the way we would like to be treated."
Amazon,"Data Engineer, WASE Hercules","Seattle, WA","Job Summary

DESCRIPTION

Amazon Advertising is looking for a talented Data Engineer to help our Advertisers achieve success. Amazon Advertising operates at the intersection of eCommerce and advertising, offering a rich array of digital display advertising solutions with the goal of helping our customers find and discover anything they want to buy. We help advertisers reach Amazon customers on Amazon.com, across our other owned and operated sites, on other high quality sites across the web, and on millions of Kindles, tablets, and mobile devices. We start with the customer and work backwards in everything we do, including advertising. If you’re interested in joining a rapidly growing team working to build a unique, world-class advertising group with a relentless focus on the customer, you’ve come to the right place.

Key job responsibilities

The WorldWide Advertising Success Engineering (WASE) team is looking for a motivated Data Engineer I who can implement components of end-to-end data infrastructure. You will work with business and technology teams to develop scalable and innovative solutions that source terabytes of data, both batch and streaming, transform it into data structures relevant for making daily financial and product decisions, and expose it using tools that drive insights and actions. We’re a fast-growing team with a very high focus from business to grow, so there are lots of opportunities. No chance of getting bored here.

A day in the life

In This Role You Will

Partner with engineering, product, business and finance teams to create data pipelines and structures that provide key metrics and performance indicators.
Implement and support an analytical data infrastructure, providing ad-hoc access to large datasets and computing power.
Be responsible for the operation of data infrastructure and the quality of data sets.
Explore and learn the latest AWS technologies to provide new capabilities and increase efficiency.

About The Team

We are a team of data-focused engineers within the Worldwide Advertiser Success Engineering (WASE) org, passionate about Big Data and finding novel ways to combine data and technology to build analytics and insights that are instrumental in the success of our advertisers.


Basic Qualifications

1+ years of experience as a Data Engineer or in a similar role
Experience with data modeling, data warehousing, and building ETL pipelines
Experience in SQL

Preferred Qualifications

Proven success in communicating with users, other technical teams, and management to collect requirements, describe data modeling decisions, and data engineering strategy
Demonstrated ability in data modeling, ETL development, and data warehousing
Experience with Big Data technologies, including Hadoop, Hive, and Spark
Experience using Business Intelligence reporting tools, including Quicksight, Tableau, and open source projects like Superset
Experience with AWS services, including Redshift, Lambda, EMR, and Glue
Experience with at least one modern programming language (Java, Python, Scala)
Experience with Redshift or other MPP data warehouse
Willingness to own all stages of development process: requirements, design, implementation, testing, and operational support

Amazon is committed to a diverse and inclusive workplace. Amazon is an equal opportunity employer and does not discriminate on the basis of race, national origin, gender, gender identity, sexual orientation, protected veteran status, disability, age, or other legally protected status. For individuals with disabilities who would like to request an accommodation, please visit https://www.amazon.jobs/en/disability/us.


Company - Amazon.com Services LLC

Job ID: A1923249"
Deloitte,Data Engineer Spark,"Boca Raton, FL","Are you an experienced, passionate pioneer in technology? A cloud solutions builder who wants to work in a collaborative environment. As an experienced Data Engineer, you will have the ability to share new ideas and collaborate on projects as a consultant without the extensive demands of travel. If so, consider an opportunity with Deloitte under our Project Delivery Talent Model. Project Delivery Model (PDM) is a talent model that is tailored specifically for long-term, onsite client service delivery. PDM practitioners are local to project locations, minimizing extensive travel, and provides you with a full career path within the firm.

The Team

The US Cloud Engineering Offering focuses on enabling our client's end-to-end journey from On-Premises to Cloud, with opportunities in the areas of Cloud Strategy and Op Model Transformation, Cloud Development & Integration, Cloud Migration, and Cloud Infrastructure & Managed Services. Cloud Engineering supports our clients as they improve agility, resilience and identifies opportunities to reduce IT operations spend through automation by enabling Cloud.

Work you'll do/Responsibilities:

Partner with the Analytics team to automate processes that improve data utilization for insights and reporting needs.
Develop and implement most levels of our data stack (ingestion layer, modeling layer, analytics, etc.). Debug production issues across services and multiple levels of the stack.
Design, build and improve our ETL platform and pipelines that integrate with our Data lake and Messaging platform. Manage integrations with 3rd party CDP solution(s), constantly improving the efficiency, cost-effectiveness and quality of the integration.
Determine additional technology requirements and data infrastructure needs to support growing business requirements
Write, analyze and debug SQL queries that range in difficulty from simple to complex
Ensure standards for engineering excellence, scalability, reliability, and reusability

Required

Qualifications

Bachelor's Degree preferably in Computer Science, Information Technology, Computer Engineering, or related IT discipline; or equivalent experience.
Understanding of processing and modeling large, complex data sets for Analytical use cases with Database technologies such as AWS Redshift, Spark, Teradata or equivalent.
Ability to communicate effectively and work independently with little supervision to deliver on time quality products.
Willingness to learn, be open minded to new ideas and different opinions yet knowing when to stop, analyze, and reach a decision.
Familiarity with AWS cloud technologies such as Elastic Map Reduce (EMR), Kinesis, Athena.
Familiarity with BI and Visualization platforms such as MicroStrategy and AWS Quicksight.
Travel up to 10% annually.
Limited Sponsorship: Limited immigration sponsorship may be available."
Vertafore,Data Conversion Engineer II (Remote),"United States
Remote","Posted by

Hannah Morris

Workplace Specialist at Vertafore

Send InMail

Vertafore is a leading technology company whose innovative software solution are advancing the insurance industry. Our suite of products provides solutions to our customers that help them better manage their business, boost their productivity and efficiencies, and lower costs while strengthening relationships.

Our mission is to move InsurTech forward by putting people at the heart of the industry. We are leading the way with product innovation, technology partnerships, and focusing on customer success.

Our fast-paced and collaborative environment inspires us to create, think, and challenge each other in ways that make our solutions and our teams better.

We are headquartered in Denver, Colorado, with offices across the U.S., Canada, and India.

JOB DESCRIPTION

Join a fun and fast paced team focused on developing tools to migrate customers from competitive products to Vertafore products. If you love to study, learn and implement technology solutions with some of the smartest people in the industry with a leading-edge company in the insurance software space, then this is the job for you. This position will be responsible for the analysis, design, development and testing solutions and providing conversion support for customers.

JOB DESCRIPTION

Join a fun high-energy team in delivery custom integration solutions. If you love to study, learn and implement technology solutions with some of the smartest people in the industry with a leading-edge company in the insurance software space, then this is the job for you. This position will be responsible for the analysis, design, development and implementation of data conversions for agency customers.

Our goal is to create an industry leading engineering services group specializing is software solutions for the Agency market. Our customers have significant integration and conversion challenges and need top of the line people and knowledge to help them solve their challenges.

Core Requirements and Responsibilities:

Essential job functions included but are not limited to the following:

Assist and guide customers through the conversion process to convert their source data to a Vertafore system
Perform data analysis, data mapping, data requirements, reverse engineering, validation of data and corresponding data dependencies
Perform customer welcome calls to review conversion project timeline and expectations
Develop, test, and deploy customized conversion T-SQL scripts
Participate in the design and planning of projects in conjunction with the external partners, other IT, and business users.
Assist and direct customers in performing data translations and mappings
Document all discussions, issues and solutions to problems involving the customer in the customer relationship management system
Research/diagnose problems for prompt resolution to time sensitive issues
Perform quality assurance on all converted data for accuracy from application functionality perspective
Collaborate with a team on a given project to ensure the project meets deadlines and stays within scope and budget
Contribute to team development through support and collaboration.
Contribute technical articles, case studies, white papers, analysis, advice and support
Manage and maintain code repositories for reusable assets.
Requires occasional weekend and after-hours availability for client support and conversion assistance
Perform project scoping analysis

Knowledge, Skills and Abilities:

Strong analytical and problem-solving skills
Thorough knowledge of database design and associated tools
Expertise in a variety of different file/data formats
Excellent communication skills including delivering complex technical information to non-technical people
Excellent organizational and time-management skills and can be trusted to manage your projects and meet project deadlines

Qualifications:

Undergraduate degree in engineering, math, computer science or related field preferred
Demonstrated proficiency in MS SQL Server 2008(or greater) Management Studio
Experience in data analysis and ability to reverse engineer database schema highly desirable
Demonstrated knowledge of structured development processes, technical project management, software testing and documentation
Prior insurance experience desired
You have shown the ability to organize and disseminate information efficiently and have the ability to multi-task in a fast-paced environment
You have proven to be a dedicated contributor that will work tirelessly in delivering outstanding service, support, and assistance to end customers

Additional Requirements and Details:

Travel required up to 5% of the time.
Located and working from an office location.
Occasional lifting and/or moving up to 10 pounds.
Frequent repetitive hand and arm movements required to operate a computer.
Specific vision abilities required by this job include close vision (working on a computer, etc.).
Frequent sitting and/or standing.

Is this role not an exact fit for you? Keep an eye on our Careers Page for other positions!

Vertafore is a drug free workplace and conducts preemployment drug and background screenings.

The selected candidate must be legally authorized to work in the United States.

The above statements are intended to describe the general nature and level of work being performed by people assigned to this job. They are not intended to be an exhaustive list of all the job responsibilities, duties, skill, or working conditions. In addition, this document does not create an employment contract, implied or otherwise, other than an ""at will"" relationship.

Vertafore strongly supports equal employment opportunity for all applicants regardless of race, color, religion, sex, gender identity, pregnancy, national origin, ancestry, citizenship, age, marital status, physical disability, mental disability, medical condition, sexual orientation, genetic information, or any other characteristic protected by state or federal law.

We do not accept resumes from agencies, headhunters, or other suppliers who have not signed a formal agreement with us.

The Vertafore Way | Vertafore"
Amazon Web Services (AWS),Data & ML Engineer - Nationwide Opportunities,"United States
Remote","Description

At Amazon Web Services (AWS), we’re hiring highly technical cloud computing architects and engineers to collaborate with our customers and partners on key engagements. Our consultants will develop and deliver proof-of-concept projects, technical workshops, and support implementation projects. These professional services engagements will focus on customer solutions such as Machine Learning, Data and Analytics, HPC and more.

In this role, you will work with our partners, customers and focus on our AWS Analytics and ML service offerings such Amazon Kinesis, AWS Glue, Amazon Redshift, Amazon EMR, Amazon Athena, Amazon SageMaker and more. You will help our customers and partners to remove the constraints that prevent them from leveraging their data to develop business insights.

AWS Professional Services engage in a wide variety of projects for customers and partners, providing collective experience from across the AWS customer base and are obsessed about customer success. Our team collaborates across the entire AWS organization to bring access to product and service teams, to get the right solution delivered and drive feature innovation based upon customer needs.

In our Global Specialist Practice, you will also have the opportunity to create white papers, writing blogs, build demos and other reusable collateral that can be used by our customers, and, most importantly, you will work closely with our Solution Architects, Data Scientists and Service Engineering teams.

The ideal candidate will have extensive experience with design, development and operations that leverages deep knowledge in the use of services like Amazon Kinesis, Apache Kafka, Apache Spark, Amazon Sagemaker, Amazon EMR, NoSQL technologies and other 3rd parties.

Excellent business and communication skills are a must to develop and define key business questions and to build data sets that answer those questions. You should be able to work with business customers in understanding the business requirements and implementing solutions.

Inclusive Team Culture

Here at AWS, we embrace our differences. We are committed to furthering our culture of inclusion. We have thirteen employee-led affinity groups, reaching 85,000 employees in over 190 chapters globally. We have innovative benefit offerings, and host annual and ongoing learning experiences, including our Conversations on Race and Ethnicity (CORE) and AmazeCon (gender diversity) conferences. Amazon’s culture of inclusion is reinforced within our 16 Leadership Principles, which remind team members to seek diverse perspectives, learn and be curious, and earn trust.

Work/Life Balance

Our team puts a high value on work-life harmony. Striking a healthy balance between your personal and professional life is crucial to your happiness and success here. We are a customer-obsessed organization—leaders start with the customer and work backwards. They work vigorously to earn and keep customer trust. As such, this is a customer facing role in a hybrid delivery model. Project engagements include remote delivery methods and onsite engagement that will include travel to customer locations as needed.

Mentorship & Career Growth

Our team is dedicated to supporting new members. We have a broad mix of experience levels and tenures, and we’re building an environment that celebrates knowledge sharing and mentorship. We care about your career growth and strive to assign projects based on what will help each team member develop into a better-rounded professional and enable them to take on more complex tasks in the future.

This is a customer facing role. You will be required to travel to client locations and deliver professional services when needed.


Basic Qualifications

Bachelor’s degree, or equivalent experience, in Computer Science, Engineering, Mathematics or a related field
5+ years’ experience of Data platform implementation, including 3+ years of hands-on experience in implementation and performance tuning Kinesis/Kafka/Spark/Storm implementations.
Experience with analytic solutions applied to the Marketing or Risk needs of enterprises
Basic understanding of machine learning fundamentals.
Ability to take Machine Learning models and implement them as part of data pipeline
5+ years of IT platform implementation experience.
Experience with one or more relevant tools ( Flink, Spark, Sqoop, Flume, Kafka, Amazon Kinesis ).
Experience developing software code in one or more programming languages (Java, JavaScript, Python, etc).
Current hands-on implementation experience required

Preferred Qualifications

Masters or PhD in Computer Science, Physics, Engineering or Math.
Hands on experience working on large-scale data science/data analytics projects.
Ability to lead effectively across organizations.
Hands-on experience with Data Analytics technologies such as AWS, Hadoop, Spark, Spark SQL, MLib or Storm/Samza.
Implementing AWS services in a variety of distributed computing, enterprise environments.
Proficiency with at least one the languages such as C++, Java, Scala or Python.
Experience with at least one of the modern distributed Machine Learning and Deep Learning frameworks such as TensorFlow, PyTorch, MxNet Caffe, and Keras.
Experience building large-scale machine-learning infrastructure that have been successfully delivered to customers.
Experience defining system architectures and exploring technical feasibility trade-offs.
3+ years experiences developing cloud software services and an understanding of design for scalability, performance and reliability.
Experience working on a code base with many contributors.
Ability to prototype and evaluate applications and interaction methodologies.
Experience with AWS technology stack.
Written and verbal technical communication skills with an ability to present complex technical information in a clear and concise manner to a variety of audiences.

Amazon is committed to a diverse and inclusive workplace. Amazon is an equal opportunity employer and does not discriminate on the basis of race, national origin, gender, gender identity, sexual orientation, protected veteran status, disability, age, or other legally protected status. For individuals with disabilities who would like to request an accommodation, please visit https://www.amazon.jobs/en/disability/us.

Pursuant to the San Francisco Fair Chance Ordinance, we will consider for employment qualified applicants with arrest and conviction records

Pursuant to the Los Angeles Fair Chance Ordinance, we will consider for employment qualified applicants with arrest and conviction records.

Virtual Jobs Where Work Can Be Performed In Colorado

For employees based in Colorado, this position starts at $ 108,900 per year. A sign-on bonus and restricted stock units may be provided as part of the compensation package, in addition to a range of medical, financial, and/or other benefits, dependent on the position offered.


Company - Amazon Web Services, Inc.

Job ID: A1029325"
Dice,Cloud Data Engineer (AWS/Azure/Google Cloud Platform) - 100% remote - W2 only,"United States
Remote","Dice is the leading career destination for tech experts at every stage of their careers. Our client, Empower Professionals, is seeking the following. Apply via Dice today!

Hi, Hope you are doing well! My name is Aayushi. I have a very urgent position that we are exclusively recruiting for one of our Fortune Client. We are looking to fill this requirement urgently. Role Cloud Data Engineer (AWSAzureGoogle Cloud Platform) Location 100 remote Duration ndash 6 months (CTH) W2 only Required Skills (AWSAzureGoogle Cloud Platform) Basic Qualifications base knowledge of Spark Azure Data Engineer 5+ years of Data Engineer Experience SQL + one of Scala, Python in a data engineering context Cloud (Azure, AWS, Google Cloud Platform) Added from the call (Must have) Data bricks"
Skupos Inc.,Senior Data Engineer,"San Francisco Bay Area
Remote","Posted by

Jamie Stevens

Director of People at Skupos Inc.

Send InMail

Company. What we are building:




Skupos drives revenue growth across all segments of the convenience retail industry through technology that connects both retailers and brands to their shoppers. With a focus on independent stores and small chains which make up nearly 80% of the market, the Skupos platform enables both retailers and brands to compete through better understanding and serving their customers. Founded in 2016, a growing network of 14,000+ customers across all 50 states rely on Skupos to boost sales volume and increase their customer base.




Role. An overview of the opportunity:




At Skupos, data is everything. Our data integrates the forgotten, fragmented world of mom-and-pop corner with the glitzy, gigantic world of CPG conglomerates.

With tens of millions daily transactions, Skupos is looking for a data engineer to wrangle and tame our data flows, expand and optimize our data and data pipeline architecture.




Team. The team and our people:




This role will be part of the Data Platform team, the core functional team building our data products. The team works closely with the Data Infrastructure, Product and BI teams.




Responsibilities. Your responsibilities will include:




Building and maintaining the data and reporting layers for customer facing products.
Collaborate closely with Data Infrastructure and Analytics teams to build complex data pipelines to deliver CICD complete deployment, move data cross - platforms including real time systems.
Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.
Work with the Data Infrastructure team to build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL and AWS ‘big data’ technologies.
Create data tools, analytical datasets for Data Analytics and Machine Learning teams that assist them in building and optimizing our product
Maintain and deliver continuous improvement of core projects through automation and process enhancement
Work with all data teams and consumers to strive for greater functionality in our data systems
Partner with engineers, product managers, and data scientists to breakdown data requirements, analyze source data sets, address data quality issues and effectively build and automate ETL pipelines at scale.
Partner with ML engineers, data infrastructure team to define and own Data engineering tools, products and processes in place and define/set SLAs for each.
Have deep understanding of existing data integration challenges and solutions with optimal ETL solutions and querying techniques




Experience and Skills. Candidates should have:




Bachelor’s degree or equivalent, ideally in a technical or quantitative field (advanced degree is a plus).
4-6+ years of experience working with SQL
Advanced with proficiency with Python, Java, or Scala in a production environment
1-3 years of experience building and optimizing large-scale data pipelines, architectures and data sets.
Recent experience (2+ years) with a modern data warehouse (e.g., Snowflake, BigQuery, Redshift, Pentaho)
Experience with data warehousing architecture and understanding of data modeling concepts and best practices (e.g., normalization/denormalization).
Production experience with Spark
Familiarity with stream-processing platforms and message-queues (e.g. Flink, Hadoop, Kafka) is a plus




Salary is based on experience and location.




Salary range based on Denver Market: $140,000 - $155,000.




Benefits. What we offer:




Competitive salary
Medical, dental, and vision insurance
401(k) retirement savings plan
Discretionary time off (DTO)
Wellness stipend
And more!




A Note on Covid...




We are fortunate to continue to grow during this unfortunate time. Our top priority is to ensure the health and safety of both our current and future Skupeeps.

As of July, our physical office spaces have reopened on a voluntary basis. Our Skuad members are allowed onsite if they are fully vaccinated (2 weeks past final vaccine dose). That being said, we will continue to manage our interview process virtually, don't be surprised if children or pets make an appearance.

We deeply care about you as our candidate, so let the People Team know if there’s anything we can do to make your interview process go more smoothly - we are in your corner!"
IBM,Data Engineer- Industrial,"Austin, TX","459400BR

Introduction

At IBM, work is more than a job - it's a calling: To build. To design. To code. To consult. To think along with clients and sell. To make markets. To invent. To collaborate. Not just to do something better, but to attempt things you've never thought possible. Are you ready to lead in this new era of technology and solve some of the world's most challenging problems? If so, lets talk.

Your Role and Responsibilities

As an IBM Data Engineer, you will directly help clients transform their business and solve complex problems. You will be involved in the transformation by doing the critical work of getting the data in a state where and it be effectively used by the client for its business, analytics and AI. You will do this at scale with understanding of the needs of the various stakeholders. You will be responsible for implementing solutions that are cutting edge and utilizing best practices with solid documentation, unit testing, performance testing, capacity planning, monitoring, alerting and governing.

If you are hired into a Colorado work location, the anticipated compensation range for the position $63,300 to $172,500 is based on a full-time schedule. Your ultimate compensation may vary depending on your job-related skills and experience. For part time roles, the compensation will be adjusted appropriately.

POST COVID you are expected to travel up to 75% (4 days a week) of the time to meet our client needs across the US.

Benefits

Health Insurance. Paid time off. Corporate Holidays. Sick leave. Family planning. Financial Guidance. Competitive 401K. Training and Learning. We continue to expand our benefits and programs, offering some of the best support, guidance and coverage for a diverse employee population.

http://www-01.ibm.com/employment/us/benefits/

CAREER GROWTH

At Our Core, We Are Committed To Believing And Investing In Our Workforce Through

Our goal is to be essential to the world, which starts with our people. Company-wide we kicked off an internal talent strategy program called Go Organic.

Skill Development: Helping our employees grow their foundational skills

Finding the Dream Career at IBM: Navigating our company with the potential for many careers by channeling an employee’s strengths and career aspirations

Diversity of People: Diversity of thought driving collective innovation

In 2015, Go Organic filled approximately 50% of our open positions with internal talent that were promoted into the role.

CORPORATE CITIZENSHIP

With an employee population of 375,000 in over 170 countries, amazingly we connect, collaborate, and care. IBMers drive a corporate culture of shared responsibility. We love grand challenges and everyday improvements for our company and for the world. We care about each other, our clients, and the communities we live, work, and play in!

http://www.ibm.com/ibm/responsibility/initiatives.html

http://www.ibm.com/ibm/responsibility/corporateservicecorps

Required Technical and Professional Expertise


BS in Data Science, Machine Learning, Statistics, or AI
3+ years of proficiency in Python
3+ years of experience with applying deep learning and machine learning techniques to solve problems
3+ years of Experience with common Python libraries used by data scientists (e.g., NumPy, Pandas, SciPy, scikit-learn, matplotlib, Seaborn, etc.) and deep learning libraries (pytorch or Tesnorflow)
Experience working with structured and unstructured data
Experience building end-to-end data pipelines and deploying in the Cloud (AWS or Azure or GCP)
Demonstrated ability to think strategically about business, product, and technical challenges in an enterprise environment
Experience in Agile development
Excellent oral and written communication skills
Ability to collaborate in a team environment
Prior experience working with clients in the industrial space
Experience with advanced analytics


Preferred Technical And Professional Expertise


Master’s degree in quantitative Field
Demonstrated leadership abilities, with team leader or managerial experience preferred
Proficiency with dealing with big data (Spark)
Hands-on experience deploying analytical models to solve business problems
Experience working in a consulting or services environment


About Business Unit

IBM Services is a team of business, strategy and technology consultants that design, build, and run foundational systems and services that is the backbone of the world's economy. IBM Services partners with the world's leading companies in over 170 countries to build smarter businesses by reimagining and reinventing through technology, with its outcome-focused methodologies, industry-leading portfolio and world class research and operations expertise leading to results-driven innovation and enduring excellence.

This job requires you to be fully COVID-19 vaccinated prior to your start date, where legally permissible. Proof of vaccination status will be required. If you are unable to be vaccinated due to medical, pregnancy or religious reasons, we offer accommodations in accordance with applicable law.

Your Life @ IBM

Are you craving to learn more? Prepared to solve some of the world's most unique challenges? And ready to shape the future for millions of people? If so, then it's time to join us, express your individuality, unleash your curiosity and discover new possibilities.

Every IBMer, and potential ones like yourself, has a voice, carves their own path, and uses their expertise to help co-create and add to our story. Together, we have the power to make meaningful change – to alter the fabric of our clients, of society and IBM itself, to create a truly positive impact and make the world work better for everyone.

It's time to define your career.

About IBM

IBM’s greatest invention is the IBMer. We believe that through the application of intelligence, reason and science, we can improve business, society and the human condition, bringing the power of an open hybrid cloud and AI strategy to life for our clients and partners around the world.

Restlessly reinventing since 1911, we are not only one of the largest corporate organizations in the world, we’re also one of the biggest technology and consulting employers, with many of the Fortune 50 companies relying on the IBM Cloud to run their business.

At IBM, we pride ourselves on being an early adopter of artificial intelligence, quantum computing and blockchain. Now it’s time for you to join us on our journey to being a responsible technology innovator and a force for good in the world.

Location Statement

Benefits

In addition to a competitive benefits program consisting of medical and life insurance, retirement plans, and time off, eligible employees may also have access to

IBM offers a wide range of resources for eligible IBMers to thrive both inside and outside of work.


12 weeks of paid parental bonding leave. Family care options are also available to support eligible employees during COVID-19.
World-class training and educational resources on our personalized, AI-driven learning platform. IBM's learning culture supports your restless attitude to grow your skills and build the depth and scale of knowledge needed to achieve your career goals.
Well-being programs to support mental and physical health.
Financial programs that empower you to plan, save, and manage your money (including expert financial counseling, 401(k), IBM stock discount, etc.).
Select educational reimbursement opportunities.
Diverse and inclusive employee resource groups where you can network and connect with IBMers across the globe.
Giving and volunteer programs to benefit charitable organizations and local communities.
Discounts on retail products, services, and experiences.


We consider qualified applicants with criminal histories, consistent with applicable law.

Being You @ IBM

IBM is committed to creating a diverse environment and is proud to be an equal opportunity employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, gender, gender identity or expression, sexual orientation, national origin, genetics, pregnancy, disability, age, veteran status, or other characteristics. IBM is also committed to compliance with all fair employment practices regarding citizenship and immigration status."
Openly,Data Engineer II,"United States
Remote","Openly is rebuilding insurance from the ground up. We are re-envisioning and enhancing every aspect of the customer experience. Doing this requires a rapidly growing team of exceptional, curious, empathetic people with a wide range of skill sets, spanning technology, data science, product, marketing, sales, service, claims handling, finance, etc.

Now is the perfect time to join the journey. Here’s why




It’s working. We’re in multiple states and on our way to operating countrywide. We have thousands of agents selling our product and millions of dollars of annual customer premiums.
We’re well backed & stable. We closed our $40M Series B fundraise. We are supported by some of the top investors globally, including Google’s “Gradient” AI-focused fund, Obvious Ventures (Beyond Meat, etc.), and Advance Venture Partners.
It’s not too late! Despite this traction and stability, we’re still early enough in the journey that there’s time to make a real difference during Openly’s formative period.




If you’d like to understand more about Openly’s mission, consider checking out this video (https://vimeo.com/267654520) from a company pitch we gave several years ago at Techstars.




Job Details




We’re hiring a Data Engineer II to be a part of our data engineering team to build and enhance the data solutions for Openly's insurance platform. You will play a key role in how we manage, structure, store, and access the data that enables our insurance products and customer experiences.




Key Responsibilities




Design, create, and maintain data pipelines
Work with data users, data science and business teams, to create data solutions to be used in various projects
Write code to enhance our data management framework and services to make business and data processes easier, faster, and safer.
Collaborate with our product, operations, and technology teams to develop and deploy new solutions related to data architecture and data pipelines to enable a best-in-class product for our data users
Contribute to architectural and operational decisions, deployment techniques, technologies, policies, processes, etc.
Participate in daily stand ups, weekly 1:1's, and biweekly retros
Assist in educating others on different aspects of data (e.g. data management best practices, data pipelining best practices)
Share your knowledge with others in the company (e.g. tech talks, knowledge shares within the data engineering team)




Key Requirements




1 to 2 years of data engineering and data management experience
Scripting skills in one or more of the following: Python
Experience with Google Cloud data store and data orchestration technologies
Hands on experience and understanding of the entire data pipeline: Data replication tools, Databases, Big Data Platforms, cloud based data platforms, and service based data orchestration.
Understanding of and data support for data visualization tools, such as Google Cloud Data Studio and Looker.
Understanding of modern next generation data warehouse platform, such as the Lakehouse.
Proficiency with SQL optimization and development.
Ability to understand data architecture and modeling as it relates to business goals and objectives
Ability to gain an understanding of data requirements and translate them into a working solution.
Experience with terraform but not required




*If you don't think you meet all of the criteria above, but still are interested in the job, please apply. Nobody checks every box, and we're looking for someone excited to join the team.




Our stack

Backend/Core: Go & Postgresql
Frontend: VueJS, Webpack, Nuxt, & Tailwind
Research/Data Science: R, ArcGIS, H2O & Python
Single binary for now, but separate services are likely later
Our infrastructure runs on Google Cloud. We use: GKE, Cloud Run, Cloud Build, and CloudSQL. We use gitlab for both code hosting and running CI/CD pipelines




Benefits & Perks




Competitive salary, corporate bonus program, equity position in a start-up company
Company-sponsored medical, dental, vision insurance plans, short-term and long-term disability, life insurance, 401k with corporate contribution, and FSA plan
Company-paid 12 weeks parental leave policy
The company fully embraces the “work-from-anywhere” mentality, even before COVID restrictions.
Paid Time Off
Fun, fast-paced, startup environment




U.S. Citizens, Green Card Holders, and those authorized to work in the U.S. for any employer will be considered.




Openly is committed to equal employment opportunity and non-discrimination for all employees and qualified applicants without regard to a person's race, color, sex, gender identity or expression, age, religion, national origin, ancestry, ethnicity, disability, veteran status, genetic information, sexual orientation, marital status, or any characteristic protected under applicable law. Openly is an E-Verify Employer in the United States. Openly will make reasonable accommodations for qualified individuals with known disabilities under applicable law."
Anuvu,Data Engineer,"Illinois, United States
Hybrid","About Anuvu

For over a decade, our clients in aviation and maritime have used our technology-driven products and services to keep their passengers entertained and connected to the things they love, from anywhere in the world. Brands such as Southwest Airlines, Norwegian Cruise Lines, Emirates and Celebrity Cruises have trusted us to provide solutions from high-speed broadband internet to movies, television and games.

Our team of global experts take pride in providing clients what they need today while creating a strategic road map for tomorrow through reliable, scalable, and affordable content and connectivity solutions designed to meet the ever-changing technology needs of our clients' passengers and guests.




Be Part of the Movement

Anuvu is embarking on a new chapter with a new story to tell. As one of the leading providers of high-speed connectivity and entertainment solutions for worldwide mobility markets, we deliver what customers need today, while remaining flexible and future-focused.

Role Summary

We are currently looking to hire a Data Engineer who will report into the Senior Manager, Air Network and become the newest member of our team. They should have a strong background in Python and SQL along with good problem solving skills, critical thinking and the ability to dig in and work your way backwards on your own. Successful candidates will grasp our infrastructure with ease and also understand data and business rules. If this is you, we look forward to hearing from you.




Location: Must be in Illinois.




Remote Options: Position will be Hybrid. (remote work is part of our hybrid model).




What You'll Be Doing

Analyze complex data elements and systems, data flows, dependencies, and relationships to troubleshoot data issues across the business and presents solutions to development team.
Perform ad-hoc analyses of data stored in Airview and write SQL and/or Python scripts, stored procedures, functions.
Design and build scalable pipelines to process terabytes of data.
Focus on the design, implementation, and operation of data management systems to meet business needs. This includes designing how the data will be stored, consumed, and integrated into our systems.
Developing metrics using data infrastructure to monitor performance of systems.
Creation and management of databases to support large scale aggregation processes
Contribute to the vision for data infrastructure, data science, and analytics.

What We’re Looking For

Bachelor’s Degree required
At least 4-5 years of experience as a software or data developer /engineer
Strong knowledge of Python
Strong knowledge of SQL or desire to learn,
Comfortable navigating in Linux environment with bash shell scripting will be added advantage.
Experience (or desire to learn0 building and deploying on AWS, especially with RDS, EC2, S3, EMR and Redshift.
Ability to design, build, and maintain ETL pipelines and data warehouses.
Comfort with the DevOps side of engineering.
Experience with Web Development Frameworks such as Django or Grafana is a plus.

This Might Be the Right Place if You….

Are a team fit; can help advance our global, inclusive culture
Are self-starter who likes to roll up your sleeves and take initiative with minimal direction
Can think about tomorrow, while getting your work done today
Are a collaborative team player; primary audience will be internal teams
Are curious and open to learning and finding new solutions
Are able to provide and accept feedback in a constructive manner
Are organized, have strong communication skills, and do whatever it takes to get things done




The Benefits of Working Here

A career with Anuvu is a unique opportunity to grow your knowledge and skills within a casual culture that thrives on collaboration and innovation. In addition to a providing an outstanding work environment, we offer competitive benefits including paid time off, disability and life insurance, flexible scheduling, employee assistance, and more!




Advanced | Agile | Applied




Anuvu is an equal opportunity employer. Qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability or protected veteran status.

Due to COVID, our interviews are now done in a safe environment using online platforms like Zoom and Microsoft Teams.

Let Innovation Move You."
Skupos Inc.,"Senior Data Engineer, Infrastructure","San Francisco Bay Area
Remote","Posted by

Jamie Stevens

Director of People at Skupos Inc.

Company. What we are building:

Skupos drives revenue growth across all segments of the convenience retail industry through technology that connects both retailers and brands to their shoppers. With a focus on independent stores and small chains which make up nearly 80% of the market, the Skupos platform enables both retailers and brands to compete through better understanding and serving their customers. Founded in 2016, a growing network of 14,000+ customers across all 50 states rely on Skupos to boost sales volume and increase their customer base.




Role. An overview of the opportunity:




At Skupos, data is everything. Our data integrates the forgotten, fragmented world of mom-and-pop corner with the glitzy, gigantic world of CPG conglomerates.

With the volume of incoming transactions growing everyday, Skupos is looking for a data engineer to wrangle and tame our data flows, expand and optimize our data ingestion and process platform architecture. If you are passionate about building scalable solutions that process terabytes of data in a matter of minutes, we look forward to hearing from you !




Team. The team and our people:




This role will be part of the core functional team building our data ingestion/processing architecture. This team works closely with the Data Infrastructure and the Data Platform.




Responsibilities. Your responsibilities will include:




Design, develop and support the data ingestion and data processing platform
Work with the core function team members to build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using AWS big data technologies (EMR/MSK/Spark/Kafka)
Collaborate closely with Data Infrastructure to deliver CICD complete deployment. Maintain and deliver continuous improvement of core projects through automation and process enhancement
Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.
Work with all data teams and consumers to strive for greater functionality in our data systems
Partner with engineers, product managers, and data scientists to breakdown data requirements, analyze source data sets, address data quality issues and effectively build and automate ETL pipelines at scale.
Partner with ML engineers, data infrastructure team to define and own Data engineering tools, products and processes in place and define/set SLAs for each.
Have deep understanding of existing data integration challenges and solutions with optimal ETL solutions and querying techniques




Experience and Skills. Candidates should have:




Bachelor’s degree or equivalent, ideally in a technical or quantitative field (advanced degree is a plus)
4+ years of experience building and optimizing large-scale data pipelines, architectures and data sets on the public cloud, preferably AWS
Production experience with AWS MSK / Apache Kafka
Experience with AWS EMR / Apache Spark
Experience with AWS MWAA / Apache Airflow
Experience with Terraform or Kubernete or other infrastructure as code
Advanced with proficiency with Python, Java (optional) or Scala (optional) in a production environment
Experience with data warehousing architecture and understanding of data modeling
Experience with more AWS services is plus
2+ years with a Snowflake is a plus




Salary is based on experience and location.




Salary range: $130,000 - $145,000."
Munich Re,Data Engineer,"New York, NY","Posted by

Barb McClay

Talent Acquisition Executive

We're adding to our diverse team of experts and are looking to hire those who are committed to building a culture that enables the creation of innovative solutions for our business units and clients.

The Company
As a member of Munich Re's US operations, we offer the financial strength and stability that comes with being part of the world's preeminent insurance and reinsurance brand. Our risk experts work together to assemble the right mix of products and services to help our clients stay competitive - from traditional reinsurance coverages, to niche and specialty reinsurance and insurance products.

The Opportunity
Future focused and always one step ahead
This is the rare opportunity to join an extraordinary team of people on an exceptional mission. You will become a member of the Munich Engine team, a successful innovation initiative of Munich Re which got spun-out into a business unit and which is building the next generation digital underwriting engine.
The team you join has a start-up entrepreneurial culture but operates within a large, high profile global company - the world's leading reinsurance company.

Key Responsibilities of this position include:



Build and Maintain Munich Engine's Data/ML Platform
Build and maintain robust scalable data processing pipelines in Microsoft Azure
Optimize data ingestion process for better reliability and throughput
Design and develop data models for analytics
Work with various complex data sources at various levels of granularity and deliver high-quality data
Drive data architecture design decisions considering future growth
Work closely with actuaries and data scientists to understand their objectives and translate to ensure architectural fit
Own features from design through delivery with ongoing support
Communicating effectively across diverse disciplines(with Product Management, Data Science, Actuarial, etc) to collect requirements.
Experience providing clear data engineering technical leadership, mentoring, and best practices for data management and quality within and across teams.
Develop expert knowledge of data and analytics infrastructure within Munich Re
Implement processes and technology to monitor and improve data quality

At Munich Re US, we see Diversity and Inclusion as a solution to the challenges and opportunities all around us. Our goal is to foster an inclusive culture and build a workforce that reflects the customers we serve and the communities in which we live and work. We strive to provide a workplace where all of our colleagues feel respected, valued and empowered to achieve their very best every day. We recruit and develop talent with a focus on providing our customers the most innovative products and services.
We are an equal opportunity employer. Reasonable accommodations may be made to enable individuals with disabilities to perform the essential functions.


Successful candidates should possess the following skills/capabilities:



Bachelor's degree (MS preferred) in Computer Science, Statistics, Math or equivalent combination of education and experience
5+ years of experience with data modeling, data processing, and data analytics as data engineer
5+ years of hands-on Python and SQL experience required
Experience in P&C insurance industry (preferred experience working with insurance homeowner data)
Expert Data Model skills on designing and building models for semi-structured, relational, graph data
Expert SQL skills including T-SQL, Hive, Spark SQL, etc
Expert Knowledge in database technologies such as Postgres, SQL Server, MySQL, MongoDB, Azure CosmosDB
Experience in data pipeline development using Azure Data Factory, T-SQL, PySpark, Airflow, etc.
Experience with data visualization tools such as Power BI
3+ yeas of hands-on Microsoft Azure experience highly preferred
Drive and dedication, as well as creativity and hands-on attitude
Curiosity in searching for new solutions outside of traditional approaches
Demonstrated ability to experiment with and learn new technologies
Strong oral and written communication and interpersonal skills
Excellent analytical, problem solving and organizational skill

At Munich Re US, we see Diversity and Inclusion as a solution to the challenges and opportunities all around us. Our goal is to foster an inclusive culture and build a workforce that reflects the customers we serve and the communities in which we live and work. We strive to provide a workplace where all of our colleagues feel respected, valued and empowered to achieve their very best every day. We recruit and develop talent with a focus on providing our customers the most innovative products and services.

We are an equal opportunity employer. Reasonable accommodations may be made to enable individuals with disabilities to perform the essential functions.

"
Human Interest,Data Engineer,"United States
Remote","Human Interest is on a mission to ensure that people in all lines of work have access to retirement benefits. Nearly half of all working Americans are not saving enough for their future because they are employed by a company that doesn’t offer a retirement plan. Human Interest is changing that by making it affordable and accessible for small and medium sized businesses to offer employees a path to financial independence through retirement savings.




Our values (the guiding principles that reflect our view on what’s important and what’s right): In it for customers, autonomous & accountable, outcomes driven, inclusive collaboration, and decisive.

Data is central to our success. It forms the core of all of our customer facing reporting, but is also at the heart of how we make decisions as a company. We’re building the first full stack, automated retirement management platform in the industry – and data tells us how to optimize. We are a fast growing company, and it’s time to build an enterprise grade data stack. You will be responsible for architecting and executing a strategy to meet the needs of all key business areas and build an industry leading data environment.




What you get to do every day

Help build a world class data environment
Write, configure, deploy, and maintain the tools needed to deliver clean data, faster
Partner with analytics team to understand the current use cases and supply clean and consistent data models
Work with engineering teams across the company to improve data tracking
Build consistent data models that act as the source of truth for all metrics within the company
Build, deploy and maintain ETLs to pull data out to all third party systems such as salesforce or email automation systems
Partner with product and operations teams to drive efficiency by automating manual tasks




What you bring to the role

3+ years of experience building and maintaining production data pipelines, databases or web applications
Strong software development & scripting skills
Strong SQL skills
Experience building ETLs against various sources, including REST endpoints
Experience building and deploying services in AWS
Desire to learn, think creatively, and share knowledge with others
Proactive mindset - keep an eye out for anything that can be improved, and help us get there




Please feel free to apply to this position even if you do not meet 100% of the requirements listed above.




Why you will love working at Human Interest

Mission - Highly collaborative startup dedicated to supporting employee engagement and growth. It’s an opportunity to help solve one of the biggest unsolved problems in America: saving for retirement.

Compensation - Competitive salary and stock options for every employee

Benefits -

A great 401(k) plan: our own! Our 401(k) includes a dollar-for-dollar employer match up to 4% of compensation.
Employees also receive 100% paid employee health, vision, and dental premiums; dependent premiums are covered at 50%.
Generous PTO and parental leave policies.
Monthly work from home stipend; annual wellness stipend.
Employee Resource Groups including Veterans, Lift Ev’ry Voice, Pride, LatinX, Families, and Women in Tech.
Fun online and regional events and celebrations and department and company-wide offsites.
The vast majority of our positions can be 100% remote.




About Human Interest:

We’re a high-growth, Series D-funded company that’s changing the retirement industry. Named one of America’s Best Startup Employers by Forbes, one of the Best Places to Work by the San Francisco Business Times, and a Top Company by Y Combinator, we’ve raised $337M and are backed by leading investors, including TPG (The Rise Fund), SoftBank, Glynn Capital, NewView Capital, USVP, Wing, Uncork, and others.




Inclusive collaboration makes us a better business

Human Interest is an equal opportunity employer. All applicants will receive consideration for employment without regard to race, color, religion, sex, gender, gender identity or expression, sexual orientation, national origin, genetics, disability, age, veteran or military status, pregnancy, or any other characteristics protected under federal, state, or local laws.

Human Interest employees must adhere to the Company’s security policies and Code of Ethics.

Please note Human Interest does not accept unsolicited resumes from any source other than directly from candidates. We will not consider resumes from vendors including and without limitation search firms, staffing agencies, fee-based referral services, and recruiting agencies. Pursuant to the San Francisco Fair Chance Ordinance, we will consider employment-qualified applicants with arrest and conviction records. We comply with CCPA guidelines. See more: https://humaninterest.com/disclosures"
Plus,Data Engineer,"San Francisco Bay Area
On-site","Posted by

Adam Austad 2nd

Recruiter at Plus

Responsibilities

Create and maintain optimal data pipeline architecture.
Build the infrastructure required for optimal extraction, transformation, and loading of data using SQL and big data technologies.
Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.
Build analytics tools that utilize the data pipeline to provide actionable insights into customer acquisition, operational efficiency and other key business performance metrics.
Work with stakeholders including the Executive, Engineering, and Operation teams to assist with data-related technical issues and support their data infrastructure needs.
Keep our data secure through multiple data centers and AWS regions.

Qualifications

MS or PhD in CS, EE, mathematics, statistics or related field.
Advanced working SQL knowledge and experience working with relational databases, as well as working familiarity with a variety of databases.
Experience building and optimizing ‘big data’ data pipelines, architectures and data sets.
Build processes supporting data transformation, data structures, metadata, dependency and workload management.
Build processes supporting data transformation, data structures, metadata, dependency and workload management.
Experience with data pipeline and workflow management tools, e.g. Airflow.
Experience with AWS cloud services: EC2, EMR, RDS, Redshift.
Experience with stream-processing systems: Storm, Spark-Streaming, etc.
Experience with object-oriented/object function scripting languages: Python, Java, C++, Scala, etc.

Perks

Work on industry-leading technologies at an energetic, exciting Silicon Valley startup
Forge the future of autonomous transportation while collaborating with AI experts from around the world
Catered free lunch, unlimited snacks and beverages
Competitive salary and outstanding benefits package"
Dice,Cloud Data Engineer,"Nottingham, NH","RESPONSIBILITIES Kforce has a client that is seeking a Cloud Data Engineer in Nottingham, MD.

Summary The Cloud Data Engineer will design and implement our enterprise cloud infrastructure, software, and integrations. As a key member of a highly productive team, provide experience and expertise to deliver cutting-edge solutions. This role consists of designing, developing, and maintaining the enterprise cloud architecture.

Responsibilities

Cloud Data Engineer will lead the hands-on technical delivery of new services and capabilities within the AWS environment

Manage the AWS Cloud Infrastructure and assist in the maintenance of the SQL Server environment, identifying and troubleshooting faults as well as providing recommendations to ensure proper utilization

Gather and process raw data from multiple disparate sources (including writing scripts, calling APIs, write SQL queries, etc.) into a form suitable for data warehousing and machine learning

Review and interpret ongoing business requirements Research required data and build appropriate deliverables

Refactor or develop relational data models, dimensional data models, data dictionaries and necessary metadata using Amazon Redshift in AWS

As a Cloud Data Engineer, you will research and propose opportunities for data acquisition and new uses for existing data

Code, test, and document new or modified data systems to create robust and scalable datamachine learning models for analytics

Contribute to the design and direction of enterprise-wide data architecture as well as design documentation deliverables

Complete projects, prioritize tasks, and provide frequent progress reports with limited direction from team leads based on company needs

Create supporting technical and functional documentation, including data flow diagrams, and provide support for deployed solutions

Working both independently and in collaboration with our application developers and data analysts

Requirements

Bachelor's degree in a related discipline (Computer Science, Information Systems Management, Engineering, or similar) or equivalent work experience AWS Certifications are a plus

Advanced Expertise with AWS services such as EC2, Redshift, S3, SageMaker, Lambda, Matillion and API Gateway Glue and Comprehend are a plus

Expertise with building and enhancing Machine Learning Models in SageMakerPython

Advanced SQL Query, ETLELTLET and Data Architecture experience

Broad experience in building out VPCs, subnets and network routing on AWS

Experience using Microsoft's SQL Server, SSIS, SSAS, SSRS (Reporting Services), PowerBI Dashboards, and OLAP services

Ability to quickly identify and troubleshoot problematic faults in data pipelines and infrastructure

Ability to design tables, data-marts, andor databases to suit business needs

Knowledge of data mapping, data integration, database design and data warehouse concepts

Experience with Jira, Bitbucket, and GitKraken with a good understanding of Git

Desire and ability to learn emerging technologies and methodologies

Ability to interpret requests and requirements in order to build appropriate automated solutions

Ability to work with shifting deadlines in a fast-paced environment

Strong computer skills including Microsoft Office (Visio, Excel, Word,

Outlook)

Strong attention to detail, interpersonal skills, and notation skills

Kforce is an Equal OpportunityAffirmative Action Employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, pregnancy, sexual orientation, gender identity, national origin, age, protected veteran status, or disability status."
Bungalow,Senior Data Engineer,"United States
Remote","Bungalow is the largest and fastest-growing roommate living company in the United States. We’re on a mission to create a world where everyone can truly feel at home, wherever they want to live.

We work with homeowners across the US to take the hassle out of property management and maximize what they earn so we can provide renters with the best homes the market has to offer.

And we know it can be challenging living with roommates, but we believe it’s worth it. We’re removing the hassle from moving and everyday living, and providing an affordable product that helps people live in the cities where they have better access to opportunity.

Join our community! Bungalow is looking for a Data Engineer to build and manage data science pipelines, contribute to architecture and design, and help to evolve data engineering best practices and standards across the organization.

Working cross-functionally with our engineering, product and data science teams, you will be responsible for enabling the future supply growth of the company - the most meaningful area to impact change in the business during the next 12 months. The data that you help clean, organize and optimize will power data science algorithms to allow the supply sales team to identify, underwrite, and close deals significantly faster, at a higher level of accuracy, and with less humans in the loop.

An ideal candidate will share a passion for our core mission and is fueled by building modern, high-quality products and systems. Someone with a positive attitude, and an empathy-first approach to working with peers and customers - a person who’s not afraid to take on tough challenges with a team.

Our core engineering values are being human focused and team centric – we’re continual learners who give each other permission to fail and take ownership over the things that we build. Our fast-paced, ever-changing environment requires us to move-quickly, and constantly adapt. We are ruthlessly analytical, ensuring our decisions are data-driven, well considered, while maintaining a high bar for quality of delivery.

What You Will Do

Developing and maintaining our 24/7 data pipeline
Working closely with the data science, data analytics and product teams
Maintain our data warehouse and our data lake with up-to-date and quality data
Design robust scalable data architectures
Build automated tools for bootstrapping data science projects, testing, and monitoring performance of machine learning models

Who You Are

3+ years of software/data engineering experience; ideally 2+ years of data engineering experience and 1+ year of software development experience
Strong aptitude with Python and advanced SQL (window functions, analytics functions, joins, etc.)
Familiar with data modelling approaches and their performance trade-off
Hands-on experience implementing ELT (or ETL) best practices at scale
Experience working with tools like Spark, RedShift, Stitch
Experience with monitoring and alerting tools like AWS Cloudwatch or PagerDuty
Strong attention to detail, great communication and an ownership mentality
Team player who can work cross functionally across engineering, data science and business
Comfortable working in a remote environment

Technology Ecosystem

Languages: Python (3.8+)
Frameworks: Airflow
Data: PostgreSQL, AWS Redshift
Ops: AWS, Docker
CI/CD: Github Actions, AWS CodePipeline
Other: Git

What We Value

Build better. Always challenge the status quo, constantly look for ways to improve across the entire organization.
Focus on impact. Ruthlessly prioritize and continue to ask: ""Does it scale?""
Community first. We always look for ways to make our applicants, customers, and teammates feel more at home. We are the steady & trusted source for members new and old.
Feedback obsessed. We are constantly looking for feedback (from customers & teammates) in order to find ways to improve our experience across all facets of the organization. We never settle for the status quo, we consistently drive progress and growth by challenging our mindset, and iterating our processes.
Move fast. We are less afraid of making mistakes than we are of losing opportunities by moving too slowly. We are a culture of builders & innovators.
Be an owner. You are the expert & ruler of your domain. If you see something broken, fix it or work with the team to make sure we are always at our best. Always look for ways to improve organizationally and personally.

More About Our (FTE) Benefits

Whether you work in an office or a distributed team, Bungalow is highly collaborative and yes, fun! To support you at work (and play) we offer some fantastic perks: generous time off to relax and recharge, beautiful offices, commuter benefits, team happy hours and continuous learning! Now the ultra important stuff: we also have world-class healthcare, vision and dental insurance- we have your back!

Applicant Notice

The California Consumer Privacy Policy Act (the ""CCPA"") obligates covered businesses to disclose to consumers (including employees and job applicants), at or before the point of collecting personal information (""PI""), the categories of PI to be collected and the purposes for which the categories of PI shall be used.

In The Course Of The Job Application Process, We May Collect The Following Categories Of PI For The Purposes Of Evaluating You As a Job Applicant

Identifiers such as a real name, alias, postal address, unique personal identifier, online identifier, Internet Protocol address, email address, account name, driver’s license number, passport number, social security number, or other similar identifiers;
Professional, education, or employment-related information;
Other information you voluntarily provide to us, including, without limitation, demographic information, benefits information, or family and dependent information.

Currently, we are implementing procedures in order to reduce the risk of spreading the COVID-19 virus in and throughout the workplace. Therefore, we are notifying you that if you visit our physical offices and facilities for an interview, we also may collect the following additional category of PI: medical and health information. Specifically, we may collect your body temperature along with information about whether you have or display certain symptoms such as fatigue, cough, sneezing, aches and pains, runny or stuffy nose, sore throat, diarrhea, headaches, or shortness of breath, whether you have recently been in close contact with anyone who has exhibited any of these symptoms, and whether you have recently been in contact with anyone who has tested positive for COVID-19.

We will not collect any additional categories of personal information or use your personal information collected for any other purpose without providing you with additional notice consistent with the CCPA.

"
Slack,Staff Data Engineer - Slack,"San Francisco, CA","To get the best candidate experience, please consider applying for a maximum of 3 roles within 12 months to ensure you are not duplicating efforts.

Job Category Products and Technology Job Details Staff Data Engineer

Slack is looking for data engineers to join our Insights team! We build data products that are used across our large enterprise customers to access metrics that are critical to their workflow.

In this role, you will partner cross-functionally with business domain partners, analytics, and engineering to design and implement our data model. You will design, build and scale data pipelines that transform billions of records into measurable data that enable insights.

You will work on initiatives that directly key decision makers within our Enterprise customer base by building and scaling our data models that provide key adoption and engagement metrics.

You have strong technical skills, you are comfortable contributing to an up-and-coming data ecosystem, and you can build a strong data foundation for the company. You are a self-starter, detail and quality oriented, and passionate about having a huge impact at Slack!

What you will be doing


Translate business requirements into data models that are easy to understand and used by different subject areas
Design, implement and build pipelines that deliver data with measurable quality under the SLA
Work partner with product teams, data analysts and engineering teams to build foundational data sets that are trusted, well understood, and aligned with business strategy
Be a champion of the overall strategy for data across multiple teams and different use cases
Increase access to foundational company metrics through process and technical foundations
Identify, document and promote data engineering standard methodologies throughout Slack
What you should have


6+ years of experience working in data architecture, data modeling, master data management, metadata management
Recent accomplishments working with relational as well as NoSQL data stores, methods and approaches (logging, columnar, star and snowflake, dimensional modeling)
A proven track record in scaling and optimizing schemas, performance tuning SQL and ETL pipelines in OLAP and Data Warehouse environments
Proven skills with either Python or Java programming language
Familiar with data governance frameworks, SDLC, and Agile methodology
Excellent written and verbal communication and interpersonal skills, and ability to efficiently collaborate with technical and business partners
Hands-on experience with Big Data technologies (e.g Hadoop, Hive, Spark), which is a huge plus
Bachelor's degree in Computer Science, Engineering or a related field, equivalent training, fellowship, or work experience


Come join us!

Slack has a positive, diverse, and supportive culture—we look for people who are curious, inventive, and work to be a little better every single day. In our work together we aim to be smart, humble, hardworking and, above all, collaborative. If this sounds like a good fit for you, why not say hello?

Slack is registered as an employer in many, but not all, states. If you are not located in or able to work from a state where Slack is registered, you will not be eligible for employment.Visa sponsorship may not be available in certain remote locations.

Visa sponsorship is not available for candidates living outside the country of this position.

Pursuant to the San Francisco Fair Chance Ordinance and the Los Angeles Fair Chance Initiative for Hiring, Salesforce will consider for employment qualified applicants with arrest and conviction records.

For Colorado-based roles: Minimum annual salary of $166,000. You may also be offered a bonus, restricted stock units, and benefits. More details about our company benefits can be found at the following link: https://www.getsalesforcebenefits.com/

At Salesforce we believe that the business of business is to improve the state of our world. Each of us has a responsibility to drive Equality in our communities and workplaces. We are committed to creating a workforce that reflects society through inclusive programs and initiatives such as equal pay, employee resource groups, inclusive benefits, and more. Learn more about Equality at Salesforce and explore our benefits.

Salesforce.com and Salesforce.org are Equal Employment Opportunity and Affirmative Action Employers. Qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender perception or identity, national origin, age, marital status, protected veteran status, or disability status. Salesforce.com and Salesforce.org will not pay any third-party agency or company that does not have a signed agreement with Salesforce.com or Salesforce.org.

Salesforce welcomes all.

Accommodations

If you require assistance due to a disability applying for open positions please submit a request via this Accommodations Request Form .

Posting Statement

At Salesforce we believe that the business of business is to improve the state of our world. Each of us has a responsibility to drive Equality in our communities and workplaces. We are committed to creating a workforce that reflects society through inclusive programs and initiatives such as equal pay, employee resource groups, inclusive benefits, and more. Learn more about Equality at Salesforce and explore our benefits.

Salesforce.com and Salesforce.org are Equal Employment Opportunity and Affirmative Action Employers. Qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender perception or identity, national origin, age, marital status, protected veteran status, or disability status. Salesforce.com and Salesforce.org do not accept unsolicited headhunter and agency resumes. Salesforce.com and Salesforce.org will not pay any third-party agency or company that does not have a signed agreement with Salesforce.com or Salesforce.org .

Salesforce welcomes all.

Pursuant to the San Francisco Fair Chance Ordinance and the Los Angeles Fair Chance Initiative for Hiring, Salesforce will consider for employment qualified applicants with arrest and conviction records.As a federal contractor, Salesforce is required to verify that all US-based employees are fully vaccinated against COVID-19. If you receive an offer and are unable to get vaccinated for religious or medical reasons, you may request a reasonable accommodation.

"
Intuitive.Cloud,Data Engineer,"New York City Metropolitan Area
Remote","Posted by

Rajiv Kannan KG

Sr TAG HR Specialist

Data Engineer
Remote
Long Term Contract

Notes: Look for Data Engineer with Spark, Data lake, glue and Pyspark

MUST HAVE SKILLS:
• Kubernetes
• Amazon EMR
• Java (Spring Boot)
KEY SKILLS:
• AWS Glue
• Data Lake
• Terraform
• IAM
• AWS Lambda
• PySpark
ROLE SCOPE:
• Assist in Kubernetes cluster setup (using Terraform)
• Assist with configuration and implementation of ISTIO application mesh in Kubernetes
• Assist in design and build of customer’s defined infrastructure as code approach (using Terraform)
• Assist in data pipeline architecture design, development and optimization
• Advise customer on AWS best practices
GENERAL REQUIREMENTS:
• AWS Certified
• Senior Consultant
• Understanding of Large-Scale Enterprise AWS Deployments (250+ AWS Accounts)
• We want to have someone who has at least 6 months tenure in AWS (or partner equivalent)
• The desire is to have resources aligned from a time zone perspective in the US
• Experience of leading on one or more AWS ProServe engagement and owning / driving the
scope, in collaboration with the customer
• Strong preference for past experience working with regulated industries (FSI’s, Healthcare,
Government)
• Global Financial Services experience a plus
• All resources will have to go through the JPMC background check; 3-4 week on-boarding period
is needed once a candidate is identified. The identified resource will have to visit a 3rd party lab
for fingerprinting and photo
"
Workday,Senior Data Engineer,"Pleasanton, CA","Do What You Love. Love What You Do.

At Workday, we help the world’s largest organizations adapt to what’s next by bringing finance, HR, and planning into a single enterprise cloud. We work hard, and we’re serious about what we do. But we like to have fun, too. We put people first, celebrate diversity, drive innovation, and do good in the communities where we live and work.

About The Team

The Enterprise Data Services organization in Business Technology takes pride in enabling data driven business outcomes to spearhead Workday’s growth through trusted data excellence, innovation and architecture thought leadership. The team is responsible for developing and supporting Data Services, Data Warehouse, Analytics, MDM, Data Quality and Advanced Analytics/ML for multiple business functions including Sales, Marketing, Services, Support and Customer Experience. We leverage leading modern cloud platforms like AWS, Reltio, Tableau, Snaplogic, MongoDB in addition to the native AWS technologies like Spark, Airflow, Redshift, Sagemaker and Kafka.

About The Role

Come be a part of something big.

If you want to be a part of building something big that will drive value throughout the entire global organization, then this is the opportunity for you. You will be working on top priority initiatives that span new and existing technologies - all to deliver outstanding results and experiences for our customers and employees.

Our Enterprise Architecture and Data Services team is currently looking for a Data Engineer that enables self-service analytics team (Marketing, Sales, Finance and Services) to explore outliers and take actions that differentiate us from our competition. You will work closely with other team members like solution architects, technical leads, and business analysts to understand what the business is trying to achieve, move data from source to target, and design optimal data models. You will also be responsible for building and maintaining the data platform. This hands-on technical role demands excellent knowledge and can demonstrate best practices in the industry.

An ideal candidate will have extensive knowledge of the data warehouse and data engineering using latest tools and Open-source frameworks.

About You

Basic Qualifications

BS/MS in computer science or equivalent is required
Develop and automate high-performance data processing systems to drive Workday business growth and improve the product experience.
Evangelize high quality software engineering practices towards building data models and pipelines at scale.
Build reliable, efficient, testable, & maintainable data pipelines.
Design and Develop data pipelines using Metadata driven ETL Tools and Open-source data processing frameworks.
Hands-on experience with source version control, continuous integration and experience with release/change management delivery tools.
Provide production support and resolve high priority incidents and the development coding issues.
Work with cross functional teams to enable data insights through Data lifecycle.


Other Qualifications

8+ years of experience designing and building scalable and robust data pipelines to enable data-driven decisions for the business.
Experience with very large-scale data warehouses and data engineering projects.
Experience building analytical solutions to Sales, Finance, Product and Marketing teams.
Prior experience with CRM systems like SFDC is required.
Experience developing low latency data processing solutions like AWS Kinesis, Apache Kafka, Apache Spark Stream processing and other Data Integration tools.
Strong experience in one or more programming languages for processing of large data sets, such as Python, Scala.
Working experience with SQL and NoSQL databases. Should be proficient in writing advanced SQLs, Expertise in performance tuning of SQLs.
Experience working with AWS Cloud Data Services like S3, EC2, EMR, Lambda, Redshift etc.
Ability to create enterprise data models, STAR schemas for data consuming.
Extensive experience in troubleshooting data issues, analyzing end to end data pipelines and in working with users in resolving issues
Ability to mentor, guide, and lead associate engineers in the team.


As a federal contractor, Workday is requiring all new hires to verify that they are fully-vaccinated against COVID-19 within 72 hours of beginning employment with Workday, consistent with applicable law. Workday is an equal opportunity employer. Candidates who are not vaccinated due to a sincerely held religious belief, medical reasons, or other legally-protected reason should contact accommodations@workday.com to explore what, if any, reasonable accommodations or exemptions Workday is able to offer.

Pursuant to applicable Fair Chance law, Workday will consider for employment qualified applicants with arrest and conviction records.

Workday is an Equal Opportunity Employer including individuals with disabilities and protected veterans.

Are you being referred to one of our roles? If so, ask your connection at Workday about our Employee Referral process!"
Dice,Sr. Data Engineer,"Sunnyvale, CA
On-site","Dice is the leading career destination for tech experts at every stage of their careers. Our client, Maxonic, Inc., is seeking the following. Apply via Dice today!

Sr. Data Engineer Our client is one of the leading manufacturers within the medical devices industry. Position logistics SUNNYVALE, CA Fulltime Direct Hire JOB DESCRIPTION 2+ years of experience in Kubernetes. CKAD andor CKA certification desired. 2+ years hands on experience in Cloud Deployment. Must be adept in containers and serverless technologies. Has through understanding of Cloud Security model. 3+ years of experience in Python andor C 3+ years of experience in OOP languages 3+ years of experience in writing SQL statements 3+ years of experience in ETL design, implementation, and maintenance 3+ years of experience in distributed systems and databases Comfortable working with any flavor of Linux Excellent written and communication skills About Maxonic Since 2002 Maxonic has been at the forefront of connecting candidate strengths to client challenges. Our award winning, dedicated team of recruiting professionals are specialized by technology, are great listeners, and will seek to find a position that meets the long-term career needs of our candidates. Benefits offered to our employees include Medical Insurance, HSA (Health Savings Account), 401K Plan, Paid Sick Time Off Interested in Applying? Please apply below with your most current resume. Feel free to email Sarita Vaykakkara ( or call )"
Flatiron Health,"Data Reporting Engineer, Spotlight","United States
Remote","We’re looking for a Data Reporting Engineer to join our Spotlight team to help us accomplish our mission to improve lives by learning from the experience of every cancer patient. Here's what you need to know about the role, our team and why Flatiron Health is the right next step in your career.

What you’ll do

In this role, you will join a team of Data Reporting Engineers which curates custom real-world evidence datasets to empower our life science partners to deliver better cancer treatments - deliver them faster, and develop them more cost effectively than existing alternatives. You will be responsible for technical execution of dataset curation, writing SQL and Python in a proprietary Extract Transform and Load (ETL) framework.

In Addition, You’ll Also

You’ll work to dig into data nuances working closely with cross functional team members like Research Oncologists and Quantitative Scientists to help develop new data variables.

Help define business logic rules, using SQL and Python to turn those rules into derived data variables
Write automated SQL checks to ensure data quality and integrity
Integrate and link new data sources, work with upstream Flatiron teams on data requirements and quality thresholds
Identify common needs across dataset requests, and move shared logic into templatized ETL code
Own and improve the library of templatized ETL pipeline code
Become an expert in our data models

Who you are

You’re fluent in SQL, experienced working with complex datasets, excited to learn about the cancer patient journey, and are proactive in collaborating with cross-functional teammates and/or stakeholders. You’re a kind, passionate and collaborative problem-solver who seeks and gives candid feedback, and values the chance to make an important impact.

You have used SQLite and PostgreSQL and/or are familiar with another SQL environment
You have experience working with Python
You communicate effectively and empathetically with cross-functional stakeholders
You are flexible and have exceptional problem-solving skills
You thrive in a cross-functional environment

Extra Credit

You have experience working with ETL frameworks
You have experience working with Pandas
You have healthcare industry knowledge/context (especially oncology specific knowledge)

Why You Should Join Our Team

A career at Flatiron is a chance to work with everyone involved in the future of cancer care and research—all under one roof. Researchers, data scientists, designers, clinicians, technologists and many more all work together to improve cancer care and accelerate research.

We Offer

At Flatiron, we strive to build and maintain an environment where employees from all backgrounds are valued, respected and have the opportunity to succeed. You'll also find a culture of continuous learning, broad and inclusive employee support offerings, and a commitment to supporting our team members in all aspects of their lives—at home, at work and everywhere in between.

Flatiron University training curriculum which includes presentation skills, meeting mastery, coding languages and more
Career coaching opportunities
Hackathons for all employees (not just our engineers!)
Professional development benefit for attending conferences, industry events and external courses
Internal mobility via an internal job board
Opportunities to contribute to Flatiron open-source initiatives
Work/life autonomy via flexible work hours and flexible paid time off
Generous parental leave (16 weeks for either parent)
Flatiron-sponsored fitness classes

Flatiron Health is proud to be an Equal Employment Opportunity employer.

We do not discriminate based upon race, religion, color, national origin, gender (including pregnancy, childbirth, or related medical conditions), sexual orientation, gender identity, gender expression, age, status as a protected veteran, status as an individual with a disability, or other applicable legally protected characteristics.

COVID-19: Please note that, pursuant to Flatiron's health and safety protocol relating to COVID-19, all Flatiron employees, including individuals selected for hire, are required to disclose their COVID-19 vaccination status, and must be fully vaccinated in order to access our offices or travel to customer sites. Vaccination requirements are subject to legally required exemptions for disability or sincerely-held religious beliefs, though exemptions may not be feasible for some external-facing roles."
Axon,"Data Solutions Engineer (Remote, US)","Scottsdale, AZ
Remote","Join Axon and be a Force for Good.

At Axon, we’re on a mission to Protect Life. We’re explorers, pursuing society’s most critical safety and justice issues with our ecosystem of devices and cloud software. Like our products, we work better together. We connect with candor and care, seeking out diverse perspectives from our customers, communities and each other.

Life at Axon is fast-paced, challenging and meaningful. Here, you’ll take ownership and drive real change. Constantly grow as you work hard for a mission that matters at a company where you matter.

We are an equal opportunity employer that promotes justice, advances equity, values diversity and fosters inclusion. We’re committed to hiring the best talent — regardless of race, creed, color, ancestry, religion, sex (including pregnancy), national origin, sexual orientation, age, citizenship status, marital status, disability, gender identity, genetic information, veteran status, or any other characteristic protected by applicable laws, regulations and ordinances — and empowering all of our employees so they can do their best work. If you have a disability or special need that requires assistance or accommodation during the application or the recruiting process, please email recruitingops@axon.com. Please note that this email address is for accommodation purposes only. Axon will not respond to inquiries for other purposes.


Axon requires employees be vaccinated against COVID-19 as a condition of employment, unless a medical or religious accommodation is approved. As such, newly hired employees will be required to provide proof of their COVID-19 vaccination.


Your Impact

This is an entry level role with opportunity for rapid professional growth and the chance to contribute to the success of a software startup with accelerated growth inside a public company. You will collaborate with various departments and teams to position the client for a successful engagement. Come work as a vital member of a client facing, cross-functional team to deliver on the value proposition of our Software Solutions.

What You'll Do

Location: Remotely from the United States.

Reports to: Manager, Integrations Architect

Direct Reports: 0


Work as a member of the Software Services Team
Represent Axon’s technical expertise to an agency throughout all stages of software implementation.
Document an agency’s technical requirements and make the right technical decisions to accomplish the agency’s intended outcomes.
Build customer specific integration pipelines for Extracting Transforming and Loading Digital Evidence and Data from agencies legacy solutions into the Axon SaaS environment.
Document an agency’s system-level requirements (e.g. metadata mapping, network and hardware specifications, end-to-end data flows, as well as overall system capacity, performance, recoverability, etc.).
Assess impact and feasibility of proposed technical solution — including integration into agency environment and lifecycle management — evaluating alternatives and delivering solutions that accomplish agency outcomes.


What You Bring


Bachelors in a technical or a quantitative field or equivalent experience.
Proficient SQL Skills: experience writing complex queries and Statements, Data Definitions, Data modeling and Normalization
Minimum of 1 year experience engineering in a backend language (including Python, SQL, Node.JS, Java, Typescript, or Bash).
Demonstrated knowledge of technical and systems-level solutions for enterprise software.
Demonstrated outcome-driven thinking. Can make hard tradeoff assessments
Professional engineering certifications from a CSP
Demonstrated leadership experience through on-campus and/or community involvement
Self-starter who thrives with problem-solving and embraces ambiguity
Enthusiasm and commitment to AXON's Mission, Core Values, and Program Mission
Willingness to join forces and drive company-wide initiatives across multiple departments
Experience with Azure Databricks and/or Azure Data Pipelines


Benefits That Benefit You


Competitive salary and 401K with employer match
Discretionary paid time off
Robust parental leave policy
An award-winning office/working environment
Ride along with real police officers in real life situations, see them use technology, get inspired
And more...

Benefits listed herein may vary depending on the nature of your employment and the location where you work

NOTE: The above job description is not intended as, nor should it be construed as, exhaustive of all duties, responsibilities, skills, efforts, or working conditions associated with this job. The job description may change or be supplemented at any time in accordance with business needs and conditions.

Some roles may also require legal eligibility to work in a firearms environment."
Nexient,"Data Engineer (Python, Pyspark, Airflow, AWS)","United States
Remote","About Your Future Team

Nexient is on a mission: To rid the world of crappy software, one sprint at a time.

Every day, our 1000+ Agile developers, designers, and strategists empower each other to find passion in our work while we innovate the world. This culture of teamwork and curiosity is fueled by a product-minded approach to our work, crafting extraordinary custom software solutions and growing careers along the way.

Our clients range from some of America’s favorite brands in retail, healthcare, financial services to fast-emerging disruptors – You might have read about us in the NY Times. We’re also recognized as a Gartner Cool Vendor, HFS Hot Vendor, and America’s leading 100% U.S. Agile software services partner.

If you’re looking to change the trajectory of your career, make an impact on Day One, develop custom software on the best technology around – and maybe even make people’s lives easier -- then you belong here, with us.

We are looking to hire Data Engineers with experience in Python, Pyspark/Spark, Airflow, AWS and visualization/reporting experience.

Programming/languages

Python, SQL
JavaScript, Java or Scala, a plus

Data Engineering

Spark/Pyspark (or Flink)
Kafka, queue/messaging paradigms, a plus

Aws

security/networking basics, as well as S3, Kinesis, Glue, RDS, Lambda and Step Functions
Redshift, Athena a plus

Visualization/reporting Experience Would Be a Plus

Why Nexient

As a vital member of our Engineering Practice, you will be part of one of our 100+ small cross-functional teams working side by side with some of the most talented developers, UX designers, analysts, quality engineers, and product managers out there.

From Silicon Valley to Ann Arbor, Columbus, and beyond – a career with Nexient will offer you:

Remote, hybrid, or onsite opportunities: Here, you decide the work arrangement that best suits your needs and preferences.

Fulfilling and challenging projects: You will be exposed to different scenarios, multiple types of clients, and the most complex business and technology initiatives.

Access to the most cutting-edge technologies: You’re always going to be advancing your skillset by learning and working with the newest and most relevant technologies available out there.

A positive and passionate culture: Boring and divided aren’t in our vocabulary. In fact, it’s amazing to see how, despite our exponential growth, the Nexient culture and community stay as warm, diverse, and close as ever.

Plenty of opportunities for you to explore and grow: We will empower you to dream as big as you want and support you to make your career what YOU want it to be.

Creative runway: We take pride in encouraging our people to question the status quo.

Our Benefits Program Includes

Medical, dental, and vision coverage
Generous PTO, plus holidays
401(k) plan
Life insurance and short-term & long-term disability
Maternity leave
Employee Assistance Program
Flexible spending accounts
Health savings account

Nexient is an inclusive, equal opportunity employer. At Nexient, your uniqueness makes us better. Your uniqueness is why you are valued, you are heard, you add to our culture, and you belong. Diversity & Inclusion is hardcoded into our DNA and at the core of everything we do. Together, not separately, we strive to become even more diverse to help fuel our innovation and connect us closer with the clients and communities we serve.

"
Belcan,Data Engineer I,"Santa Clara, CA
On-site","Details

#NowHiring #DataEngineer

Job Title: Data Engineer I

Contract: 11 Months, potential to convert to a permanent position. If offered a permanent position, immediate eligibility for retirement benefits, health insurance, and pay increase.

Belcan is a leading provider of professional IT, Engineering, Workforce Solutions and staffing in the United States, Canada, UK, Europe and India.

A Data Engineer I Job in Santa Clara, CA is currently available through Belcan. In this role you should have hands-on experience with speech analysis, transcription, language annotation, and other forms of data markup. If you are interested in this role, Apply Today!

Job Description

Bachelor's or Master's degree in Linguistics, Computational Linguistics, or a relevant field
2+ years of experience in computational linguistics and natural language processing, language data processing, syntax, and semantics.
Excellent knowledge of aspects of linguistics analysis, particularly on semantics, pragmatics, conversation and discourse analysis. * Hands-on experience with language annotation, speech analysis, transcription, and other forms of data markup.
Native or near-native fluency in US English and Latin American Spanish or Canadian French.
Proficient in regular expressions and Python (and/or other scripting languages).
Experience with language annotation and other forms of data markup.
Excellent communication, strong organizational skills with a keen eye for details.
Comfortable working in a fast-paced, highly collaborative, and dynamic work environment.
Willingness to support several projects at one time and to accept reprioritization as necessary.

Preferred Qualifications

Familiarity with annotation tools, crowdsourcing platforms and workflow (e.g. MTURK, SageMaker Ground Truth)
Hands-on experience with speech analysis, transcription, language annotation, and other forms of data markup.
Experience with machine learning, statistical language modeling.
Practical knowledge of version control and agile development.

Location: Santa Clara, CA

Zip Code: 95054

Keyword""s: #SantaClarajobs; #DataEngineerjobs;

Start Date: Right Away

#ZR

Belcan is a global supplier of engineering, technical recruiting, and IT services to customers in the aerospace, industrial, and government sectors. Belcan engineers"" better outcomes through adaptive and integrated services-from jet engines, airframe, and avionics to heavy vehicles, chemical processing, and cybersecurity. Belcan takes a partnering approach to provide customer-driven solutions that are flexible, scalable, and cost-effective. Our unique capabilities have led to continuous growth and success for 63+ years. We are a team-driven Equal Opportunity Employer committed to workforce diversity."
Amazon Web Services (AWS),Sr. Data & ML Engineer,"Tempe, AZ","Description

At Amazon Web Services (AWS), we’re hiring highly technical Data and Machine Learning engineers to collaborate with our customers and partners on key engagements. Our consultants will develop and deliver proof-of-concept projects, technical workshops, and support implementation projects. These professional services engagements will focus on customer solutions such as Machine Learning, Data and Analytics, HPC and more.

In this role, you will work with our partners, customers and focus on our AWS offerings such Amazon Kinesis, AWS Glue, Amazon Redshift, Amazon EMR, Amazon Athena, Amazon SageMaker and more. You will help our customers and partners to remove the constraints that prevent them from leveraging their data to develop business insights.

AWS Professional Services engage in a wide variety of projects for customers and partners, providing collective experience from across the AWS customer base and are obsessed about customer success. Our team collaborates across the entire AWS organization to bring access to product and service teams, to get the right solution delivered and drive feature innovation based upon customer needs.

You will also have the opportunity to create white papers, writing blogs, build demos and other reusable collateral that can be used by our customers. Most importantly, you will work closely with our Solution Architects, Data Scientists and Service Engineering teams.

The ideal candidate will have extensive experience with design, development and operations that leverages deep knowledge in the use of services like Amazon Kinesis, Apache Kafka, Apache Spark, Amazon Sagemaker, Amazon EMR, NoSQL technologies and other 3rd parties.

Inclusive Team Culture

Here at AWS, we embrace our differences. We are committed to furthering our culture of inclusion. We have thirteen employee-led affinity groups, reaching 85,000 employees in over 190 chapters globally. We have innovative benefit offerings, and host annual and ongoing learning experiences, including our Conversations on Race and Ethnicity (CORE) and AmazeCon (gender diversity) conferences. Amazon’s culture of inclusion is reinforced within our 16 Leadership Principles, which remind team members to seek diverse perspectives, learn and be curious, and earn trust.

Work/Life Balance

Our team puts a high value on work-life harmony. Striking a healthy balance between your personal and professional life is crucial to your happiness and success here. We are a customer-obsessed organization—leaders start with the customer and work backwards. They work vigorously to earn and keep customer trust. As such, this is a customer facing role in a hybrid delivery model. Project engagements include remote delivery methods and onsite engagement that will include travel to customer locations as needed.

Mentorship & Career Growth

Our team is dedicated to supporting new members. We have a broad mix of experience levels and tenures, and we’re building an environment that celebrates knowledge sharing and mentorship. We care about your career growth and strive to assign projects based on what will help each team member develop into a better-rounded professional and enable them to take on more complex tasks in the future.

This is a customer facing role. You will be required to travel to client locations and deliver professional services when needed.


Basic Qualifications

Bachelor’s degree in Computer Science, Engineering, Mathematics or a related field or equivalent professional or military experience
8+ years of experience of Data platform implementation
3+ years of hands-on experience in implementation and performance tuning of Kinesis, Kafka, Spark or similar implementations
Hands on experience with building data or machine learning pipeline
Experience with one or more relevant tools (Flink, Spark, Sqoop, Flume, Kafka, Amazon Kinesis)
Experience developing software code in one or more programming languages (Java, JavaScript, Python, etc)
Current experience with hands-on implementation

Preferred Qualifications

Masters or PhD in Computer Science, Physics, Engineering or Math.
Familiar with Machine learning concepts
Hands on experience working on large-scale data science/data analytics projects
Hands-on experience with technologies such as AWS, Hadoop, Spark, Spark SQL, MLib or Storm/Samza.
Experience Implementing AWS services in a variety of distributed computing, enterprise environments.
Experience with at least one of the modern distributed Machine Learning and Deep Learning frameworks such as TensorFlow, PyTorch, MxNet Caffe, and Keras.
Experience building large-scale machine-learning infrastructure that have been successfully delivered to customers.
Experience defining system architectures and exploring technical feasibility trade-offs.
3+ years experiences developing cloud software services and an understanding of design for scalability, performance and reliability.
Ability to prototype and evaluate applications and interaction methodologies.
Experience with AWS technology stack.
Written and verbal technical communication skills with an ability to present complex technical information in a clear and concise manner to a variety of audiences.

Amazon is committed to a diverse and inclusive workplace. Amazon is an equal opportunity employer and does not discriminate on the basis of race, national origin, gender, gender identity, sexual orientation, protected veteran status, disability, age, or other legally protected status. For individuals with disabilities who would like to request an accommodation, please visit https://www.amazon.jobs/en/disability/us.


Company - Amazon Web Services, Inc.

Job ID: A1414215"
Retina AI,Python Data Integrations Engineer,"Santa Monica, CA","At Retina, our team (former Facebook, PayPal, and Stanford alums) has built a product that can predict individual customer lifetime value at the earliest point of customer contact and changed how brands view their relationships with their customer. We use data science to predict future buying behavior of consumers and recommend actions that businesses can take around those predictions.

Our APIs connect directly to a company’s data warehouse and start to make recommendations on how they should move marketing, customer service and retention dollars towards the more profitable customers. In customer acquisition alone, Retina has the opportunity to optimize a $389 billion industry spend.

We recently raised our series A funding and work with brands like Dollar Shave Club, Nestlé, Madison Reed, and Capital One. We have more than 10x our revenue in the last year and we’re looking for the brightest minds who believe in data driven decision making at scale to join our growing team!

Overview:

At Retina AI, we use Python as the ""glue"" between our various data systems. This is the first position at Retina dedicated to that role, and offers an opportunity to have a large impact across multiple parts of the organization.

Responsibilities:

Develop and deploy new data integrations between the Retina platform and external data systems. Including: data warehouses, web APIs, and customer data platforms
Orchestrate our data science models and build out automated monitoring and alerts. Take the models built by our data scientists into production.
Collaborate with Retina data scientists and engineers to build out scalable solutions to new needs


Qualifications:

3+ years of Python 3 development experience across multiple applications.
2+ years of experience with SQL Databases: Example: Snowflake, AWS Kinesis, Redshift, MySQL
1+ years of experience interfacing with Web/REST-ful APIs. Example: Google Ads, Facebook Ads
Polyglot software engineering mindset: find the right tool for the job and master it
Strong problem solving skills and attention to detail


Preferred:

Experience with ""big data"" systems such as Apache Spark / Databricks
Marketing data analytics systems
AWS Cloud Experience
Experience in data security


We are a people-oriented company; this means taking care of our team members is critical to our success. We believe that if we hire the smartest people and invest in them, a high-performing team -- and subsequently a high performing company -- will flourish.

Here's why you'll love working at Retina:

Ownership:

Take ownership of products, client relationships, and the trajectory of your role. Working in a startup means you’re a vital member of a small team — and your work will definitely be recognized.

Exciting challenges:

Our product is solving business challenges in new and exciting ways. You’ll deliver capabilities companies don’t even know they need yet, and learn amazing things along the way.

Innovation:

Your coworkers will be the type of people who take risks, welcome new perspectives, and live for great new ideas. Learn how to approach problems differently and with agility.

Unlimited opportunities:

Encounter endless opportunities that fall outside of your wheelhouse. At Retina, you will try out new skills and gain valuable experience that puts you on a career path you love.

Great culture:

You can always rely on your team. Our culture is all about learning, growing, and working together to build something great. Find out what it’s like to work with the best.

We also have some amazing benefits and perks that include:

Health Coverage: 99% health coverage for employees and 75% for dependents. (BlueShield PPO Platinum)

Vacation: Unlimited vacation and ample sick leave

Setup Your Own Kit: Buy what you need to get a comfortable work environment (Pick your laptop, headphones and any other accessories needed)

Conferences: 1 Paid conference and unlimited conferences where you present)

Health and Education: $250 per month towards Gym, Books, Audiobooks, or Safari Books online

Meal & Coffee Card: $250 Debit card each month for meals, coffee, etc."
Scientific American,Data Analytics Engineer,"New York, NY","Posted by

Holly E. Ciccarella

US Recruitment Manager at Springer Nature

Job description:







Springer Nature is seeking a Data Analytics Engineer for its New York based Scientific American Team. The team develops new data products for the research community. This is an exciting opportunity for the Data Analytics Engineer, expanding from strong foundations to build new solutions and services. We are looking for someone who is able to deliver solutions and work independently, with support from the wider team where necessary.







As a Data Analytics Engineer, you will be responsible for ensuring continuous flow of data with minimum latency between data sources. You will be developing, testing and deploying data pipelines into the production environment. You’ll be responsible for implementation of Google Analytics 4 server side tracking, following the existing solution that will be provided. You’ll be analyzing big data from very highly trafficked websites and content. You will provide actionable analysis and insight into the behavior of users. Driving change that improves their experience with SpingerNature and our customers, contributing to our purpose to advance discovery with some of the most interesting datasets available.







You’ll be working in close partnership with data analysts, scientists and engineers and researchers from Springer Nature. You’ll be working with the latest data and analytics technologies including graph databases, Google Analytics, Google Tag Manager, BigQuery, Looker and Plotly Dash as well as previewing solutions from Google and other partners.











Role responsibilities will include:



Build streaming/batch Data pipelines for extraction/loading/transforming data between various data sources at scale in different formats.
Work closely with Data Scientists /Analysts to understand the requirements and develop the data solutions in line with the business requirements.
Maintain the current cloud infrastructure and help onboard the new applications.
Developing sophisticated segmentation, analysis and dashboards that support Springer Nature and its customers to advance discovery.
Implementing cutting edge analytics and data science solutions in partnership with the wider teams
Consulting with stakeholders throughout the business to shape and implement solutions in support of business objectives.
Play a key role in shaping solutions, creating the right governance and safety checks to maintain tracking accuracy.





Role requirements:



BA degree with a strong analytical/quantitative background or equivalent experience (e.g. Data Science, Statistics, Mathematics, Econometrics, Physics, Computer Science etc.)
Strong working knowledge of SQL, Google Analytics, Google Tag Manager and Python
Excellent problem solving capabilities
Knowledge of Machine Learning concepts is beneficial but not essential as training will be provided
Prior experience with schema designing data modeling
Familiarity with Google Cloud products (BigQuery, Colab, Data Studio, Looker, Dataform, Google Analytics) or other cloud data platforms is beneficial but not essential
Well organized and accurate with good time management











Desired Skills and Experience

N/A"
Carta Healthcare,Data Engineer,"California, United States
Remote","At Carta Healthcare, we believe in a multidisciplinary approach to solving problems. Our mission is to automate and simplify the work that burns out clinical staff, so they can focus on patient care. Our AI Enabled Technology offers a complete solution (people, process and technology) to support the Healthcare Registry Data Market. We design products that transform the way hospitals use data to deliver care. We make analyzing data fast, easy, and useful for everyone. We give clinicians time back to focus on research and care that improve patient lives by reducing paperwork. Carta Healthcare is a remote organization with headquarters in San Francisco and Portland, Oregon. To learn more about our AI Enabled Solutions and more about our company, please visit www.carta.healthcare




We are looking for a Data Engineer responsible for managing the interchange of data between the server and the users. Your primary focus will be development of all server-side logic, building data warehouses, and managing business intelligence applications, which are used by some of marquee clients in healthcare. You will also be responsible to ensure high performance and responsiveness to requests from the front-end. A basic understanding of front-end technologies is necessary as well.




Are you driven to make an impact in healthcare? We believe that the path to better patient outcomes starts with more fulfilled, better utilized clinicians who are liberated to focus on their true calling, helping patients. At Carta, we design products that transform the way hospitals use data to deliver care. We make analyzing data fast, easy, and useful for everyone.

You love solving complex engineering problems. You’re a positive individual who cultivates conceptual thinking and brings a curiosity and experimental approach to solving engineering challenges. You enjoy building new features and writing elegant code. If this describes you--you belong at Carta.

Responsibilities:

Analyze and problem solve production deployments
Manage client configuration following our agile process
Develop infrastructure to translate data to our FHIR knowledge graph
Collaborate with other team members and stakeholders
Help design our client configuration
Plan and coordinate production engineering processes on a daily basis to produce high quality products.
Perform engineering analysis to reduce downtime and outages.
Provides training and guidance to team members to accomplish production goals.
Stay current with product specifications, engineering technology and production processes.
Investigate problems, analyze root causes and derive resolutions.




What you’ll need:

Degree in Computer Science or related field
Python expertise (4+ years experience) - Pandas, Jupyter, Flask, Docker
Postgres or SQL expertise
Experience building data pipelines.
Experience with structured Enterprise Architecture practices, hybrid cloud deployments, and on-premise-to-cloud migration deployments and roadmaps
Experience in network infrastructure, security, data or application development
A hands-on, engaged approach to solving problems
Excellent communication skills and experience in collaborative environments
The desire to be continually learning about emerging technologies/industry trends




Why we love Carta Healthcare, and why you will too!

Industry leading products
Work hard, and have fun doing it
Work alongside some of the most talented and dedicated teammates
Mission driven
Competitive benefits package including great healthcare benefits and 401k"
Rover.com,"Data Engineer II, Data Platform","Seattle, WA","Who We Are:

Want to make an impact? Join our pack and come work (and play!) with us.

We believe everyone deserves the unconditional love of a pet—and at Rover, our mission is to make it easier to experience that love. Founded in 2011, the Rover app and website connect dog and cat parents with loving pet sitters and dog walkers in neighborhoods across the US, Canada, and Europe. We empower our community of trusted pet sitters and dog walkers to run their own pet care businesses on Rover with the tools and security of a global company to back them.

Headquartered in Seattle, Washington, we work closely with our teams in Barcelona, London, San Antonio, Spokane and remote locations. We’ve got a reputation for being a great place to work, having been named among the 100 Best Companies to Work For in Seattle Business Magazine and Washington’s Best Workplaces in the Puget Sound Business Journal. We're an agile, fast-growing company, and our leadership comes from some of the world's most respected tech companies.

At Rover, our furry coworkers are just as important as our human ones—and we wouldn’t have it any other way. Along with making the joys of pet parenthood more accessible, we’re committed to fostering a diverse, inclusive, and welcoming community of pet people—and that starts with our employees. Want to join our pack? Come work (and play!) with us.

Data Engineering at Rover:

Data powers everything at Rover. We are a small team that works closely with all parts of the organization to ensure that the data available to the business is fresh and accurate, our application databases are healthy, and our reporting infrastructure meets the needs of everyone from analysts to the C-suite. We love fine-tuning our databases, breathe observability, and enjoy building self-serve tools that reduce time to insights for our users!

Do you want to help modernize our Data Platform by contributing to our ETL platform? Perhaps you're passionate about the customer, and want to build tooling that makes machine learning model development a breeze. We touch all things data-related at Rover, which means you'll get the chance to work on exciting and impactful projects and learn a ton along the way!

Your Responsibilities:

Build tools that empower our engineers, data scientists, and analysts to quickly get key business insights.
Design, develop, and test high-performance batch and stream data processing systems and tools.
Modernize and monitor Rover's infrastructure and data pipelines to ensure Lights On, Doors Open!


Required Skills:

3+ years of data engineering or relevant industry experience.
Strong coding skills in both data and software engineering contexts.
Experience developing ETL tools and/or managing data infrastructure.


Nice To Have:

Experience building self-service data tooling for Data Scientists and Analysts.
Passionate about automated testing and delivering good quality code.
Experience working with tools like Airflow or Luigi to schedule ETL jobs.
Familiarity with our tech stack: Python, Redshift, MySQL, Postgres, Kafka, Spark, Docker, Kubernetes, Terraform.


Benefits Of Working At Rover:

Competitive compensation and benefits package including stock options, 401k match, flexible PTO, commuter benefits and medical, dental, and vision insurance
Also, pet benefits! Including $1000 toward adopting your first dog or cat
Stocked fridges, coffee, soda, and lots of treats (for humans and dogs) and free catered lunches semi-monthly
Regular team activities, including happy hours, snow tubing, game nights, and more


Rover is licensed to let employees work remotely in the following states: WA, CA (no San Fran), TX, FL, MT, WI and NY (no NYC).

Rover is an equal opportunity employer committed to promoting a diverse, inclusive and inventive environment with the best employees. We’re driven by seeing our people succeed and grow, and we work to ensure everyone contributes to their fullest potential. We consider all qualified applicants without regard to age, race, color, ancestry, national origin, religion, disability, protected veteran status, sex, gender identity or expression, sexual orientation, or any other protected status in accordance with applicable laws, regulations and ordinances.

We are committed to work with you to look for reasonable accommodation to participate in the job application or interview process, to perform essential job functions, and to receive other benefits and privileges of employment. Please contact us to request accommodation."
Foot Locker,Data & Analytics Engineer,"St Petersburg, FL
Remote","Overview

Our global house-of-brands inspires and empowers youth culture. Relentlessly committed to fuel a shared passion for self-expression, we create unrivaled experiences at the heart of the sport and sneaker communities through the power of our people. If you want to be a part of something bigger than you can imagine, you’ve come to the right place. To learn more about the incredible impact we’re making on both our local and global communities, Click Here!

Foot Locker, Inc. is seeking an experienced individual who has a proven track record of building enterprise level platform components to support product development from multiple teams and lines of business. This role is expected to drive engineering and innovation through collaboration across our Data Platforms, Data Architecture, Data science and our products team to help push Foot Locker, inc. to the next level. Our current Data & Analytics Platform technology stack comprises of cloud solutions (Microsoft Azure), Databricks, Confluent Kafka, Snowflake and Power BI. Ideal candidate is expected to have a engineering experience around using tools building ETL, ELT Pipeline and Business Intelligence Dashboards. The Data & Analytics Engineer does not have direct supervisory responsibilities

Responsibilities

Build new Data and Analytics products helping support Foot Locker business initiatives.
Help grow our Data Catalog through ingestions of a variety of internal and third party data sources
Must be able to support and contribute to assigned POD’s at an Data & Analytics engineering capacity with minimal supervision working within the Agile / Scrum project methodology
Define data standards that will enable more efficient and effective decision making
Design, Engineer and Implement ETL solutions and resolve technical issues
Design Power BI datasets and dashboards, including complex DAX functions.
Participate in a collaborative, peer review based environment fostering new ideas via cross team guilds / specialty groups
Maintain comprehensive documentation around our processes / decision making
Participate in the continuous evolution of our schema / data model as we find more data sources to pull into the platform
Support our Data Scientists by helping enhance their machine learning models to be more scalable during training and model deployment
Qualifications

Bachelors Degree in Computer science or related field.
Minimum 3 years of overall Engineering experience in a Data and Analytics function.
Strong knowledge and experience working as a Data Engineer using industry standard ETL tools.
Strong knowledge and experience with Datawarehouse.
Experience with one or more Reporting/Dashboarding tool.
Demonstrated experience with agile scrum methodology
Must possess well-developed verbal and written communication skills.
Previous experience at a fortune 500 company preferred

Must Have

Advanced SQL skills
Spark experience
Basic Python knowledge
Datawarehouse Skills

Nice to Have (A Plus)

Cloud technology experience
Strong knowledge and experience with Power BI or equivalent BI Tools.
Prior knowledge and experience in Dimensional modeling (Data Marts, Databricks/Star/Snowflake, Normalization, SCD2).
Retail experience"
Digital Additive,Data Engineer,"Atlanta, GA
Hybrid","Posted by

Sheri Kornblum

Senior Talent Acquisition Manager at Digital Additive

Send InMail

Digital Additive is redefining the digital marketing space in Atlanta— and we’re growing! Our team is currently seeking a Jr. Data Engineer – Marketing Analytics.




Our team strives to exemplify Digital Additive’s commitment to one-to-one communications and demonstrate the care and value we place on the work we do. We are a trusted strategic partner to our clients – helping them turn their brand, data, and technology investments into actionable marketing projects.




Who Are We Looking For?




Do you have a passion for customer messaging, “done right,” and a desire to grow your career? As a Jr. Data Engineer – Marketing Analytics you will experience an Atlanta-based eCRM (electronic Customer Relationship Management) focused agency specializing in utilizing the Salesforce Marketing Cloud Platform.




You will get to be side by side some of the most talented, dedicated, and fun digital marketers in the industry. You will be part of a team that is dedicated to meeting our clients’ goals while providing you an opportunity to grow both personally and professionally.




What you will be doing:




You will assemble large, complex data sets that meet analytics and business requirements.
You will build tools to provide actionable insight into key business performance metrics to optimize our services and create value for our clients.
Guided by our Analytics Director you will leverage best practices and functionality expanding and optimizing our data and data pipeline architecture.
You will assist with identifying, designing, and implementing internal process improvements including re-designing infrastructure for greater scalability, optimizing data delivery, and automating manual processes to support our current and future data initiatives.
You will work across all teams implementing and/or creating methods to improve data reliability and quality as well as optimizing data flow and collection.
You will spearhead best practices that enable a quality product while keeping our data separated and secure through multiple data centers and AWS regions.




About you:




Data and metrics excite you. You enjoy optimizing data systems and building them from the ground up. All while striving to increase our client’s ability to predict their customers behavior.
You have a desire to learn and grow as a data pipeline builder and data wrangler.
Working knowledge of SQL and relational databases and/or AWS cloud services: EC2, EMR, RDS, Redshift, Matillion, as well as familiarity with object-oriented/object function scripting languages: Python, Java, C++, Scala, etc.
You have a desire to learn as someone new to the Data Engineer field or as a recent graduate with a bachelor’s degree in Computer Science, Statistics, Informatics, Information Systems or another quantitative field.
Should you not have a degree you have proven work experience in a relevant position.
Visa Sponsorship not available.







More Details




At Digital Additive, our employees’ health and financial wellbeing along with personal and career growth are top priority. Our benefits package reflects our core values (Smart, Conscientious, Genuine, Nimble and Dedicated). We expect the best from our team, and we strive to give the best back to our talented team members.




Our Commitment




At Digital Additive, we understand the value of genuine communication. That’s why we are committed to creating a space where a diverse mix of talented people have the opportunity to do their best work."
Toptal,Senior Data Engineer,"Los Angeles, California, United States
Remote","About The Job

Toptal developers work with speed and efficiency to deliver the highest quality of work. We are looking for someone who is passionate about their client’s business, and ready to work on exciting projects with Fortune 500 companies and Silicon Valley startups, with great rates and zero hassles. If you are looking for a place to advance your career, enhance your skill set, and build connections around the globe, Toptal is right for you.

About Toptal

Toptal is an exclusive network of top freelancers from around the world. Fortune 500 companies and Silicon Valley startups hire Toptal for their most important projects. Toptal is one of the fastest-growing fully remote networks and empowers freelance software developers, designers, finance experts, product managers, and project managers worldwide to grow and excel in their freelance careers.

Toptal clients vary in sizes and industries, from enterprise organizations and big tech companies to Silicon Valley startups and renowned universities. Once you enter the network, our matchers will contact you with project opportunities that fit your expertise and preferences. We have experts in over 120 countries who get to work remotely on projects that meet their career ambitions.

About The Role

As a Data Engineer, your main goal is to be one step ahead of data scientists and analysts. You will support them by providing infrastructure and tools they can use to deliver end-to-end solutions to business problems that can be developed rapidly and maintained easily. This is more than building and maintaining ETL pipelines. We need innovation, creativity, and solutions that will have a significant impact on the client’s velocity.

Requirements

3+ years of professional experience in software development
Working experience with Python and Pandas.
Familiarity with the basic principles of distributed computing and data modeling.
Extensive experience with object-oriented design and coding and testing patterns, including experience with engineering software platforms and data infrastructures.
Working experience with Airflow and Luigi is a big plus.
Working experience with Scala is a plus.
Familiarity with Google Cloud Platform (e.g. GCS and BigQuery) is a plus.
Working experience with Dimensional Modeling and Rails is a plus.
Outstanding communication and interpersonal skills.
Full-time availability is a strong advantage

If you’re interested in pursuing an engaging career working on full-time freelance jobs for exclusive clients, take the next step by clicking apply and filling out the short form to get started.

#RemoteJobDataEngineering

"
Avanade,Azure Data Engineer,"Los Angeles, CA","Job Description

Some see forests. You see trees—by species, height, leaf shape, and role in the ecosystem. Come be our branch manager.

Explore the opportunity below and see if Avanade is right for you.

Avanade and you

You’re passionate about next-generation technologies and always looking to build new skills. Come help define the future by creating and implementing solutions in the Microsoft ecosystem. You’ll learn, you’ll grow, you’ll get to do some really cool stuff.

Together we go beyond

We have bold ambitions and are always ready to add people to our diverse team so we can stretch the boundaries of innovation and success. 

The work

You know how to translate a bunch of disparate data points into a cohesive picture. You collect, aggregate, store and reconcile data from various sources. You can design and build pipelines, streams, reporting tools, generators and more to provide information and insight. You know how to read the patterns and trends that influence business outcomes.

The opportunity

Our clients—notable companies, non-profits, and government agencies around the world—need your skills with data. We need them, too! We have a solid track record of digital innovation and big ideas and ambitions to keep doing more. Data is at the core of it all.

The team

You’ll be surrounded by people who share your enthusiasm and/or skills. And you’ll be in a place that values different ideas, perspectives, and experiences. We ask everyone to bring their full, unique selves to Avanade, along with an openness to grow and change in their understanding of the world.

The community

The Analytics community could easily be voted ""Least likely to get caught in a rainstorm without an umbrella."" Finding data, organizing it and then making sense of it to inform decisions is the lifeblood of this group.

Your Experience

Likely includes transforming business needs into technical solutions. Being a Microsoft Certified Solutions Associate, Solutions Expert or Database Administrator is a plus. You also have a bachelor’s or master’s degree in a quantitative field like computer science, applied mathematics, statistics, or machine learning—or the equivalent combination of education and experience.

Your skills

Succeeding in this job means you likely possess professional skills around data profiling, cataloguing, and mapping, and transforming business needs into technical solutions. We require all potential applicants to have hands-on experience with: Azure Data Factory, Azure Data Lake, Azure Databricks. The ideal candidate also has experience working with SQL, Spark/ PySpark and ELT/ETL pipelining.

What We Offer You 

Distinctive experiences 

Leading organizations—corporations, non-profits, government agencies—look to Avanade to help them make the most of their investment in Microsoft and to push the boundaries of digital innovation. The result? Interesting work that is well supported, always evolving, and sure to keep you engaged and interested. 

Limitless learning 

We give lifelong learners a rich menu to choose from, drawing from our relationship with Microsoft and other partners. We’ll back you up as you pursue big ideas and give you honest feedback to move forward advance your growth and our clients’. 

Ambitious growth 

We think big and achieve, knowing we have a global team of people who are just as passionate about innovating as we are and eager to grow with us. Growth at Avanade doesn’t have to look like a traditional ascent up the career ladder (although that is an option). We welcome people to go deep into their specialties and to embrace agility and go after new skills in different areas of the business 

Total rewards 

We are always looking for ways to elevate our employee experience to give our people opportunities to share in Avanade's success. In addition to a competitive salary and attractive benefits, we offer an employee stock purchase plan, recognition programs with awards tied to them, and additional perks like flexible work arrangements and thriving employee networks. 

We’re interested in getting to know you and figuring out what we can do together. Hit Apply Now and let’s start the conversation. 

Avanade requires all new hires to be fully vaccinated against COVID-19 as a condition of employment. Avanade will consider requests for medical or religious accommodation to this vaccination requirement on an individual basis.

About Avanade

Avanade is the leading provider of innovative digital, cloud and advisory services, industry solutions and design-led experiences across the Microsoft ecosystem. Every day, our 56,000 professionals in 26 countries make a genuine human impact for our clients, their employees and their customers.

We have been recognized as Microsoft’s Global SI Partner of the Year more than any other company. With the most Microsoft certifications (60,000+) and 18 (out of 18) Gold-level Microsoft competencies, we are uniquely positioned to help businesses grow and solve their toughest challenges.

We are a people first company, committed to providing an inclusive workplace where employees feel comfortable being their authentic selves. As a responsible business, we are building a sustainable world and helping young people from underrepresented communities fulfil their potential.

Majority owned by Accenture, Avanade was founded in 2000 by Accenture LLP and Microsoft Corporation. Learn more at www.avanade.com ."
Avanade,Azure Data Engineer,"Los Angeles, CA","Job Description

Some see forests. You see trees—by species, height, leaf shape, and role in the ecosystem. Come be our branch manager.

Explore the opportunity below and see if Avanade is right for you.

Avanade and you

You’re passionate about next-generation technologies and always looking to build new skills. Come help define the future by creating and implementing solutions in the Microsoft ecosystem. You’ll learn, you’ll grow, you’ll get to do some really cool stuff.

Together we go beyond 

We have bold ambitions and are always ready to add people to our diverse team so we can stretch the boundaries of innovation and success. 

The work 

You know how to translate a bunch of disparate data points into a cohesive picture. You collect, aggregate, store and reconcile data from various sources. You can design and build pipelines, streams, reporting tools, generators and more to provide information and insight. You know how to read the patterns and trends that influence business outcomes.

The opportunity 

Our clients—notable companies, non-profits, and government agencies around the world—need your skills with data. We need them, too! We have a solid track record of digital innovation and big ideas and ambitions to keep doing more. Data is at the core of it all.

The team 

You’ll be surrounded by people who share your enthusiasm and/or skills. And you’ll be in a place that values different ideas, perspectives, and experiences. We ask everyone to bring their full, unique selves to Avanade, along with an openness to grow and change in their understanding of the world.

The community 

The Analytics community could easily be voted ""Least likely to get caught in a rainstorm without an umbrella."" Finding data, organizing it and then making sense of it to inform decisions is the lifeblood of this group.

Your Experience

Likely includes transforming business needs into technical solutions. Being a Microsoft Certified Solutions Associate, Solutions Expert or Database Administrator is a plus. You also have a bachelor’s or master’s degree in a quantitative field like computer science, applied mathematics, statistics, or machine learning—or the equivalent combination of education and experience.

Your skills

Succeeding in this job means you likely possess professional skills around data profiling, cataloguing, and mapping, and transforming business needs into technical solutions. We require all potential applicants to have hands-on experience with: Azure Data Factory, Azure Data Lake, Azure Databricks. The ideal candidate also has experience working with SQL, Spark/ PySpark and ELT/ETL pipelining.

What We Offer You 

Distinctive experiences 

Leading organizations—corporations, non-profits, government agencies—look to Avanade to help them make the most of their investment in Microsoft and to push the boundaries of digital innovation. The result? Interesting work that is well supported, always evolving, and sure to keep you engaged and interested. 

Limitless learning 

We give lifelong learners a rich menu to choose from, drawing from our relationship with Microsoft and other partners. We’ll back you up as you pursue big ideas and give you honest feedback to move forward advance your growth and our clients’. 

Ambitious growth 

We think big and achieve, knowing we have a global team of people who are just as passionate about innovating as we are and eager to grow with us. Growth at Avanade doesn’t have to look like a traditional ascent up the career ladder (although that is an option). We welcome people to go deep into their specialties and to embrace agility and go after new skills in different areas of the business 

Total rewards 

We are always looking for ways to elevate our employee experience to give our people opportunities to share in Avanade's success. In addition to a competitive salary and attractive benefits, we offer an employee stock purchase plan, recognition programs with awards tied to them, and additional perks like flexible work arrangements and thriving employee networks. 

 We’re interested in getting to know you and figuring out what we can do together. Hit Apply Now and let’s start the conversation. 

Avanade requires all new hires to be fully vaccinated against COVID-19 as a condition of employment. Avanade will consider requests for medical or religious accommodation to this vaccination requirement on an individual basis.

About Avanade

Avanade is the leading provider of innovative digital, cloud and advisory services, industry solutions and design-led experiences across the Microsoft ecosystem. Every day, our 56,000 professionals in 26 countries make a genuine human impact for our clients, their employees and their customers.

We have been recognized as Microsoft’s Global SI Partner of the Year more than any other company. With the most Microsoft certifications (60,000+) and 18 (out of 18) Gold-level Microsoft competencies, we are uniquely positioned to help businesses grow and solve their toughest challenges.

We are a people first company, committed to providing an inclusive workplace where employees feel comfortable being their authentic selves. As a responsible business, we are building a sustainable world and helping young people from underrepresented communities fulfil their potential.

Majority owned by Accenture, Avanade was founded in 2000 by Accenture LLP and Microsoft Corporation. Learn more at www.avanade.com ."
IBM,Data Engineer- Security Analytics & Reporting,"Cambridge, MA","526316BR

Introduction

As a Data Scientist at IBM, you will help transform our clients’ data into tangible business value by analyzing information, communicating outcomes and collaborating on product development. Work with Best in Class open source and visual tools, along with the most flexible and scalable deployment options. Whether it’s investigating patient trends or weather patterns, you will work to solve real world problems for the industries transforming how we live.

Your Role and Responsibilities

Data Engineers work with Data Analysts and Data Scientists to improve the quality and accuracy of the information, enabling businesses to make more responsible and informed decisions.

We are looking for an experienced data engineer to join our team:


You will use various methods to transform raw data into useful data systems.
You will create data models to facilitate statistical analysis and optimal data retrieval.
You will strive for efficiency by aligning data systems with business goals.
You will have strong analytical skills and the ability to combine data from different sources creating effective data models that will provide accurate business answers.


Above all, we are looking for applicants who will thrive in an open, vibrant, flexible, fun-spirited, collaborative environment and desire creative freedom and an opportunity to work on high performing teams. If you are detail-oriented, with excellent organizational skills and experience in this field, we’d like to hear from you.

Job Duties


Analyze and organize raw data
Build data systems and pipelines
Evaluate business needs and objectives
Conduct complex data analysis and report on results
Prepare data for prescriptive and predictive modeling
Combine raw information from different sources
Explore ways to enhance data quality and reliability
Identify opportunities for data acquisition communicating with internal customers
Work in an agile, collaborative environment
Collaborate with data analysts, data scientists and architects on several projects
Build algorithms and prototypes


About The Team

The SOS Security Analytics & Reporting Tribe

SYSTEMSPROUD

Required Technical and Professional Expertise


4+ years of experience developing and implementing data models
2+ years of experience writing and analysing complex SQL
1+ years Cognos (or similar) experience
1+ years of experience in data analytics, statistics, or similar field.
Exceptional technical knowledge in data modeling
Excellent technical understanding of databases and data storage systems
Excellent problem-solving skills
Excellent communication and collaboration skills


Preferred Technical And Professional Expertise


Understanding of security and compliance
Experience with dashboards and reports design
Experience with Python, R, Java or similar


About Business Unit

IBM Systems helps IT leaders think differently about their infrastructure. IBM servers and storage are no longer inanimate - they can understand, reason, and learn so our clients can innovate while avoiding IT issues. Our systems power the world’s most important industries and our clients are the architects of the future. Join us to help build our leading-edge technology portfolio designed for cognitive business and optimized for cloud computing.

This job requires you to be fully COVID-19 vaccinated prior to your start date, where legally permissible. Proof of vaccination status will be required. If you are unable to be vaccinated due to medical, pregnancy or religious reasons, we offer accommodations in accordance with applicable law.

Your Life @ IBM

Are you craving to learn more? Prepared to solve some of the world's most unique challenges? And ready to shape the future for millions of people? If so, then it's time to join us, express your individuality, unleash your curiosity and discover new possibilities.

Every IBMer, and potential ones like yourself, has a voice, carves their own path, and uses their expertise to help co-create and add to our story. Together, we have the power to make meaningful change – to alter the fabric of our clients, of society and IBM itself, to create a truly positive impact and make the world work better for everyone.

It's time to define your career.

About IBM

IBM’s greatest invention is the IBMer. We believe that through the application of intelligence, reason and science, we can improve business, society and the human condition, bringing the power of an open hybrid cloud and AI strategy to life for our clients and partners around the world.

Restlessly reinventing since 1911, we are not only one of the largest corporate organizations in the world, we’re also one of the biggest technology and consulting employers, with many of the Fortune 50 companies relying on the IBM Cloud to run their business.

At IBM, we pride ourselves on being an early adopter of artificial intelligence, quantum computing and blockchain. Now it’s time for you to join us on our journey to being a responsible technology innovator and a force for good in the world.

Location Statement

Benefits

In addition to a competitive benefits program consisting of medical and life insurance, retirement plans, and time off, eligible employees may also have access to

IBM offers a wide range of resources for eligible IBMers to thrive both inside and outside of work.


12 weeks of paid parental bonding leave. Family care options are also available to support eligible employees during COVID-19.
World-class training and educational resources on our personalized, AI-driven learning platform. IBM's learning culture supports your restless attitude to grow your skills and build the depth and scale of knowledge needed to achieve your career goals.
Well-being programs to support mental and physical health.
Financial programs that empower you to plan, save, and manage your money (including expert financial counseling, 401(k), IBM stock discount, etc.).
Select educational reimbursement opportunities.
Diverse and inclusive employee resource groups where you can network and connect with IBMers across the globe.
Giving and volunteer programs to benefit charitable organizations and local communities.
Discounts on retail products, services, and experiences.


We consider qualified applicants with criminal histories, consistent with applicable law.

IBM will not be providing visa sponsorship for this position now or in the future. Therefore, in order to be considered for this position, you must have the ability to work without a need for current or future visa sponsorship.

Being You @ IBM

IBM is committed to creating a diverse environment and is proud to be an equal opportunity employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, gender, gender identity or expression, sexual orientation, national origin, genetics, pregnancy, disability, age, veteran status, or other characteristics. IBM is also committed to compliance with all fair employment practices regarding citizenship and immigration status.

"
Gables Search Group,Data Engineer Space Force,"Colorado Springs, CO
On-site","Posted by

Michelle Daniels

Executive Recruiter, LION

Data Engineer
Locations: DC, Colorado Springs, LA
Job Description
• Serves as Data Engineer for Space Force’s C2 DaaS (Warp Core) Team
• Works in Integrated Products Teams to:
o Scope new data integrations
o Design data models
o Perform data cleansing and transformation within the Foundry platform.
• Provides guidance and support for operators and external developers in:
o Accessing and leveraging the data foundation to create new workflows
• Performs Data Engineering and Pipeline Maintenance
o Setup transfers of data feeds from source systems into location accessible to
Foundry
o Ingest new data sources using Foundry’s data ingestion UIs
o Debug issues related to delayed or missing data feeds
o Write transformations and derive new datasets
o Monitor build progress and debug problems
o Using Spark for distributed computation
• Designs and Develops Application/Workflow Configuration
o Use Foundry’s application development framework to design applications
that address operational questions
o Rapid development and iteration cycles with users
o Test and troubleshoots application issues
o Investigate data questions surrounding applications
• Works in and across teams, specifically with:
o Warp Core Program Management Team
o Space C2 SMEs
o Project Leads
Qualifications:
• BS in Computer Science / Engineering (or related career field)
• Proficiency with one or more programming languages such as:
o Python (Pyspark, Pandas – Familiar or willing to learn)
o SQL
o R or similar languages
• 5+ years’ experience working with large scale data for business solutions
• Analytical mindset and eagerness to solve technical problems with:
o Data structures
o Storage systems
o Cloud infrastructure
o Front-end frameworks, and other technical tools.
• Experience with Gotham or Foundry (preferred)
• Strong communication skills
• Must take ownership and accountability of role/responsibility within team
• Must pass Hackerrank screening
Clearance: Secret (Required) TS/SCI (Preferred)"
Accenture,Data Engineer,"Austin, TX","We are:

We are a leading partner to the world’s major cloud providers. The formation of Accenture Cloud First, with a $3 billion investment over three years, demonstrates our commitment to deliver greater value to our clients when they need it most. Our Cloud First multi-service group of more than 70,000 cloud professionals delivers a full stack of integrated cloud capabilities like data, edge, integrated infrastructure and applications, deep ecosystem skills, culture of change along with pre-configured industry solutions to shape, move, build and operate our clients’ businesses in the cloud. We combine world-class learning and talent development expertise; deep experience in cloud change management; and cloud-ready operating models with a commitment to responsible business by design — with security, data privacy, responsible use of artificial intelligence, sustainability and ethics and compliance built into the fundamental changes Accenture helps companies achieve.

You are:

As part of our Data & AI group, you will lead technology innovation for our clients through robust delivery of world-class solutions. There will never be a typical day and that’s why people love it here. The opportunities to make a difference within exciting client initiatives are unlimited in the ever-changing technology landscape.

The work:

You will be part of a highly collaborative and growing network of technology and data experts, who are taking on today’s biggest, most complex business challenges using the latest data and analytics technologies. We will nurture your talent in an inclusive culture that values diversity. You will have an opportunity to work in roles such as Cloud Data Engineer, Data Modeler or Data Architect covering all aspects of Data including Data Management, Data Governance and Data Migration. Come grow your career in Technology at Accenture!

What you need:


Minimum of 2 years of professional experience with at least one of the following: Python, Spark, Hive, Databricks, Kafka, MongoDB, Scala, Cassandra, or MapReduce


Bachelor’s Degree or equivalent work experience (12 years) or an Associate’s Degree with 6 years of work experience


Bonus points if you have:


Minimum of 2 years of experience with any of the following: NoSQL / DW CosmosDB, Synapse, Cassandra, DynomoDB, or PostgreSQL


Minimum of 2 years of professional experience in data analysis or data engineering


Experience working with AWS/GCP/Microsoft Azure


Create a value chain to help address the challenges of acquiring data, evaluating its value, distilling & analyzing.


Lead data modeling activities to capture and model data requirements, business rules, and logical and physical models


Examine data from multiple sources, and share insights which provide competitive advantage


As required by Colorado law under the Equal Pay for Equal Work Act, Accenture provides a reasonable range of compensation for roles that may be hired in Colorado. Actual compensation is influenced by a wide array of factors including but not limited to skill set, level of experience, and specific office location. For the state of Colorado only, the range of starting pay for this role is $97,632 - $161,200 and information on benefits offered is here.

COVID-19 update:

The safety and well-being of our candidates, our people and their families continues to be a top priority. Until travel restrictions change, interviews will continue to be conducted virtually.

Subject to applicable law, please be aware that Accenture requires all employees to be fully vaccinated as a condition of employment. Accenture will consider requests for accommodation to this vaccination requirement during the recruiting process.

What We Believe

We have an unwavering commitment to diversity with the aim that every one of our people has a full sense of belonging within our organization. As a business imperative, every person at Accenture has the responsibility to create and sustain an inclusive environment.

Inclusion and diversity are fundamental to our culture and core values. Our rich diversity makes us more innovative and more creative, which helps us better serve our clients and our communities. Read more here

Equal Employment Opportunity Statement

Accenture is an Equal Opportunity Employer. We believe that no one should be discriminated against because of their differences, such as age, disability, ethnicity, gender, gender identity and expression, religion or sexual orientation.

All employment decisions shall be made without regard to age, race, creed, color, religion, sex, national origin, ancestry, disability status, veteran status, sexual orientation, gender identity or expression, genetic information, marital status, citizenship status or any other basis as protected by federal, state, or local law.

Accenture is committed to providing veteran employment opportunities to our service men and women.

For details, view a copy of the Accenture Equal Opportunity and Affirmative Action Policy Statement.

Requesting An Accommodation

Accenture is committed to providing equal employment opportunities for persons with disabilities or religious observances, including reasonable accommodation when needed. If you are hired by Accenture and require accommodation to perform the essential functions of your role, you will be asked to participate in our reasonable accommodation process. Accommodations made to facilitate the recruiting process are not a guarantee of future or continued accommodations once hired.

If you would like to be considered for employment opportunities with Accenture and have accommodation needs for a disability or religious observance, please call us toll free at 1 (877) 889-9009, send us an email or speak with your recruiter.

Other Employment Statements

Applicants for employment in the US must have work authorization that does not now or in the future require sponsorship of a visa for employment authorization in the United States.

Candidates who are currently employed by a client of Accenture or an affiliated Accenture business may not be eligible for consideration.

Job candidates will not be obligated to disclose sealed or expunged records of conviction or arrest as part of the hiring process.

The Company will not discharge or in any other manner discriminate against employees or applicants because they have inquired about, discussed, or disclosed their own pay or the pay of another employee or applicant. Additionally, employees who have access to the compensation information of other employees or applicants as a part of their essential job functions cannot disclose the pay of other employees or applicants to individuals who do not otherwise have access to compensation information, unless the disclosure is (a) in response to a formal complaint or charge, (b) in furtherance of an investigation, proceeding, hearing, or action, including an investigation conducted by the employer, or (c) consistent with the Company's legal duty to furnish information.

"
Nexient,"Data Engineer (Python, Pyspark, Airflow, AWS)","Ann Arbor, MI","About Your Future Team

Nexient is on a mission: To rid the world of crappy software, one sprint at a time.

Every day, our 1000+ Agile developers, designers, and strategists empower each other to find passion in our work while we innovate the world. This culture of teamwork and curiosity is fueled by a product-minded approach to our work, crafting extraordinary custom software solutions and growing careers along the way.

Our clients range from some of America’s favorite brands in retail, healthcare, financial services to fast-emerging disruptors – You might have read about us in the NY Times. We’re also recognized as a Gartner Cool Vendor, HFS Hot Vendor, and America’s leading 100% U.S. Agile software services partner.

If you’re looking to change the trajectory of your career, make an impact on Day One, develop custom software on the best technology around – and maybe even make people’s lives easier -- then you belong here, with us.

We are looking to hire Data Engineers with experience in Python, Pyspark/Spark, Airflow, AWS and visualization/reporting experience.

Programming/languages

Python, SQL
JavaScript, Java or Scala, a plus

Data Engineering

Spark/Pyspark (or Flink)
Kafka, queue/messaging paradigms, a plus

Aws

security/networking basics, as well as S3, Kinesis, Glue, RDS, Lambda and Step Functions
Redshift, Athena a plus

Visualization/reporting Experience Would Be a Plus

Why Nexient

As a vital member of our Engineering Practice, you will be part of one of our 100+ small cross-functional teams working side by side with some of the most talented developers, UX designers, analysts, quality engineers, and product managers out there.

From Silicon Valley to Ann Arbor, Columbus, and beyond – a career with Nexient will offer you:

Remote, hybrid, or onsite opportunities: Here, you decide the work arrangement that best suits your needs and preferences.

Fulfilling and challenging projects: You will be exposed to different scenarios, multiple types of clients, and the most complex business and technology initiatives.

Access to the most cutting-edge technologies: You’re always going to be advancing your skillset by learning and working with the newest and most relevant technologies available out there.

A positive and passionate culture: Boring and divided aren’t in our vocabulary. In fact, it’s amazing to see how, despite our exponential growth, the Nexient culture and community stay as warm, diverse, and close as ever.

Plenty of opportunities for you to explore and grow: We will empower you to dream as big as you want and support you to make your career what YOU want it to be.

Creative runway: We take pride in encouraging our people to question the status quo.

Our Benefits Program Includes

Medical, dental, and vision coverage
Generous PTO, plus holidays
401(k) plan
Life insurance and short-term & long-term disability
Maternity leave
Employee Assistance Program
Flexible spending accounts
Health savings account

Nexient is an inclusive, equal opportunity employer. At Nexient, your uniqueness makes us better. Your uniqueness is why you are valued, you are heard, you add to our culture, and you belong. Diversity & Inclusion is hardcoded into our DNA and at the core of everything we do. Together, not separately, we strive to become even more diverse to help fuel our innovation and connect us closer with the clients and communities we serve."
Underdog.io,Data Engineer,"New York, United States
On-site","We're looking for a Data Engineer to join a company in the Underdog.io network.

The Underdog.io network is a curated group of some of the fastest growing startups and tech companies in the country. We actively turn away more than 50% of companies that attempt to join.

We accept companies that offer competitive salaries, benefits, and perks. They're working on interesting technical challenges and must be respectful of your time to stay active.

Our companies look for Data Engineers proficient in Python, Java, SQL, Hadoop, C++ and more. The ideal candidate is passionate about building clean pipelines and maintaining data products relied on by many. Many of our companies are looking for mid-to-senior level talent, both individual contributors and managers.

To apply to the network, we'll ask you to fill out a 60-second web form. It's absolutely free.

If accepted, you'll hear directly from founders, hiring managers, and other key decision makers starting the following Monday. Our platform will hide your profile from your current employer.

Apply today!

Building an inclusive and diverse workplace is one of Underdog.io’s core values. We warmly welcome people of all backgrounds, experiences, and perspectives.

C++,Python (Programming Language),Java,Data Products,Pipelines,Hadoop,MapReduce,SQL,Data Warehousing,Data Architecture"
"Click Therapeutics, Inc.",Data Engineer,"New York, NY","Who We Are:

Click Therapeutics, Inc. develops and commercializes software as prescription medical treatments for people with unmet medical needs. Through cognitive and neurobehavioral mechanisms, Click’s Digital Therapeutics™ enable change within individuals, and are designed to be used independently or in conjunction with biomedical treatments. The Clickometrics® adaptive data science platform continuously personalizes user experience to optimize engagement and outcomes. Following a groundbreaking clinical trial, Click’s industry-leading smoking cessation program is available nationwide through a wide variety of payers, providers, and employers. Click’s lead prescription program is entering into a multi-center, randomized, controlled, parallel-group, phase III FDA registration trial for the treatment of Major Depressive Disorder in adults. Major pipeline expansion and progression is expected.

About the Role:

We are looking for a data engineer with expertise in designing, building and maintaining data infrastructure . Your work will help people with unmet medical needs, including those who wish to quit smoking, those with major depression disorder, schizophrenia and migraine —ultimately improving lives through engineering.

Key Challenges:


Help us design and build a data pipeline using state-of-the-art technologies with data security at utmost importance.
Employ elegant solutions to help ensure Click's data products meet compliance needs (e.g., GDPR and HIPAA) in different regions of the world.


Responsibilities:


Design, build and maintain analytical data pipeline which includes both data processing and data reporting.
Onboarding data from both internal and external systems
Collaborate with Product, Engineering, Science, Data analysts and Data scientists to implement rich and re-usable datasets/metrics
Make our data infrastructure and applications scalable , reliable and secure.
Strong attitude towards automating routine tasks via coding/scripting
Research on security and privacy requirements and provide solutions


Qualifications:


2+ years of experience in building/operating highly available, distributed systems of data extraction, ingestion, and processing of large data sets
Experience writing complex, highly-optimized SQL queries.
Experience with reporting to enable explanatory and exploratory analytics.
Python development experience.


Preferred Qualifications:


Have experience with dbt, Airflow, Snowflake and AWS infrastructure
Have experience implementing APIs to share data with internal / external vendors.
Have experience implementing streams.
Understanding of privacy and security regulations (e.g., GDPR, HiTrust, HIPPA)


Benefits:

Stock options | Competitive salary with annual review | 401(k) matching | Annual performance-based cash bonus | Comprehensive medical benefits through Aetna | Flexible Spending Accounts | Life insurance and disability benefits | Open vacation policy / unlimited PTO | Generous paid parental leave | Commuter subsidies | Monthly catered lunches | Choice of Mac, Windows, or Linux equipment | Sponsored company events | Free, unlimited office snacks and beverages | Much more…

Equal Employment Opportunity

Click Therapeutics is committed to equal opportunity in the terms and conditions of employment for all employees and job applicants without regard to race, color, religion, sex, sexual orientation, age, gender identity or gender expression, national origin, disability or veteran status. Click Therapeutics also complies with all applicable national, state and local laws governing nondiscrimination in employment as well as employment eligibility verification requirements of the Immigration and Nationality Act. All applicants must have authorization to work for Click Therapeutics in the U.S. In certain circumstances it may be advantageous to Click Therapeutics to support the application(s) for temporary visa classification and/or sponsor applications for permanent residence so that a foreign national colleague can accept or remain in a work assignment in the U. S. For certain classes of temporary visas, the resulting work authorization may be specific to Click Therapeutics and the specific job and/or work site. Click Therapeutics may at its business discretion decide to or refrain from obtaining, maintaining and/or extending the temporary visa status and/or sponsoring a colleague for permanent residency and /or employment eligibility, considering factors such as availability of qualified U.S. workers and the colleague's long-term prospects for securing lawful permanent residence, among other reasons. Employment applicants requiring immigration sponsorship must disclose, when initial application for employment is made, whether or not they are legally authorized to work for Click Therapeutics in the U.S. and, if so, whether that authorization permits them to work in the job they seek. In no case should Click Therapeutics support of a colleague's temporary visa application or sponsorship of a colleague for permanent residence be construed to guarantee success of that application or amend or otherwise invalidate the ""at-will"" employment relationship between the colleague and Click Therapeutics"
Brado,Data Engineer,"United States
Remote","Posted by

Angela Wilmering

Sr. Director, Human Resources at Brado

Job Overview

We are looking for a talented software engineer with a passion for data to join us in building a scalable, easy-to-use marketing data platform. Your mission is to develop and provide the tools and data pipelines necessary to allow marketers, analysts, and data integrators across Brado’s clients to accomplish their goals which include increasing revenue, reducing churn, enhancing customer satisfaction, and running critical business operations. You will contribute to the vision for data infrastructure and analysis tools and work with your fellow engineers, data analysts, and reporting and measurement specialist to establish best practices for creating systems and datasets that marketing will use. You should possess deep technical skills, be comfortable working in and contributing to our data infrastructure, and be excited about building a strong data foundation for the company.




Responsibilities for Data Engineer

Identify data needs for our clients, our marketing team and data science team, understand specific requirements for metrics and analysis, and build efficient and scalable data pipelines to enable a data-driven marketing approach
Design, develop and maintain marketing databases, datasets, pipelines, and warehouses to enable advanced segmentation, targeting, automation, and reporting
Facilitate data integration and transformation requirements for moving data between applications; ensuring interoperability of applications with database, data warehouse, and data mart environments
Assist with the design and management of our technology stack used for data storage and processing
Develop and implement quality controls and departmental standards to ensure quality standards, organizational expectations, and regulatory requirements.
Contribute to the development and education plans on data engineering capabilities, systems, standards, and processes
Anticipate future demands of initiatives related to people, technology, budget and business within your department and design/implement solutions to meet these needs.
Communicate results and business impacts of insight initiatives to stakeholders within and outside of the company.




Qualifications for Data Engineer

3+ years of experience in a data engineering role
Expert in a programming language such as Python or Scala, and their respective standard data processing libraries
Strong experience in relational databases, SQL, data warehouses, and ETL pipelines
Experience with big data tools: Hadoop, Spark, Kafka, etc.
Experienced in integrating data from core platforms like Marketing Automation, CRM, and Analytics into a centralized warehouse. Knowledge of Marketo, Salesforce.com and Google Analytics is a big plus
Experienced in integrating and aggregating media and spend data from advertising partners and platforms into databases, warehouses, and marketing systems
Rigor in high code quality, automated testing, and other engineering best practices
Delight in building great tools that are a joy to use
BSc in Computer Science, Statistics, Informatics, Information Systems or another quantitative field




Please check us out at https://brado.net/careers/ to tell us about yourself and apply!"
Dice,ML/Data Science Engineer,"Atlanta, CA
On-site","Dice is the leading career destination for tech experts at every stage of their careers. Our client, Sumeru, is seeking the following. Apply via Dice today!

Job Title - ML Developer Data Science Engineer Location - Sunnyvale, CA or Atlanta, GA Job Type - Contract Job Description Primary Responsibilities Design machine learning systems to automate predictive models. Develop ML algorithms to analyze historical data to make predictions. Run tests, performing statistical analysis, and interpreting test results. Develop and support data pipelines to extract, transform and load data into Google Data Center data warehouse. Demonstrate ability and willingness to learn quickly and complete large volumes of work with high quality. Demonstrate outstanding collaboration, interpersonal communication and written skills with the ability to work in a team environment. Minimum qualifications Bachelor's degree in a quantitative discipline (e.g., Statistics, Computer Science, Math, Physics, Engineering), or equivalent practical experience. 3+ years of experience in analytics or similar fields. Experience with scripting languages (e.g. Python, Perl, etc.). Experience manipulating data sets in SQL and using statistical software (e.g., R, SAS, MATLAB, NumpyPandas). Experience in software engineering, including gathering data for analysis, performing data filtering and preparation, and running tests at scale. Preferred qualifications Masters degree in a quantitative discipline. Experience in machine learning (e.g., supervised learning, clustering). Experience designing data models or table schema. Experience writing and maintaining extract, transform, and load scripts which operate on a variety of structured and unstructured sources. Knowledge of statistics (e.g., probability theory, hypothesis testing, regressions), and experimentation theory. Thanks and Regards Sudarshana Technical Recruiter Desk +1 Email 2401 15th Street NW, Washington DC - 20009"
Amazon Web Services (AWS),Sr. Data & ML Engineer,"Phoenix, AZ","Description

At Amazon Web Services (AWS), we’re hiring highly technical Data and Machine Learning engineers to collaborate with our customers and partners on key engagements. Our consultants will develop and deliver proof-of-concept projects, technical workshops, and support implementation projects. These professional services engagements will focus on customer solutions such as Machine Learning, Data and Analytics, HPC and more.

In this role, you will work with our partners, customers and focus on our AWS offerings such Amazon Kinesis, AWS Glue, Amazon Redshift, Amazon EMR, Amazon Athena, Amazon SageMaker and more. You will help our customers and partners to remove the constraints that prevent them from leveraging their data to develop business insights.

AWS Professional Services engage in a wide variety of projects for customers and partners, providing collective experience from across the AWS customer base and are obsessed about customer success. Our team collaborates across the entire AWS organization to bring access to product and service teams, to get the right solution delivered and drive feature innovation based upon customer needs.

You will also have the opportunity to create white papers, writing blogs, build demos and other reusable collateral that can be used by our customers. Most importantly, you will work closely with our Solution Architects, Data Scientists and Service Engineering teams.

The ideal candidate will have extensive experience with design, development and operations that leverages deep knowledge in the use of services like Amazon Kinesis, Apache Kafka, Apache Spark, Amazon Sagemaker, Amazon EMR, NoSQL technologies and other 3rd parties.

Inclusive Team Culture

Here at AWS, we embrace our differences. We are committed to furthering our culture of inclusion. We have thirteen employee-led affinity groups, reaching 85,000 employees in over 190 chapters globally. We have innovative benefit offerings, and host annual and ongoing learning experiences, including our Conversations on Race and Ethnicity (CORE) and AmazeCon (gender diversity) conferences. Amazon’s culture of inclusion is reinforced within our 16 Leadership Principles, which remind team members to seek diverse perspectives, learn and be curious, and earn trust.

Work/Life Balance

Our team puts a high value on work-life harmony. Striking a healthy balance between your personal and professional life is crucial to your happiness and success here. We are a customer-obsessed organization—leaders start with the customer and work backwards. They work vigorously to earn and keep customer trust. As such, this is a customer facing role in a hybrid delivery model. Project engagements include remote delivery methods and onsite engagement that will include travel to customer locations as needed.

Mentorship & Career Growth

Our team is dedicated to supporting new members. We have a broad mix of experience levels and tenures, and we’re building an environment that celebrates knowledge sharing and mentorship. We care about your career growth and strive to assign projects based on what will help each team member develop into a better-rounded professional and enable them to take on more complex tasks in the future.

This is a customer facing role. You will be required to travel to client locations and deliver professional services when needed.


Basic Qualifications

Bachelor’s degree in Computer Science, Engineering, Mathematics or a related field or equivalent professional or military experience
8+ years of experience of Data platform implementation
3+ years of hands-on experience in implementation and performance tuning of Kinesis, Kafka, Spark or similar implementations
Hands on experience with building data or machine learning pipeline
Experience with one or more relevant tools (Flink, Spark, Sqoop, Flume, Kafka, Amazon Kinesis)
Experience developing software code in one or more programming languages (Java, JavaScript, Python, etc)
Current experience with hands-on implementation

Preferred Qualifications

Masters or PhD in Computer Science, Physics, Engineering or Math.
Familiar with Machine learning concepts
Hands on experience working on large-scale data science/data analytics projects
Hands-on experience with technologies such as AWS, Hadoop, Spark, Spark SQL, MLib or Storm/Samza.
Experience Implementing AWS services in a variety of distributed computing, enterprise environments.
Experience with at least one of the modern distributed Machine Learning and Deep Learning frameworks such as TensorFlow, PyTorch, MxNet Caffe, and Keras.
Experience building large-scale machine-learning infrastructure that have been successfully delivered to customers.
Experience defining system architectures and exploring technical feasibility trade-offs.
3+ years experiences developing cloud software services and an understanding of design for scalability, performance and reliability.
Ability to prototype and evaluate applications and interaction methodologies.
Experience with AWS technology stack.
Written and verbal technical communication skills with an ability to present complex technical information in a clear and concise manner to a variety of audiences.

Amazon is committed to a diverse and inclusive workplace. Amazon is an equal opportunity employer and does not discriminate on the basis of race, national origin, gender, gender identity, sexual orientation, protected veteran status, disability, age, or other legally protected status. For individuals with disabilities who would like to request an accommodation, please visit https://www.amazon.jobs/en/disability/us.

For employees based in Colorado, this position starts at $122,300 per year. A sign-on bonus and restricted stock units may be provided as part of the compensation package, in addition to a range of medical, financial, and/or other benefits, dependent on the position offered.


Company - Amazon Web Services, Inc.

Job ID: A1414163"
The Walt Disney Company,"Senior Data Engineer, Content","Santa Monica, CA","Our Data and Analytics team for Disney Streaming Services (DSS), a segment under the Disney Media & Entertainment Distribution (DMED) is looking for a Senior Data Engineer. Data is essential for all our decision-making needs whether it’s related to product design, measuring advertising effectiveness, helping users discover new content or building new businesses in emerging markets. This data is deeply valuable and gives us insights into how we can continue improving our service for our users, advertisers and our content partners. Our Content & Engagement Data Engineering team is seeking a highly hardworking Data Engineer with a strong technical background and passionate about diving deeper into Big Data to develop state of the art Data Solutions.

Responsibilities :


Contribute to the design and growth of our Data Products and Data Warehouses around Content Performance and Content Engagement data.
Design and develop scalable data warehousing solutions, building ETL pipelines in Big Data environments (cloud, on-prem, hybrid)
Our tech stack includes Hadoop, AWS, Snowflake, Spark and Airflow
Help architect data solutions/frameworks and define data models for the underlying data warehouse and data marts
Collaborate with Data Product Managers, Data Architects and Data Engineers to design, implement, and deliver successful data solutions
Maintain detailed documentation of your work and changes to support data quality and data governance
Ensure high operational efficiency and quality of your solutions to meet SLAs and support commitment to our customers (Data Science, Data Analytics teams)
Be an active participant and advocate of agile/scrum practice to ensure health and process improvements for your team


Basic Qualifications :


5+ years of data engineering experience developing large data pipelines
Strong SQL skills and ability to create queries to extract data and build performant datasets
Hands-on experience with distributed systems such as Spark, Hadoop (HDFS, Hive, Presto, PySpark) to query and process data


Preferred Qualifications:


Experience with at least one major MPP or cloud database technology (Snowflake, Redshift, Big Query)
Nice to have experience with Cloud technologies like AWS (S3, EMR, EC2)
Solid experience with data integration toolsets (i.e Airflow) and writing and maintaining Data Pipelines
Familiarity with Data Modeling techniques and Data Warehousing standard methodologies and practices
Good Scripting skills, including Bash scripting and Python
Familiar with Scrum and Agile methodologies
You are a problem solver with strong attention to detail and excellent analytical and communication skills


Required Education :


Bachelor’s or Master’s Degree in Computer Science, Information Systems or related field


919488BR"
FDM Group,Junior Data Engineer,"Jacksonville, FL
Hybrid","FDM is currently seeking numerous ambitious and driven candidates with the aptitude for tech to work as FDM consultants. We help people like you make their first step into business every day. Are you ready to make yours?




About This Role

As a Junior Data Engineer with FDM, you will undergo industry specific training and continue your career journey as an FDM consultant by working for one of our globally recognized clients. Some common positions you could be working in include Data Engineer, Data Architect, ETL Engineer and more.

As a Junior Data Engineer, you will design, develop and test a large-scale, custom distributed data pipeline using the Java, Python and the latest Big data technologies. You will be a technology advocate and share expertise with other team members. You will be a key partner to the business and the rest of the team throughout the delivery cycle and will work through challenging and interesting problems.

No previous qualifications or STEM Field experience is required.

About the FDM Career Development Program

The program begins with training in one of our career streams in business and technology that are in high demand in the job market. We will provide you with the industry-specific technical and professional skills you need in your chosen stream, after which you will start working as an FDM Consultant with our globally recognized clients.




Upon completion of the program, many see their journey with FDM culminating in an accelerated career progression, better placing them to apply for senior roles within the industry.

Minimum Qualifications

Bachelor’s Degree
A strong aptitude for technology and the interest and drive to expand your technical skill set
Able to commit to work for FDM for a minimum of two years working as an FDM consultant following the training period.
Eligible to work in the US.

Benefits

A chance to work for some of the most prestigious companies in the world.
Opportunity to earn professional certifications related to your career path.
Continuous career support and upskilling from FDM.
Comprehensive and competitive benefits package.
Access to a network of mentors and like-minded individuals.
Relocation support (if necessary)
Industry recognized paid training.




FDM Group is an Equal Opportunity Employer and all qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, national origin, age, disability, veteran status or any other status protected by federal, provincial or local laws."
Park Place Technologies,Data Engineer II,"United States
Remote","ALL APPLICANTS MUST CURRENTLY RESIDE IN THE U.S.

No Sponsorship is available for this position.




Who We Are:

As the global leader in third party maintenance, our 2500 Park Place Associates provide support to 21,000+ customers in more than 154+ countries. We are proud to service 90% of Fortune 500 companies and 40% of Forbes 100 clients.




Our company’s strength and success are a credit to our Associates, and Park Place Life is how we communicate and deliver our culture internally. We have been awarded as a NorthCoast 99 “Best Workplace” winner for 10 consecutive years in recognition of our employee commitment. Park Place Life is about collaboration, responsiveness, diversity, and integrity, and represents everything that makes our company great and our culture unique.




Top Rated Benefits We Offer:

We cover 100% of your Healthcare benefits!
Profit Sharing!
401K matching contributions and earnings are always 100% vested.
Plus much more!!!




Position Overview:

The Data Engineer II will help create reporting and analysis tools using SSIS, Azure Data Factory, T-SQL, SSAS, Power BI and other business intelligence applications to support our reporting platform. This position will be supporting various stages of the end-to-end data integration and report development lifecycle, with specific responsibilities related to the requirements and architecture of our reference system.




What you’ll be doing:

Implement T-SQL code (procedures and views) and tabular models in line with established plans and architectural standards for the department.
Implement, enhance, triage existing stored procedures and SSIS packages employed in the overnight or real-time integration with source systems.
Implement Azure Data Factory pipelines to support data sourcing, transformations and load into Azure Synapse and other data repositories
Implement, enhance SSAS multi-dimensional and tabular cubes.
Act as a liaison with architects, business analysts, developers and testing teams for content input and document reviews to ensure that report functionality is well documented.
Interpret requirements provided by business analysts and occasionally directly from end users
Pro-actively understand changes to project scope and make relevant changes to report design and stored procedures.
Understand relational databases and data analysis, data models, data extracts, and reporting in context.
Assist Quality Assurance team with validating reports.
Escalating issues in a timely manner and suggesting improvements.
Providing peer review support to work produced by others, confirming use of relevant coding standards; approving work produced by others.
Demonstrate understanding of the Software Development Life Cycle.




What we’re looking for:

Advanced experience with Microsoft SQL Server and T-SQL code development.
Experience in Power BI or comparable reporting tools.
Experience in ETL tools (SSIS or Azure Data Factory)
Solid understanding of database development best practices.
Knowledge of, or experience with, predictive analytics.




A Bonus If You Have:

Experience with SQL Server 2016 Enterprise, Azure Synapse, Python/R, Databricks, or C#
Web development experience is a plus.




Education:

Bachelor’s degree




Travel:

<10%

If you are a qualified individual with a disability or a disabled veteran, you may request a reasonable accommodation if you are unable or limited in your ability to access job openings or apply for a job on this site as a result of your disability. You can request reasonable accommodations by calling 1-877-778-8707.




Park Place Technologies is an Equal Opportunity Employer M/F/D/V.




Park Place Technologies has an in-house recruiting team that focuses exclusively on the hiring needs of our company. We are not currently accepting additional third-party agreements or unsolicited resumes. If you would like to be considered as a preferred partner with Park Place Technologies, please submit your detailed information to careers@parkplacetech.com. Any CVs submitted directly to hiring managers will be considered unsolicited and become the property of Park Place Technologies.

Seniority Level
Associate
Industry
Information Technology and Services
Employment Type
Full-time
Job Functions
Project Management"
Vodori,Data Analytics Engineer,"Chicago, IL
Remote","Posted by

Sue Kiefer, SPHR

VP, Head of People and Culture

Send InMail

Data Analytics Engineer




Vodori is a fast-growing, venture-backed SaaS technology company building robust, cloud-based software for life science companies. Every day we are on a mission to transform lives by empowering life science companies to bring their vital drugs, vaccines, medical devices, and diagnostic solutions to market faster. Following our recent Series A capital raise, we are scaling rapidly and looking for bold, innovative people who like to move, think and act swiftly while driving towards team and company goals.




Our Pepper Cloud Product Suite streamlines and automates how life science companies get critical marketing, medical, and scientific content to key audiences like patients, healthcare providers, and key opinion leaders. Our software is used by thousands of life science professionals in more than 50 countries worldwide.




Vodori is in a rapid growth phase and in search of a Data Analytics Engineer with experience to help build the future vision of our software suite. The Data Team’s mission is to create a data foundation that allows us to develop intelligent products that drive impact through informed decision making and powerful insights. Our data and analytics engineers work collaboratively with software engineering, product management, and our customers to drive innovative ways of using data to create differentiators in our product suite. You are a self-starter, energized by learning new technologies and techniques, and focused on results. You will help lead a growing data team which is core to the success of Vodori’s vision. In this role you will see a direct connection between your work, Vodori’s growth, and user satisfaction.




Responsibilities:

Conceptualize and contribute to overall platform data architecture; while helping to prepare for the growth of our data science initiatives
Collaborate with engineers, product managers, and data scientists customers to understand data needs, representing key data insights visually in a meaningful way
Partner with business stakeholders to answer questions with data, and identify data opportunities that drive impact
Build dashboards and reports, and carry out exploratory analysis
Build clean, transformed, and well-tested data sets
Optimize data pipelines, models, and dashboards to ensure highly accurate and reliable business reporting;
Train business users on how to use data platform and visualization tools
Apply software engineering best practices to analytics code (ex version control, testing, continuous integration)
Stay up to date with the latest technologies and trends in data engineering and analytics to improve our existing technology stack




Qualifications:

5+ years of experience in data warehousing, familiarity with Snowflake
2+ years experience with ETL processes and data transformation, including scheduling jobs (DBT, Python, cron). Bonus using templating languages (such as Jinja), and manipulating multiple data formats (XML, JSON)
5+ years experience writing SQL
5+ years experience with data modeling
Experience with market-leading data visualization software (Sisense, Looker, etc) in B2B environment preferred
Excellent communication skills; ability to communicate effectively with key stakeholders and engineers
Proactive, self-motivated, and can work independently with strong organizational and time management skills




What We Offer

In addition to joining in our mission to help life science companies bring their products to market faster, we offer:

Competitive salary
Medical, dental, vision benefits
Retirement program and company match
Stock options
Remote work option
10 weeks paid family leave
Unlimited vacation time (Yes, really!)
Employee recognition programs
Employee assistance program
Transit benefits
Professional growth and training opportunities
An environment that rewards creativity, calculated risk taking, and continuous learning




Our Values

We look for people who care about our mission and align with our values:

Customer first, always
Be bold, pursue the unexpected
Support the team (it’s not about you)
Aspire to excellence
Move, think, act swiftly
Keep it simple




And now for the legal disclosures, because, well there are always legal disclosures:




We are an Equal Opportunity Employer. We do not discriminate on the basis of age, ancestry, color, gender, gender expression, gender identity, genetic information, marital status, national origin or citizenship (including language use restrictions), denial of family and medical care leave, disability (mental and physical), including HIV and AIDS, medical condition (including cancer and genetic characteristics), race, religious creed (including religious dress and grooming practices), sex, (including pregnancy, child birth, breastfeeding, and medical conditions related to pregnancy, child birth or breastfeeding), sexual orientation, military or veteran status, or other status protected by federal, state and/or local laws.




To all recruitment agencies: Unless you have been requested to work on this position, or any other position with Vodori, please do not forward any resumes to Vodori employees. Vodori is not responsible for any fees related to unsolicited resumes."
Oracle,Data Scientist/Engineer,"Austin, TX
On-site","Job Description

Designs, develops and programs methods, processes, and systems to consolidate and analyze unstructured, diverse “big data” sources to generate actionable insights and solutions for client services and product enhancement.

Interacts with product and service teams to identify questions and issues for data analysis and experiments. Develops and codes software programs, algorithms and automated processes to cleanse, integrate and evaluate large datasets from multiple disparate sources. Identifies meaningful insights from large data and metadata sources; interprets and communicates insights and findings from analysis and experiments to product, service, and business managers.

Job duties are varied and complex utilizing independent judgment. May have project lead role. 5 years relevant work experience. BS/BA preferred.

If you are a Colorado resident, Please Contact us or Email us at oracle-salary-inquiries_us@oracle.com to receive compensation and benefits information for this role. Please include this Job ID: 148438 in the subject line of the email.

Responsibilities

Oracle Utilities Analytics Insights (OUAI) Data Science & Analytics | Data Analyst

At Oracle Utilities, we’re applying cutting-edge computer science to one of humanity’s greatest challenges: Energy. We are looking to hire a Data Analyst to take OUAI clients and R&D to new heights. In this role you will work with our talented and growing team of Data Scientists and Data Analysts to plan, execute, and deliver actionable analytic insights to electric, water, and gas utilities through the OUAI (Oracle Utilities Analytics Insights) product and its 100TB+ of cloud-hosted data. Our analytics team works actively across the product/R&D and client management teams to define analyses based on customer needs, build quick solutions and then works on productizing those solutions in a repeatable and generalizable way as part of product. We use a combination of technology and platforms such as MySQL, R, Python, and our proprietary tools, to execute these analyses and drive value.

We use smart meter data and cutting-edge cloud platforms to learn how a number of devices deployed by utilities across the grid operate, with more projects like this on our roadmap. We are looking for someone passionate about the energy/water space who wants to use data science to drive meaningful societal impact. This function cuts across data science and model implementation in production systems.

Position reports to Lead Data Scientist – OUAI.

Responsibilities

Efficiently execute well-defined, discrete analytic tasks based on the needs of client, as communicated by the client management team;
Work with client management team and our customers to define and take ownership of scope, intermediate deliverables, and timelines around larger-scale analytic deliverables;
Work with application management team, and where necessary, other members of the analytics team to efficiently execute larger-scale analytic deliverables and operationalize the results;
Identify recurring problems and bottlenecks that might be improved through upgrades to our software product, new technologies in the analytics team infrastructure, or further research into statistical methodology.

Preferred Qualifications

MS in Computer Science, Mathematics, Statistics, Physics, Economics or other STEM field;
3-7 years of professional work experience in a data analytics role involving customer-facing deliverables and internal team coordination;

Prior experience and expertise in smart metering data is a big plus, but strong candidates without this experience will also be considered.

Prior experience framing and conducting analyses in a relational database environment (e.g. Oracle DB, MySQL, PostgreSQL, MSSQL) and through spreadsheet-based tools;
Prior experience with programming languages (e.g. Python) and statistical programming languages (e.g. R, SAS, SPS, etc.) strongly preferred;
A passion for and curiosity about new big data analytics technologies and methods;
Strong written & oral communication skills;
Prior experience working in complex data environments within the energy and utility industry preferred. A successful candidate will demonstrate the following qualities:
Ability to write SQL queries and capability to do exploratory analysis on structured data.
Ability to write well structured, testable Python or PySpark code to implement use cases using Machine Learning techniques in a production grade environment.
Prior experience working as a data scientist in role that at a minimum requires creation of ""proof of concept"" data science models.
Ability and willingness to be able to use OUAI’s proprietary tool to recreate or build rule-based solutions to analytics problems, mostly classification
Breadth and depth across machine learning techniques: specifically Deep Learning (using TF/Keras or other frameworks), as well as other ML techniques to re-create ML equivalents of legacy rule-based classification algorithms.
Model evaluation and validation expertise, should be demonstrated as a consequence of having to do this in current work developing ML models.
Ability to work closely with other stakeholders in the client delivery organization for OUAI (Customer success and Implementation Engineering)
Be a team player and be willing to delve into the details of the product to make sure clients are seeing value.

About Us

Innovation starts with inclusion at Oracle. We are committed to creating a workplace where all kinds of people can be themselves and do their best work. It’s when everyone’s voice is heard and valued, that we are inspired to go beyond what’s been done before. That’s why we need people with diverse backgrounds, beliefs, and abilities to help us create the future, and are proud to be an affirmative-action equal opportunity employer.

Oracle is an Equal Employment Opportunity Employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, sexual orientation, gender identity, disability and protected veterans status, age, or any other characteristic protected by law. Oracle will consider for employment qualified applicants with arrest and conviction records pursuant to applicable law."
CVS Health,Lead Data Engineer,"New York, NY
On-site","Job Description

Description/Fundamental Components:

Designs and develops complex and large-scale data structures and pipelines to organize, collect and standardize data to generate insights and addresses reporting needs

Writes complex ETL (Extract / Transform / Load) processes, designs database systems and develops tools for real-time and offline analytic processing

Develop frameworks, standards & reference material for architecture and associated products

Designs data marts and data models to support Data Science and other internal customers. Behaves as mentor to junior team members to provide technical advice

Applies knowledge of Aetna systems and products to consult and advise on additional efforts across multiple domains spanning broader enterprise

Collaborates with data science team to transform data and integrate algorithms and models into highly available, production systems

Uses in-depth knowledge on Hadoop architecture, HDFS commands and experience designing & optimizing queries to build scalable, modular, and efficient data pipelines

Uses advanced programming skills in Python, Java or any of the major languages to build robust data pipelines and dynamic systems

Integrates data from a variety of sources, assuring that they adhere to data quality and accessibility standards

Experiments with available tools and advice on new tools in order to determine optimal solution given the requirements dictated by the model/use case

Required Qualifications

7 or more years of progressively complex related experience

COVID Requirements

CVS Health requires its Colleagues to be fully vaccinated against COVID-19 (including any booster shots if required), where allowable under the law, unless they are approved for a reasonable accommodation based on disability, medical condition, or religious belief that prevents them from being vaccinated.

If you are vaccinated, you are required to have received at least one COVID-19 shot prior to your first day of employment and to provide proof of your vaccination status within the first 10 days of your employment. For the two COVID-19 shot regimen, you will be required to provide proof of your second COVID-19 shot within the first 45 days of your employment. In some states and roles, you may be required to provide proof of full vaccination before you can begin to actively work. Failure to provide timely proof of your COVID-19 vaccination status will result in the termination of your employment with CVS Health.

If you are unable to be fully vaccinated due to disability, medical condition, or religious belief, you will be required to apply for a reasonable accommodation within the first 10 days of your employment in order to remain employed with CVS Health. As a part of this process, you will be required to provide information or documentation about the reason you cannot be vaccinated. In some states and roles, you may be required to have an approved reasonable accommodation before you can begin to actively work. If your request for an accommodation is not approved, then your employment may be terminated.

Preferred Qualifications

Has in-depth knowledge of large-scale search applications and building high volume data pipelines

Ability to leverage multiple tools and programming languages to analyze and manipulate large data sets from disparate data sources

Ability to understand and build complex systems and solve challenging analytical problems

Advanced knowledge in Java, Python, Hive, Cassandra, Pig, MySQL or NoSQL or similar

Advanced knowledge in Hadoop architecture, HDFS commands and experience designing & optimizing queries against data in the HDFS environment

Experience building and implementing data transformation and processing solutions

Experience with bash shell scripts, UNIX utilities & UNIX Commands

Education

Bachelor's degree or equivalent work experience in Computer Science, Engineering, Machine Learning, or related discipline.

Business Overview

Master’s degree or PhD. Preferred

Bring your heart to CVS Health

Every one of us at CVS Health shares a single, clear purpose: Bringing our heart to every moment of your health. This purpose guides our commitment to deliver enhanced human-centric health care for a rapidly changing world. Anchored in our brand — with heart at its center — our purpose sends a personal message that how we deliver our services is just as important as what we deliver.

Our Heart At Work Behaviors™ support this purpose. We want everyone who works at CVS Health to feel empowered by the role they play in transforming our culture and accelerating our ability to innovate and deliver solutions to make health care more personal, convenient and affordable.

We strive to promote and sustain a culture of diversity, inclusion and belonging every day.

CVS Health is an affirmative action employer, and is an equal opportunity employer, as are the physician-owned businesses for which CVS Health provides management services. We do not discriminate in recruiting, hiring, promotion, or any other personnel action based on race, ethnicity, color, national origin, sex/gender, sexual orientation, gender identity or expression, religion, age, disability, protected veteran status, or any other characteristic protected by applicable federal, state, or local law."
Dice,Sr AWS Data Engineer,"New York, NY
On-site","Dice is the leading career destination for tech experts at every stage of their careers. Our client, CloudPro Partners, is seeking the following. Apply via Dice today!

Title Sr AWS Data Engineer Location 100 Remote Duration 24+ Months Job Description - Must Have Production experience on enterprise grade operational Data lake formationimplementation and maintenance Hands-on experience developing systems that leverage multiple AWS services including CloudFormation, Lambda, API Gateway, S3, DynamoDBany NoSQL, Relational Database Service (RDS), Glue Catalog ETLs, Athena and Quick Sight for data management and analysis Strong expertise ingesting historic and near real time structuredsemi structuredunstructured data into data lake Ingestion, Transformation, Cataloging, In-Place Querying, Storage, and Security using AWS tools and best practices Experience in migrating legacy systems into an Enterprise Data Lake is a Plus Experience in Monitoring and Optimizing the Data lake environments Proficient in Python, SQL queries using AWS Athena and mapping of various relational DB queries to Athena ExperienceExposure to security best practices with Data lake Follow and enforce strict standards for code quality, automated testing, infrastructure-as-code and code maintainability DevOps background preferred Python andor Node.js"
Dice,Data Engineer (Development),"Plano, TX
On-site","Dice is the leading career destination for tech experts at every stage of their careers. Our client, NR Consulting LLC, is seeking the following. Apply via Dice today!

Job Title Data Engineer (Development) Location Plano, TX Duration 2 yrs(it can change based on client requirements) This position involves the developing data pipelines with ongoing support and maintenance of batchrealtime data pipelines. A successful candidate will be comfortable troubleshooting and resolving common data pipeline failures. Knowledge of the Hadoop ecosystem, Spark, and Python is required. Skills Required Spark (PySpark preferred) Kafka Microsoft Graph API Elastic search Hadoop (HDFS Hive) Python SQL RDBMS Openshift Kebernetes ServiceNow Tableau"
Dice,Hiring Azure Data Engineer @ Remote,"United States
Remote","Dice is the leading career destination for tech experts at every stage of their careers. Our client, Infinite Computer Solutions (ICS), is seeking the following. Apply via Dice today!

Hiring Azure Data Engineer for remote work. Interested Candidates apply with your updated resume. Skill Azure Data Engineer Location Remote W2 Position Wersquore looking for an Azure Data Engineer with the experience to implement and manage our enterprise data platform for healthcare research. This role includes infrastructure setup configuration, data ingestion processing, data governance processes, and daily operational provisioning. The successful candidate will be self-directed, knows how to prioritize tasks, communicates challenges, and can coordinate across teams to insure successful delivery. It is preferred that the individual have a background working with healthcare data. Primary Responsibilities Architect design big data platform utilizing Azure services that integrates with client tools such as SAS, SPSS, Knime, Tableau, etc. Setup configure Azure Big Data platform services including Synapse DW, ADLS, Data Factory, HDInsightsDatabricks, Data Catalog, and other related services. Design and implement ETLELT processes using Azure services (e.g. pipelines, data factory). Design and implement data governance processes (e.g. security, data quality) Manage daily operations of the big data platform (e.g. access control, trouble shooting, etc)."
Accenture,GCP Data Engineer,"Los Angeles, CA","We are:

We are a leading partner to the world’s major cloud providers. The formation of Accenture Cloud First, with a $3 billion investment over three years, demonstrates our commitment to deliver greater value to our clients when they need it most. Our Cloud First multi-service group of more than 70,000 cloud professionals delivers a full stack of integrated cloud capabilities like data, edge, integrated infrastructure and applications, deep ecosystem skills, culture of change along with pre-configured industry solutions to shape, move, build and operate our clients’ businesses in the cloud. We combine world-class learning and talent development expertise; deep experience in cloud change management; and cloud-ready operating models with a commitment to responsible business by design — with security, data privacy, responsible use of artificial intelligence, sustainability and ethics and compliance built into the fundamental changes Accenture helps companies achieve.

You are:

As part of our Data & AI group, you will lead technology innovation for our clients through robust delivery of world-class solutions. There will never be a typical day and that’s why people love it here. The opportunities to make a difference within exciting client initiatives are unlimited in the ever-changing technology landscape.

The work:

You will be part of a highly collaborative and growing network of technology and data experts, who are taking on today’s biggest, most complex business challenges using the latest data and analytics technologies. We will nurture your talent in an inclusive culture that values diversity. You will have an opportunity to work in roles such as Cloud Data Engineer, Data Modeler or Data Architect covering all aspects of Data including Data Management, Data Governance and Data Migration. Come grow your career in Technology at Accenture!

What you need:


Minimum of 3 years direct experience in Enterprise Data Warehouse technologies


Minimum of 3 years in a customer facing role working with enterprise clients


Minimum of 3 years hands on GCP Cloud data implementation projects experience (Dataflow, Cloud Composer, Big Query, Cloud Storage etc.)


Experience with architecting, implementing and/or maintaining technical solutions


Certifications: Google Professional Data Engineer


Bachelor's degree or equivalent (minimum 12 years) work experience. (If associate degree, must have minimum 6 years work experience)


Bonus points if you have:


Google Cloud Platform (GCP) certification (or comparable experience)


Experience in design, architecture and implementation of Data warehouses, data pipelines and 􀇄ows.


Experience with developing so􀇅ware code in one or more languages such as Java, Python and SQL.


Experience designing and deploying large scale distributed data processing systems with few technologies such as Oracle, MS SQL Server, MySQL, PostgreSQL, MongoDB, Cassandra, Redis, Hadoop, Spark, HBase, Ve􀇈ica, Netezza, Teradata, Tableau, Qlik or MicroStrategy.


Demonstrated excellent communication, presentation, and problem solving skillsData migration and data processing experience on the Google Cloud stack, specifically:


BigQuery


Cloud DataFlow


Cloud DataProc


Cloud Storage


Cloud DataPrep


As required by the Colorado Equal Pay Transparency Act, Accenture provides a reasonable range of compensation for roles that may be hired in Colorado. Actual compensation is influenced by a wide array of factors including but not limited to skill set, level of experience, and specific office location. For the state of Colorado only, the range of starting pay for this role is $90,288- $148,900 and information on benefits offered is here.

COVID-19 update:

The safety and well-being of our candidates, our people and their families continues to be a top priority. Until travel restrictions change, interviews will continue to be conducted virtually.

Subject to applicable law, please be aware that Accenture requires all employees to be fully vaccinated as a condition of employment. Accenture will consider requests for accommodation to this vaccination requirement during the recruiting process.

What We Believe

We have an unwavering commitment to diversity with the aim that every one of our people has a full sense of belonging within our organization. As a business imperative, every person at Accenture has the responsibility to create and sustain an inclusive environment.

Inclusion and diversity are fundamental to our culture and core values. Our rich diversity makes us more innovative and more creative, which helps us better serve our clients and our communities. Read more here

Equal Employment Opportunity Statement

Accenture is an Equal Opportunity Employer. We believe that no one should be discriminated against because of their differences, such as age, disability, ethnicity, gender, gender identity and expression, religion or sexual orientation.

All employment decisions shall be made without regard to age, race, creed, color, religion, sex, national origin, ancestry, disability status, veteran status, sexual orientation, gender identity or expression, genetic information, marital status, citizenship status or any other basis as protected by federal, state, or local law.

Accenture is committed to providing veteran employment opportunities to our service men and women.

For details, view a copy of the Accenture Equal Opportunity and Affirmative Action Policy Statement.

Requesting An Accommodation

Accenture is committed to providing equal employment opportunities for persons with disabilities or religious observances, including reasonable accommodation when needed. If you are hired by Accenture and require accommodation to perform the essential functions of your role, you will be asked to participate in our reasonable accommodation process. Accommodations made to facilitate the recruiting process are not a guarantee of future or continued accommodations once hired.

If you would like to be considered for employment opportunities with Accenture and have accommodation needs for a disability or religious observance, please call us toll free at 1 (877) 889-9009, send us an email or speak with your recruiter.

Other Employment Statements

Applicants for employment in the US must have work authorization that does not now or in the future require sponsorship of a visa for employment authorization in the United States.

Candidates who are currently employed by a client of Accenture or an affiliated Accenture business may not be eligible for consideration.

Job candidates will not be obligated to disclose sealed or expunged records of conviction or arrest as part of the hiring process.

The Company will not discharge or in any other manner discriminate against employees or applicants because they have inquired about, discussed, or disclosed their own pay or the pay of another employee or applicant. Additionally, employees who have access to the compensation information of other employees or applicants as a part of their essential job functions cannot disclose the pay of other employees or applicants to individuals who do not otherwise have access to compensation information, unless the disclosure is (a) in response to a formal complaint or charge, (b) in furtherance of an investigation, proceeding, hearing, or action, including an investigation conducted by the employer, or (c) consistent with the Company's legal duty to furnish information.

"
Homesite Insurance,Data Engineer II - Remote,"Phoenix, AZ
Remote","Posted by

Tom Brenneman

Talent Acquisition Manager: Enterprise IT & Product COE

Homesite Insurance was founded in 1997 and was one of the first companies to enable customers to purchase home insurance directly online, during a single visit. Since then, we've continued to innovate rapidly to meet the needs of our customers and their changing expectations.

One thing that's stayed the same since our founding: our commitment to our customers, partners and employees.

Join us on our journey as we continue to grow into a powerful contender in the field of insurance.

Homesite Insurance was founded in 1997 and was one of the first companies to enable customers to purchase home insurance directly online, during a single visit. Since then, we've continued to innovate rapidly to meet the needs of our customers and their changing expectations.

One thing that's stayed the same since our founding: our commitment to our customers, partners and employees.

Join us on our journey as we continue to grow into a powerful contender in the field of insurance.

Homesite is an innovative national property and casualty insurance company with a fast-paced environment. We're proud to be part of the American Family Mutual Insurance Company and the home for the Enterprise’s Property Center of Excellence. We support multiple products sold through a number of operating companies and distribution methods, including offerings in partnership with other large national brands. We seek a person possessing analytical rigor and strong technical skills with a desire to build a successful career in Data Engineering. The primary role will be to develop cloud-based data solutions intended to support the data needs of statistical predictive modeling projects that grow profits through the development of state of the art pricing and product plans.

Responsibilities

· Develop, maintain, and enhance data tooling to enable data integration and availability.

· Collaborate with the Modeling and Data Science teams to provide non-production dev/ops and robust monitoring of model performance.

· Assist in the implementation of a cloud data strategy including tooling, governance, and documentation.

· Develop, maintain, and enhance databases throughout the organization to support analytics projects.

· Understand corporate data structure to be able to draw data from transactional data tables existing in the company.

· Support the acquisition of external data sets, interpreting data layouts, structures, fields and values to incorporate new data into the core analytics data base.

· Analyze databases for performance optimization, including data normalization, indexing, and memory management.

· Clearly communicate complex findings to colleagues and external customers.

· Monitor the results of statistical models through the use of dashboards and ad hoc analyses.

Minimum Requirements

· Bachelor’s degree or higher in Computer Science, Engineering, Information Systems, Statistics, Machine Learning, Applied Mathematics, Physics or other quantitative discipline.

· Three or more years of programming or data engineering experience.

· Intermediate software development skills utilizing an Object-Oriented Approach with one or more of the following languages in: Python, Java/Kotlin, Go

· Familiarity with implementation of data engineering tooling such as Apache Spark, Apache Airflow, and/or cloud provider-specific tooling.

· Familiarity with pipeline and application deployment to various compute approaches including serverless, container orchestration, and cloud-based virtual machines.

· Intermediate SQL knowledge, including advanced knowledge of one of more of the following: Microsoft SQL Server, PostgreSQL, Google BigQuery, Amazon Redshift or similar

· Familiarity with Linux/Unix shell.

· Experience with at least one of the major cloud platforms: Google Cloud Platform, Amazon Web Services, and/or Microsoft Azure

· Moderate level of comfort working with Git.

· Motivated individual with strong analytic, problem solving, and troubleshooting skills

Preferred Qualifications

· Master’s degree or higher in Computer Science, Engineering, Statistics, Machine Learning, Applied Mathematics, Physics or other quantitative discipline.

· Familiarity with statical computing approaches (R, Pandas/NumPy, Mathematica, and such)

· Experience with administrative responsibilities on Windows Server environments.

· Experience with NoSQL approaches such as DynamoDB, MongoDB, and/or Google BigTable

· Familiarly with data lake approaches such as Amazon Athena/S3, Delta Lake, and/or Google Cloud Dataflow/Cloud Storage.

· Knowledge of data visualization approaches such as Tableau, D3.js, Looker, and/or Power BI.

· Experience with MLOps tooling such as DataRobot, KubeFlow, AWS Sagemaker, MlFlow, H20, GCP AI Platform, and such.

· Knowledge of statistical software languages/packages such as R, Python, DataRobot, Tableau, Mathematica, and Spark."
"Phoenix Staff, Inc.",Senior Data Engineer,"Austin, TX","Posted by

Kyle Marker

Accountant at Phoenix Staff, Inc.

Send InMail

Title: Senior Data Engineer
Location: Austin, TX




Our client has embarked on a major transformation journey. They are transforming themselves into a true engineering organization where they put customer experience and end user support first. As part of this strategy, they are consolidating several Operational functions into a single area focused on providing world-class operational support to our End users. Their Engineering, Applications and Analytics team is looking for curious, creative, and technology experts to join a growing team. They focus on delivering and supporting integrated platforms that enables finance applications, which are used by business partners to service clients globally. They also develop analytics and reporting by using revenue data.



The successful candidate is expected to be expert in platform deployed on Linux systems. The selected candidate will be fully responsible for both service delivery and operational excellence of services built using Hadoop technologies.




Your role:





Data integration, processing, and governance
Data storage and computation frameworks, performance optimizations
Analytics and visualizations
Data Management Platforms
Implement scalable architectural models for data processing and storage
Build functionality for data ingestion from multiple heterogeneous sources in batch and real-time mode
Build functionality for data analytics, search, and aggregation




What you've got:





Overall 4+ years of IT experience with 2+ years in Data related technologies
Hands-on experience with the Hadoop stack - HDFS, sqoop, kafka, NiFi, Spark, Hive, Oozie, Airflow, and other components required in building end to end data pipeline
Excellent experience in at least of the programming language Java, Scala, Python. Java preferable
Hands-on working knowledge of NoSQL and MPP data platforms like Hbase, MongoDb, Cassandra
Well-versed and working knowledge with data platform related services
Strong in debugging, monitoring end to end system




Preferred Qualifications:





Proactive in communication and good in analytical abilities
Strong in customer engagement and other stakeholders
Provide right and faster solution for any issues
Maintain best practices on managing systems and services across all environments




To find more great tech-centric jobs, please visit www.phoenixstaff.com.









Phoenix, AZ Austin, TX Las Vegas, NV www.phoenixstaff.com"
Chloris Geospatial,Data Engineer,"Boston, MA
Hybrid","Posted by

Marco Albani

CEO at Chloris Geospatial - We are hiring

We are looking for an exceptional individual to support development of Chloris Geospatial’s innovative terrestrial data and SaaS products as part of our software engineering team.

About the company

The global economy urgently needs to transition to new models of operation that remove carbon from the atmosphere and preserve and restore natural capital. Chloris Geospatial is an early-stage company developing technology that will transform the way businesses and governments use data about the environment to guide their investment and carbon management decisions. We use satellite data and artificial intelligence to create products that enable organizations to confidently invest in nature-based solutions, reduce their carbon footprint, and achieve their sustainability goals. We are mission-driven and laser-focused on building best-in-class technology that has a positive impact and is commercially successful. The founding team has deep experience and is committed to helping organizations understand their impact and make the decisions that benefit their bottom line and the planet. We seek extraordinary individuals to join our team, help us build our technology stack and commercial product suite, and enable a global revolution.

About the role

As Data Engineer, you will help build and maintain the software and data processing infrastructure to ingest and process large and complex data sets from satellites and other sources of geospatial data. You will support data scientists by providing infrastructure and tools they can use to deliver data and analytics products. You are experienced and skilled in building scalable pipelines for data ingestion, processing, and management in a cloud environment, with focus on large geospatial datasets. You know how to access and process large geospatial datasets through APIs. You have experience supporting both geospatial analytics and you are familiar with open source software tools and packages that are commonly applied to geospatial data.

Responsibilities

Collaborate on a team of software engineers, data scientists, and product developers.
Contribute to the design and implementation of Chloris Geospatial’s data processing technology stack.
Create and maintain pipelines for processing very large remote sensing and other geospatial data sets in a cloud environment.
Work with data scientists, and product engineers to build commercially valuable geospatial products, services, and insights.
Use best practices in engineering and software development (sprint planning, code reviews, testing and deployment of operational code).
Ensure that Chloris Geospatial’s software stack is up-to-date and uses best-in-class software engineering and geospatial data processing technology.

Qualifications:

2+ years experience in software engineering in a development environment preferably with Python
2+ years experience working in a cloud environment (AWS, Azure, GCP).
Experience working with, analyzing, and processing large scale geospatial data sets using state-of-the-art packages developed by the open source geospatial community (GDAL, Shapely, Rasterio, Xarray, etc)
Familiarity with common geospatial data formats such as Shapefiles, GeoTIFF, NetCDF, and ZARR
Experience working with data scientists and SaaS product developers to operationalize machine learning models in commercial products.
Strong communication skills, positive attitude, independent and resourceful, and excited to work in a fast-paced and collaborative team environment.

Ideal candidates will have:

Experience developing RESTful APIs using the OpenAPI Specification
Experience with Docker, Kubernetes, and/or Dask
Familiarity with Terraform, CloudFormation or other IaC tools
Familiarity with CICD best practices for building, testing and deploying code

Location: Boston, MA - Florence, Italy, remote options possible

Chloris Geospatial is an equal opportunity employer that values diversity. We are excited to build a diverse and inclusive team and we encourage inquiries from talented and motivated applicants from all races, religions, colors, nationalities, genders, sexual orientations, ages, and disability groups. Come join us and help us build the future!"
General Motors,Data Analytics Engineer,"Austin, TX","Job Description

Note, this position is:

This is a Hybrid position within our IT Organization. The role will allow employees to work offsite but will also require onsite work based on business needs. The selected candidate will be expected to commute to the innovation center to which they are assigned as their primary GM facility. Relocation may be provided.""

The User Experience Optimization & Enablement team is responsible for enabling self-service analytics development and empowering GM analytics professionals to deliver data driven insights. The Data Analytics Engineer is responsible for solving complex technical problems and collaborating directly with users and supporting IT teams to incubate innovation and proliferate the use of data in business decision making.

Responsibilities

Collaborate with users to solve complex technical issues impacting ad hoc data engineering, data analysis, and data science pipelines
Identify and understand user personas and usage patterns across enterprise big data and analytics platforms
Develop strategies to drive user experience improvements and work cross-functionally to achieve results
Define best practices for tools selection and scalable, efficient, and supportable development
Stay current on big data and analytics technologies and maintain technical skills
Understand enterprise data assets that can be used to derive high value insights
Communicate effectively and build trust with stakeholders from diverse technical backgrounds and positions within the company
Lead and mentor other big data technologists with diverse specialties and experience levels
Foster team-oriented environment that embraces GM cultural values - Innovate Now, Think Customer, One Team, Be Bold, Look Ahead, It's On Me, Win With Integrity, Be Inclusive

Minimal Qualifications

STEM oriented bachelor's degree or 4 years equivalent applicable experience
7 years of experience in most of the following:
Industry leading large-scale database platforms and speed layers (i.e. Hadoop, Kubernetes, Elastic Storage System, Greenplum)
Big data and analytics frameworks and languages (i.e. Hive, SQL, Spark, Python, Scala)
Data analysis and reporting tools (i.e. Power BI)
Large scale data warehouse and data integration implementations

Preferred Qualifications

Insightful and generally clever
Highly collaborative work style and the ability to work with and teach others
Ability to multi-task and stay organized in a dynamic work environment
Experience with data engineering, data architecture, and machine learning is a plus

About GM

Our vision is a world with Zero Crashes, Zero Emissions and Zero Congestion and we embrace the responsibility to lead the change that will make our world better, safer and more equitable for all.

Why Join Us

We aspire to be the most inclusive company in the world. We believe we all must make a choice every day - individually and collectively - to drive meaningful change through our words, our deeds and our culture. Our Work Appropriately philosophy supports our foundation of inclusion and provides employees the flexibility to work where they can have the greatest impact on achieving our goals, dependent on role needs. Every day, we want every employee, no matter their background, ethnicity, preferences, or location, to feel they belong to one General Motors team.

Benefits Overview

The goal of the General Motors total rewards program is to support the health and well-being of you and your family. Our comprehensive compensation plan incudes, the following benefits, in addition to many others:

Paid time off including vacation days, holidays, and parental leave for mothers, fathers and adoptive parents;
Healthcare (including a triple tax advantaged health savings account and wellness incentive), dental, vision and life insurance plans to cover you and your family;
Company and matching contributions to 401K savings plan to help you save for retirement;
Global recognition program for peers and leaders to recognize and be recognized for results and behaviors that reflect our company values;
Tuition assistance and student loan refinancing;
Discount on GM vehicles for you, your family and friends.

Diversity Information

General Motors is committed to being a workplace that is not only free of discrimination, but one that genuinely fosters inclusion and belonging. We strongly believe that workforce diversity creates an environment in which our employees can thrive and develop better products for our customers. We understand and embrace the variety through which people gain experiences whether through professional, personal, educational, or volunteer opportunities.GM is proud to be an equal opportunity employer.

We encourage interested candidates to review the key responsibilities and qualifications and apply for any positions that match your skills and capabilities.

Equal Employment Opportunity Statements

The policy of General Motors is to extend opportunities to qualified applicants and employees on an equal basis regardless of an individual's age, race, color, sex, religion, national origin, disability, sexual orientation, gender identity/expression or veteran status. Additionally, General Motors is committed to being an Equal Employment Opportunity (EEO) Employer and offers opportunities to all job seekers including individuals with disabilities. If you need a reasonable accommodation to assist with your job search or application for employment, email us atCareers.Accommodations@GM.com. In your email, please include a description of the specific accommodation you are requesting as well as the job title and requisition number of the position for which you are applying."
Piper Companies,Data Engineer,"United States
Remote","Job Description

Piper Companies is currently seeking a Data Engineer for an opportunity in Philadelphia, Pennsylvania for 100% permanently remote work. The Data Engineer will fix issues, optimize processes, and automate the engineering team’s data endeavors. Deliver elegant, scalable solutions that meet the business and technical needs.

Responsibilities of the Data Engineer:

Troubleshoot and resolve issues relating to data integrity 
Leverage distributed computing and serverless architecture such as AWS EMR & AWS Lambda, to develop pipelines for transforming data 
Help establish procedures and best practices for transforming and storing dataLead requirements 


Qualifications of the Data Engineer:

3 + years of professional Experience in a SQL Engineer role - MySQL and Postgres
3 + years’ experience with Python, Scala
3 + years’ experience with PySpark and Spark – SQL -writing, testing, debugging spark routines
1 + year/s experience using AWS CLI and boto3 


 Compensation for the Data Engineer:

Salary Range: $80,000 - $150,000 Annually
Remote Option: 100% Permanently Remote – Est time zone
Comprehensive Benefits: Medical (Cigna), Dental (Cigna), Vision (Cigna), 401K with match (ADP Retirement), PTO, and Holidays


Keywords: Data Engineer, Data, Engineer, SQL Engineer, MySql, Postgres, Python, Scala, AWS, AZURE, GCP, boto3, AWS EMR, AWS S3, AWS CLI, Apache Airflow, Apache Zeppelin, Healthcare data, Pyspark, Spark, spark routines, database, Data operations, client delivery, MongoDB, health, dental, vision, retirement, benefits, vacations"
Amazon Web Services (AWS),Data & ML Engineer - Nationwide Opportunities,"Seattle, WA","Description

At Amazon Web Services (AWS), we’re hiring highly technical cloud computing architects and engineers to collaborate with our customers and partners on key engagements. Our consultants will develop and deliver proof-of-concept projects, technical workshops, and support implementation projects. These professional services engagements will focus on customer solutions such as Machine Learning, Data and Analytics, HPC and more.

In this role, you will work with our partners, customers and focus on our AWS Analytics and ML service offerings such Amazon Kinesis, AWS Glue, Amazon Redshift, Amazon EMR, Amazon Athena, Amazon SageMaker and more. You will help our customers and partners to remove the constraints that prevent them from leveraging their data to develop business insights.

AWS Professional Services engage in a wide variety of projects for customers and partners, providing collective experience from across the AWS customer base and are obsessed about customer success. Our team collaborates across the entire AWS organization to bring access to product and service teams, to get the right solution delivered and drive feature innovation based upon customer needs.

In our Global Specialist Practice, you will also have the opportunity to create white papers, writing blogs, build demos and other reusable collateral that can be used by our customers, and, most importantly, you will work closely with our Solution Architects, Data Scientists and Service Engineering teams.

The ideal candidate will have extensive experience with design, development and operations that leverages deep knowledge in the use of services like Amazon Kinesis, Apache Kafka, Apache Spark, Amazon Sagemaker, Amazon EMR, NoSQL technologies and other 3rd parties.

Excellent business and communication skills are a must to develop and define key business questions and to build data sets that answer those questions. You should be able to work with business customers in understanding the business requirements and implementing solutions.

Inclusive Team Culture

Here at AWS, we embrace our differences. We are committed to furthering our culture of inclusion. We have thirteen employee-led affinity groups, reaching 85,000 employees in over 190 chapters globally. We have innovative benefit offerings, and host annual and ongoing learning experiences, including our Conversations on Race and Ethnicity (CORE) and AmazeCon (gender diversity) conferences. Amazon’s culture of inclusion is reinforced within our 16 Leadership Principles, which remind team members to seek diverse perspectives, learn and be curious, and earn trust.

Work/Life Balance

Our team puts a high value on work-life harmony. Striking a healthy balance between your personal and professional life is crucial to your happiness and success here. We are a customer-obsessed organization—leaders start with the customer and work backwards. They work vigorously to earn and keep customer trust. As such, this is a customer facing role in a hybrid delivery model. Project engagements include remote delivery methods and onsite engagement that will include travel to customer locations as needed.

Mentorship & Career Growth

Our team is dedicated to supporting new members. We have a broad mix of experience levels and tenures, and we’re building an environment that celebrates knowledge sharing and mentorship. We care about your career growth and strive to assign projects based on what will help each team member develop into a better-rounded professional and enable them to take on more complex tasks in the future.

This is a customer facing role. You will be required to travel to client locations and deliver professional services when needed.


Basic Qualifications

Bachelor’s degree, or equivalent experience, in Computer Science, Engineering, Mathematics or a related field
5+ years’ experience of Data platform implementation, including 3+ years of hands-on experience in implementation and performance tuning Kinesis/Kafka/Spark/Storm implementations.
Experience with analytic solutions applied to the Marketing or Risk needs of enterprises
Basic understanding of machine learning fundamentals.
Ability to take Machine Learning models and implement them as part of data pipeline
5+ years of IT platform implementation experience.
Experience with one or more relevant tools ( Flink, Spark, Sqoop, Flume, Kafka, Amazon Kinesis ).
Experience developing software code in one or more programming languages (Java, JavaScript, Python, etc).
Current hands-on implementation experience required

Preferred Qualifications

Masters or PhD in Computer Science, Physics, Engineering or Math.
Hands on experience working on large-scale data science/data analytics projects.
Ability to lead effectively across organizations.
Hands-on experience with Data Analytics technologies such as AWS, Hadoop, Spark, Spark SQL, MLib or Storm/Samza.
Implementing AWS services in a variety of distributed computing, enterprise environments.
Proficiency with at least one the languages such as C++, Java, Scala or Python.
Experience with at least one of the modern distributed Machine Learning and Deep Learning frameworks such as TensorFlow, PyTorch, MxNet Caffe, and Keras.
Experience building large-scale machine-learning infrastructure that have been successfully delivered to customers.
Experience defining system architectures and exploring technical feasibility trade-offs.
3+ years experiences developing cloud software services and an understanding of design for scalability, performance and reliability.
Experience working on a code base with many contributors.
Ability to prototype and evaluate applications and interaction methodologies.
Experience with AWS technology stack.
Written and verbal technical communication skills with an ability to present complex technical information in a clear and concise manner to a variety of audiences.

Amazon is committed to a diverse and inclusive workplace. Amazon is an equal opportunity employer and does not discriminate on the basis of race, national origin, gender, gender identity, sexual orientation, protected veteran status, disability, age, or other legally protected status. For individuals with disabilities who would like to request an accommodation, please visit https://www.amazon.jobs/en/disability/us.


Company - Amazon Web Services, Inc.

Job ID: A1437554"
Egnyte,Marketing Data Engineer,"Salt Lake County, UT
Remote","Posted by

Gina Giroux, CIR 2nd

Senior Recruiter at Egnyte

The Role

Our marketing team is growing and we are looking for a talented software engineer with a passion for data to join us in building a scalable, easy-to-use marketing data platform. Your mission is to develop and provide the tools and data pipelines necessary to allow marketers, analysts, and data integrators across Egnyte to accomplish their goals which include increasing revenue, reducing churn, enhancing customer satisfaction, and running critical business operations. You will contribute to the vision for data infrastructure and analysis tools and work with your fellow engineers, data architects, and data analysts to establish best practices for creating systems and datasets that marketing will use. You should possess deep technical skills, be comfortable working in and contributing to our data infrastructure, and be excited about building a strong data foundation for the company. We are looking for someone who is excited about having a big impact at Egnyte and is passionate about heavily impacting revenue and customer experience.




What You Will Do (Not Limited To)

· Identify data needs for the marketing team, understand specific requirements for metrics and analysis, and build efficient and scalable data pipelines to enable a data-driven marketing approach

· Develop and maintain marketing data models to enable advanced reporting on funnel performance, attribution & ROI, campaign efficiency & performance, lead management, as well as SLA’s

· Facilitate data models and own transformations for moving data between applications; ensuring interoperability of applications with database, data warehouse, and data mart environments

· Assist with the design and management of our technology stack used for data storage and processing

· Contribute to the development and education plans on data engineering capabilities, systems, standards, and processes




Qualifications

· 3+ years of working experience in a growth, software, or data engineering role

· Expertise in a programming language such as SQL and data warehousing tools preferably BigQuery, Snowflake or Redshift.

· Strong experience with BI tools preferably Tableau.

· Strong experience in relational databases, SQL, data warehouses, and ETL pipelines.

· Experienced in integrating data from core platforms like Marketing Automation, CRM, and Analytics into a centralized warehouse.

· Expert Knowledge of Marketo, Salesforce.com.

· Knowledge of Google Analytics is a plus.

· Experienced in integrating and aggregating media and spend data from advertising partners and platforms into databases, warehouses, and marketing systems.

· Rigor in high code quality, automated testing, and other engineering best practices.

· The ability to communicate cross-functionally, derive requirements, and architect shared datasets.




Our Benefits

Competitive salaries, comprehensive benefits & pre–IPO stock options

Flexible hours, generous time off (RTO, Responsible Time Off) to help support your work-life balance

Full offering of paid holidays and sick time

Paid family leave to help you grow your family (12 weeks for birth parent, 10 weeks for non-birth parent, 10 weeks for adoption)

Gym, cell phone, internet and commute reimbursement

401(k) Retirement Plan (Traditional and Roth)

Health Savings Account (HSA), Flexible Spending Account (FSA) and Employee Assistance Program (EAP)

Wellbeing programs to help you feel healthy and balanced including memberships to Aaptiv, Ginger, Headspace and more

Perks including discounted pet insurance, electronics, theme park tickets, travel, and more

SoFi online financial services: student loan refinancing, personal loans, and investing

Free personal Egnyte account for life




Equal Opportunity Employment

At Egnyte, we celebrate our differences and thrive on our diversity for the benefit of our employees, our products, our customers, our investors, and our communities. Employment at Egnyte is based solely on a person’s merit and qualifications directly related to professional competence. We do not discriminate based on race, color, ancestry, religion, sex, national origin, sexual orientation, gender identity or expression, disability, age, veteran status, marital status, pregnancy or related condition, or any other basis protected by law.




About Egnyte

Egnyte provides a unified content security and governance solution for collaboration, data security, compliance, and threat detection for multicloud businesses. More than 16,000 organizations trust Egnyte to reduce risks and IT complexity, prevent ransomware and IP theft, and boost employee productivity on any app, any cloud, anywhere. Investors include GV (formerly Google Ventures), Kleiner Perkins, Caufield & Byers and Goldman Sachs."
Apple,AMP Ops Intelligence Data Engineer,"Culver City, CA","
Summary

The people here at Apple don’t just build products — we craft the kind of wonder that’s revolutionized entire industries! It’s the diversity of those people and their ideas that supports the innovation that runs through everything we do, from amazing technology to industry-leading environmental efforts. Join Apple, and help us leave the world better than we found it!


Apple Media Products (AMP) Operations Intelligence team is looking for a passionate and highly motivated Data Engineer. We build analytics to empower Video Operations team to make effective business decisions and improve their productivity/process efficiencies. You will be responsible for building multiple reporting projects around exciting Video products like Apple TV+, Fitness+, Channels and Sports.


This role requires cross-functional interaction within our AMP Production Operations, Engineering and other analytics teams.


Key Qualifications

Experienced data analyst, data engineer or data scientist.


Experience in programming language, e.g., Python, Scala.


Experience querying in Hadoop, e.g., Spark, Hive.


Experience designing and creating datasets in Hadoop.


Experience creating ETL workflows on large volumes of raw data from multiple sources.


Experience building and managing data pipeline workflows, e.g., Airflow.


Experience working in Big Data distributed SQL query engines, e.g., Presto.


Understanding of container based orchestrating tool, e.g. Kubernetes.


Technical proficiency with SQL and relational databases.


Ability to understand API Specs, identify relevant API calls, extract/transform data and implement SQL friendly data structures.


Strong understanding of large-scale content management systems and data modeling strongly preferred.


Excellent analytical and problem solving skills.


Experience in Tableau is good to have.


Description

Develop and own solutions that translate analytical questions into effective reports that drive business action.


Identify business and technical requirements, conduct full technical discovery and architect the analytics solutions.


Design, build and support the analytics datasets, which can then be easily consumed by visualization tools like Tableau.


Build and schedule data pipeline workflows so that the created datasets refresh automatically on a recurring basis.


Evaluate ever evolving analytics technology stack and provide recommendations.


Responsible for incoming analytics requests and collaborate with other Engineering teams to accomplish monthly release cycles.


Provide technical analytics leadership and guidance.


Education & Experience

Bachelor’s or Master’s degree in a technical field (Computer Science, Engineering and Mathematics).


Additional Requirements

Location options include: Culver City, Austin, New York, and Seattle


Role Number: 200345878

"
Dice,Urgent hire for ML & Data Science Engineer,"Sunnyvale, CA
On-site","Dice is the leading career destination for tech experts at every stage of their careers. Our client, Sumeru, is seeking the following. Apply via Dice today!

Primary Responsibilities Design machine learning systems to automate predictive models. Develop ML algorithms to analyze historical data to make predictions. Run tests, performing statistical analysis, and interpreting test results. Develop and support data pipelines to extract, transform and load data into Google Data Center data warehouse. Demonstrate ability and willingness to learn quickly and complete large volumes of work with high quality. Demonstrate outstanding collaboration, interpersonal communication and written skills with the ability to work in a team environment. Minimum qualifications Bachelor's degree in a quantitative discipline (e.g., Statistics, Computer Science, Math, Physics, Engineering), or equivalent practical experience. 3+ years of experience in analytics or similar fields. Experience with scripting languages (e.g. Python, Perl, etc.). Experience manipulating data sets in SQL and using statistical software (e.g., R, SAS, MATLAB, NumpyPandas). Experience in software engineering, including gathering data for analysis, performing data filtering and preparation, and running tests at scale. Preferred qualifications Masters degree in a quantitative discipline. Experience in machine learning (e.g., supervised learning, clustering). Experience designing data models or table schema. Experience writing and maintaining extract, transform, and load scripts which operate on a variety of structured and unstructured sources. Knowledge of statistics (e.g., probability theory, hypothesis testing, regressions), and experimentation theory.."
SimplePractice,Senior Data Engineer,"Santa Monica, CA
Remote","About Us




At SimplePractice, our team is dedicated to improving the health and wellness industry by building a suite of innovative solutions for practitioners and their clients. Our product supports practitioners on their clinical journey to becoming licensed, helps them manage their business and practice once they’re up and running, and enables new clients to discover and interact with practitioners. Taking a practitioner-first approach in everything we do makes it possible for health and wellness practitioners to devote more time to their clients while they use SimplePractice to start, grow, and maintain a successful private practice.




The Role




SimplePractice is seeking a Senior Data Engineer to take the company’s data engineering to the next level of our data-driven/data-focused journey.




Our Data and Analytics Team is responsible for the data across the enterprise, organized in three functions. Information delivery is provided by our Data & Analytics group, the analytics itself is enabled by Data Engineering & Architecture, and Data Governance groups.




The information from our Analytics Team is paramount to our company's successful operation and growth. Data Engineering and Architecture covers data ingestion and transformations across our analytics platform, as well as the overall enterprise data architecture and our product’s database backend architecture and data access layer.




This role will be leasing our data ingestion efforts within the Data Engineering & Architecture team. All of which enables us to effect data sanctity, making it usable for decision making. Data Governance includes data stewardship, lineage, quality, and security. We employ best practices in delivering each of these functions to the utmost benefit of our organization. As the data vision gets implemented and the company grows, there are many opportunities for personal growth.




Responsibilities




Partner with analysts to build scalable systems that help unlock the value of data from a wide range of sources such as backend databases, event streams, and marketing platformsConsult with our Product and Engineering Teams in the creation of new data in the production environment
Create company wide alignment through standardized metrics across the company
Promote importance of dimensional data models in communicating across the organization
Manage the complete data stack from ingestion through data consumptionConnect our teams and their workflows to centralized and secure databases
Build tools to increase transparency in reporting company wide business outcomes
Define and promote data engineering best practices
Design scalable data solutions leveraging cloud data technologies, preferably in AWS
Help define data quality and data security framework to measure and monitor data quality across the enterprise
Excellent problem-solving & critical-thinking skills to meet complex data challenges and requirements in a fast paced, rapidly changing environment




Desired Skills & Experience




Along with the responsibilities and competencies specified above, we are looking for an individual who possesses a positive, action-oriented attitude and understands the importance of taking initiative within a team environment.




8+ years of progressive professional experience preferred
Top-notch SQL, statistical/window functions, complex data typesExpert in relational technology, data modeling, and in dimensional modeling
Expert in at least two database engines, preferably MySQL, Snowflake, or Athena
Metadata-driven and database-centric concepts
Database performance
Data transformationsExpert at ETL and ETL tools, including Airflow, DBT
ELT and schema-on-read concepts
Data ingestion tools, such as Kafka, Singer
At least one programming language, preferably Python
Unix/Linux scripting, such as bash
Experience with APIs, such as via curl
Experience with achieving performance through parallelism
DAGs
Experience with cloud-based infrastructure, particularly AWSCloud storage, S3
Data storage formats, such as Parquet, ORC
Experience with external tables
Unstructured and semi-structured data types, JSON
Data analyticsExperience with at least one visualization tool, preferably, Looker, Tableau, Periscope
Superb communication skills
BS/MS degree in Engineering, Mathematics, Physics, Computer Science or equivalent experience




Bonus Points




Big data tools and engines, Glue, Hive, Presto
Enterprise architecture and enterprise data architecture (data modeling and enterprise dimensional modeling)
Project & Change Management skills especially experience working in an Agile (SCRUM, Kanban) environment/team focusing on sprint by sprint deliveries"
Kin + Carta,Data Scientist / Data Engineer,"United States
Remote","Cascade Data Labs is a boutique consulting agency (operating as part of Kin + Carta) with end-to-end experience helping Fortune 500 companies form and execute their analytics strategy and infrastructure. We believe strongly that the difference between analytics success and failure comes down to practitioner-level understanding of the “details” presented across a variety of disciplines: data collection, organization, and engineering in a rapidly evolving technological landscape; analytical inference on multi-dimensional data sets; and the communication of findings to business stakeholders in visually compelling ways. As such, we seek a new breed of data professionals that can cut across these disciplines as project requirements dictate, blurring the lines between data analyst, scientist, and engineer, with the ability to work across a multitude of projects and contribute across various aspects of data product . While our culture promotes continual development and multidisciplinary growth, over time employees may gravitate towards a specific specialism and thrive in building a deep expertise in their domain of interest.

Ideal candidates will have strong quantitative backgrounds and analytical discipline. While they will have some demonstrated ability to write code, they will not have learned programming languages for the sake of building their resume, but rather as a means to express their intellectual curiosity and analytical voice. Cascade Data Labs will provide a platform and training to help them reach their full potential.

Core Responsibilities


Analyze a collection of raw data sets to create meaningful impact to large enterprise clients while maintaining a high degree of scientific rigor and discipline.
Engineer data pipelines and products to help stakeholders make and execute data driven decisions.
Communicate analytical findings in an intuitive and visually compelling way.


Potential Responsibilities (depending on aptitude, project, and seniority)


Creating highly visual and interactive dashboards via Tableau, PowerBI, or custom web applications
Conducting deep dive analysis and designing KPIs to help guide business decisions and measure success
Engineering data infrastructure, software libraries, and APIs supporting BI and ML data pipelines
Architecting cloud data platform components enabling the above
Building and tracking project timelines, dependences, and risks
Gathering stakeholder requirements and conducting technical due diligence toward designing pragmatic data-driven business solutions


Minimum Qualifications

We want all new hires to succeed in their roles at Kin + Carta. That's why we've outlined the job requirements below. To be considered for this role, it's important that you meet all Minimum Qualifications. If you do not meet all of the Preferred Qualifications, we still encourage you to apply.


Minimum of 2 years professional experience in data science, analytics, and/or engineering
Bachelors or Masters degree in quantitative studies including Engineering, Mathematics, Statistics, Computer Science or computation-intensive Sciences and Humanities. Recent graduates should include GPAs in their resumes from degrees obtained in the last 5 years.
Proficiency (can execute data ingestion to insight) in programmatic languages such as SQL, Python, R


Preferred Qualifications


Proficiency in visualization/reporting tools such as Tableau and PowerBI or programmatic visualization library such as R ggplot2, Python matplotlib/seaborn/bokeh, Javascript D3.
Proficiency scripting in UNIX environment
Proficiency in big data environments and tools such as Spark, Hive, Impala, Pig, etc.
Proficiency with cloud architecture components (AWS, Azure, Google)
Proficiency with data pipeline software such as Airflow, Luigi, or Prefect
Ability to turn raw data and ambiguous business questions into distilled findings and recommendations for action
Experience with statistical and machine learning libraries along with the ability to apply them appropriately to business problems
Proficiency in front and back-end web application development stacks and frameworks (Javascript, HTML, CSS, React/Vue/AngularJS) including API design (REST/GraphQL) and library development
Experience leading and managing technical data/analytics/machine learning projects


Job Type: Full-time

About Kin + Carta

Kin + Carta exists to make the world work better by delivering transformative digital growth for our clients. As a digital transformation firm, we apply creativity, data, and technology to help clients invent, market, and operate new digital products and services that turn prospects into customers, and customers into advocates. Kin + Carta seamlessly integrates the strategic consulting, software engineering, and marketing technology needed to help businesses 'Make It Happen'.

Consultants at Kin + Carta have the opportunity to: engage with the most cutting-edge emerging technologies, solve difficult problems for some of the world’s biggest companies, and work within a breadth of industries.

Kin + Carta is headquartered in Chicago and London with offices across four continents. Our culture and shared ways of working unite us globally, but it’s our diverse specialisms, our connections, and our courage that makes our firm a recognized best place to work.

We welcome our Kin to show up as their full selves every day. Because this is so important to us, Kin + Carta is proud to be an equal opportunity employer. To read further about our commitment to Inclusion, Diversity, Equity, and Awareness, check out our IDEA page on our website.

If you need accommodations at any point in the application or interview process, please let us know."
Dice,Immediate Hiring - AWS Cloud Data Engineer @Remote,"United States
Remote","Dice is the leading career destination for tech experts at every stage of their careers. Our client, Technogen, Inc., is seeking the following. Apply via Dice today!

TECHNOGEN, Inc. is a Proven Leader in providing full IT Services, Software Development and Solutions for 15 years. TECHNOGEN is a Small Woman Owned Minority Business with GSA Advantage Certification. We have offices in VA MD Offshore development centers in India. We have successfully executed 100+ projects for clients ranging from small business and non-profits to Fortune 50 companies and federal, state and local agencies. Role AWS Engineer Duration 12+ months contract Location Remote Job Description Production experience on enterprise grade operational Data Lake formationimplementation and maintenance Hands-on experience developing systems that leverage multiple AWS services including CloudFormation, Lambda, API Gateway, S3, DynamoDBany NoSQL, Relational Database Service (RDS), Glue Catalog ETLs, Athena and Quick Sight for data management and analysis Strong expertise ingesting historic and near real time structuredsemi structuredunstructured data into data lake Strong Experience in working with Data Scientists, Architects, and business experts to clarify requirements for components and features of the Data Lake. Design, document and implement solutions to problems involving data Ingestion, Transformation, Cataloging, In-Place Querying, Storage, and Security using AWS tools and best practices Experience in migrating legacy systems into an Enterprise Data Lake is a Plus Experience in Monitoring and Optimizing the Data Lake environments Proficient in Python, SQL queries using AWS Athena and mapping of various relational DB queries to Athena ExperienceExposure to security best practices with Data Lake Follow and enforce strict standards for code quality, automated testing, infrastructure-as-code, and code maintainability Robust debugging skills and knowledge of automated testing platforms and unit tests Ability to work in a collaborative Agile environment DevOps background preferred Python andor Node.js Container Experience (DockerPod man) Thanks Regards,"
Nesco Resource,Data Analytics Engineer,"New York, NY","Job Description

About the Role:

As a Data Analytics Engineer, you would build analytics products to inform strategic decision making, both internally and for our university partners. Our products help senior leaders understand successes and opportunities across all parts of the student journey, from engagement with our online marketing materials through enrollment, all the way to graduation.

As a Data Analytics Engineer You Will

Be responsible for collaboratively building data analytics pipelines and dashboards for multiple stakeholders. This includes

Translating high-level product requirements into technical requirements
Identifying new data we need and developing ingestion processes
Clarifying business rules and implementing them in code
Transforming data using SQL to power our analytics products
Writing automated tests, documenting your work, and reviewing others' work
Building Tableau visualizations to make data actionable
Teaching stakeholders how to use your products and incorporating their feedback

You Have

Experience using git, complex SQL, and AWS to build and deploy data products
Experience creating visualizations in Tableau
Ability to work cross-functionally to iteratively develop data products and processes
Experience analyzing data from digital marketing, CRM, SIS or LMS tools
Experience using Python, dbt, AWS Athena, or CircleCI (Bonus)

Nesco Resource and affiliates (Lehigh G.I.T Inc, and Callos Resource, LLC) is an equal employment opportunity employer and does not discriminate on the basis of race, color, religion, sex, sexual orientation, gender identity, national origin, disability, age, or veteran status, or any other legally protected characteristics with respect to employment opportunities."
Lionsgate,Data Engineer,"Englewood, CO","The STARZ Data Products & Engineering team is seeking a data engineer who’s passionate about drawing a thin line from data to insight and exposing that insight to internal users to support important business decisions. You’ll be building data pipelines, crafting our data processing framework, and helping us to integrate disparate signals into a clear picture of where the organization is at and where we can go. We’re building out data solutions so business units can use that information to form hypotheses, test them, and easily see the results.

From several sources including database systems, vendor supplied files, API endpoints, and AWS S3 storage, you’ll be working across our entire technology stack to find places where we can use data to improve our business. Our users range from extremely technical (providing us with models to run, and wanting tables or files in return) to less technical (providing inputs via a UI or spreadsheet and expecting results displayed in-line or in a report). You will also work closely with our Business Intelligence team to provide them with data models and data marts to support reporting needs. There is a heavy emphasis on providing our operational business teams with the data they need to manage their work flows. This is an opportunity to exercise your technical skills with a highly skilled team from a variety of diverse backgrounds while helping every aspect of our business to be more efficient and more data-driven.


Develop, test and maintain optimal data processing pipelines and related architectures, ensuring the overall solution will support business requirements
Build the infrastructure required for optimal extraction, transformation and loading of data from a wide variety of data sources using cloud-native technologies
Model and build large, complex data sets that meet functional/non-functional business requirements
Develop and implement processes for data ingestion, extraction, mining and production
Continue to build out our data processing framework, and expand it to support less and less technical users
Employ a variety of languages, tools and techniques to marry systems and the data generated from those systems together to maximize the strategic value of the data
Build analytics tools that utilize the data pipeline to provide actionable insights into customer acquisition, retention, behavior and other key business performance metrics

Responsibilities Cont.

Create data tools for analytics and data scientist team members that assist them in building and optimizing our products
Recommend and implement ways to improve/ensure data reliability, efficiency, and quality
Employ sophisticated analytics, statistical methods and data models to prepare data for use in predictive and prescriptive modeling
Explore and examine data to find hidden patterns 
Identify, design and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.
Discover opportunities for new data acquisition
Be prepared to translate findings to key stakeholders
Work in a heavily collaborative environment with other team members, taking on a variety of roles when necessary with a positive attitude.

2+ years working directly as a member of a technical team, preferably developing/supporting/innovating for a consumer-facing analytics product.
BS degree in information management, computer science, math, statistics, or equivalent technical field.

Data Warehousing: ETL/ELT or data migration experience
Languages: SQL, C#, Python
Database Systems: SQL Server, Oracle, MySQL
Orchestration: SSIS, Matillion, Informatica, AWS Glue or equivalent
Data Modeling: Relational and Star schemas understanding

Nice To Haves

Cloud Technologies: AWS, Azure, or Google Cloud
Data Architectures: Data Lakes
Languages: JSON, XML, YAML, Typescript, Linux shell scripting, Terraform
Database Systems: Snowflake or Redshift, Aurora, DynamoDB, PostgreSQL, Redis
Data Processing: Snowpipes, Spark, Spark Streaming, Kafka, Kinesis, Pandas, Airflow
AWS: EC2/ECS/Lambda/Step Functions/EKS/EMR, Kubernetes, docker, CloudFormation, CDK

Starz (www.starz.com), a Lionsgate company (NYSE: LGF.A, LGF.B), is a leading global media and entertainment company that produces and distributes premium streaming content to worldwide audiences across subscription television platforms. Starz is home to the flagship domestic STARZ® brand, STARZ ENCORE, 17 premium pay TV channels and the associated on-demand and online services, including the highly rated STARZ app. With the launch of the STARZPLAY international premium streaming platform and STARZ PLAY Arabia, Starz is expanding its global footprint in a growing number of territories. Sold through multichannel video distributors, including cable operators, satellite television providers, telecommunications companies, and other online and digital platforms, Starz offers subscribers more than 7,500 distinct premium television episodes and feature films, including STARZ Original series, first-run movies and other popular programming.

Starz

With the Company aligning its studio operations behind the growth of its streaming business, Starz has become one of the pre-eminent modern premium global streaming platforms. Offering subscribers more than 7.500 television episodes, including Starz original series and provocative documentaries, along with a broad catalogue of first-run movies, Starz is taking the lead in delivering relatable premium content that makes it the platform of choice among a wide spectrum of female, African-American and other historically underserved audiences. Its focused brand, premium content and freedom from legacy relationships position Starz at the forefront of the new bundles emerging throughout the media ecosystem, a compelling value proposition to complement virtually every kind of subscription platform.

Distinguished by its successful and focused content strategy, top programming, exceptional curation and speed to market, Starz has quickly scaled its platform to become one of the most widely distributed and fastest-growing OTT services in the world, with OTT subscribers making up more than half of its global subscriber base. The Starzplay International premium subscription service offers a “best of global SVOD” content portfolio in more than 50 countries throughout Europe, Latin America, Canada, Japan and India through a bespoke and expanding network of distribution partners. The Starzplay Arabia joint venture is one of the leading SVOD operators in the fast-growing Middle East and North Africa region. The proprietary and highly-rated Starz app, a proven hit with U.S. audiences, continues to roll out internationally.


Full Coverage – Medical, Vision, and Dental
Annual discretionary bonus and merit increase
Work/Life Balance – generous sick days, vacation days, holidays, and Impact Day
401(k) company matching
Tuition Reimbursement (up to graduate degree)

$95,400 - $122,400

EEO Statement

Lionsgate is an equal employment opportunity employer. All employees and applicants are evaluated on the basis of their qualifications, consistent with applicable state and federal laws. In addition, Lionsgate will provide reasonable accommodations for qualified individuals with disabilities. Lionsgate will consider for employment qualified applicants with criminal histories in a manner consistent with the requirements of applicable state and federal law."
FullStack Labs,Data Engineer - Remote - USA,"Las Vegas, NV
Remote","Summary Of Position

The STARZ Data Products & Engineering team is seeking a data engineer who’s passionate about drawing a thin line from data to insight and exposing that insight to internal users to support important business decisions. You’ll be building data pipelines, crafting our data processing framework, and helping us to integrate disparate signals into a clear picture of where the organization is at and where we can go. We’re building out data solutions so business units can use that information to form hypotheses, test them, and easily see the results.

From several sources including database systems, vendor supplied files, API endpoints, and AWS S3 storage, you’ll be working across our entire technology stack to find places where we can use data to improve our business. Our users range from extremely technical (providing us with models to run, and wanting tables or files in return) to less technical (providing inputs via a UI or spreadsheet and expecting results displayed in-line or in a report). You will also work closely with our Business Intelligence team to provide them with data models and data marts to support reporting needs. There is a heavy emphasis on providing our operational business teams with the data they need to manage their work flows. This is an opportunity to exercise your technical skills with a highly skilled team from a variety of diverse backgrounds while helping every aspect of our business to be more efficient and more data-driven.

Responsibilities

Develop, test and maintain optimal data processing pipelines and related architectures, ensuring the overall solution will support business requirements
Build the infrastructure required for optimal extraction, transformation and loading of data from a wide variety of data sources using cloud-native technologies
Model and build large, complex data sets that meet functional/non-functional business requirements
Develop and implement processes for data ingestion, extraction, mining and production
Continue to build out our data processing framework, and expand it to support less and less technical users
Employ a variety of languages, tools and techniques to marry systems and the data generated from those systems together to maximize the strategic value of the data
Build analytics tools that utilize the data pipeline to provide actionable insights into customer acquisition, retention, behavior and other key business performance metrics

Responsibilities Cont.

Create data tools for analytics and data scientist team members that assist them in building and optimizing our products
Recommend and implement ways to improve/ensure data reliability, efficiency, and quality
Employ sophisticated analytics, statistical methods and data models to prepare data for use in predictive and prescriptive modeling
Explore and examine data to find hidden patterns
Identify, design and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.
Discover opportunities for new data acquisition
Be prepared to translate findings to key stakeholders
Work in a heavily collaborative environment with other team members, taking on a variety of roles when necessary with a positive attitude.

Experience & Skills

2+ years working directly as a member of a technical team, preferably developing/supporting/innovating for a consumer-facing analytics product.
BS degree in information management, computer science, math, statistics, or equivalent technical field.

Required Skills

Data Warehousing: ETL/ELT or data migration experience
Languages: SQL, C#, Python
Database Systems: SQL Server, Oracle, MySQL
Orchestration: SSIS, Matillion, Informatica, AWS Glue or equivalent
Data Modeling: Relational and Star schemas understanding

Nice To Haves

Cloud Technologies: AWS, Azure, or Google Cloud
Data Architectures: Data Lakes
Languages: JSON, XML, YAML, Typescript, Linux shell scripting, Terraform
Database Systems: Snowflake or Redshift, Aurora, DynamoDB, PostgreSQL, Redis
Data Processing: Snowpipes, Spark, Spark Streaming, Kafka, Kinesis, Pandas, Airflow
AWS: EC2/ECS/Lambda/Step Functions/EKS/EMR, Kubernetes, docker, CloudFormation, CDK

About The Company

Starz (www.starz.com), a Lionsgate company (NYSE: LGF.A, LGF.B), is a leading global media and entertainment company that produces and distributes premium streaming content to worldwide audiences across subscription television platforms. Starz is home to the flagship domestic STARZ® brand, STARZ ENCORE, 17 premium pay TV channels and the associated on-demand and online services, including the highly rated STARZ app. With the launch of the STARZPLAY international premium streaming platform and STARZ PLAY Arabia, Starz is expanding its global footprint in a growing number of territories. Sold through multichannel video distributors, including cable operators, satellite television providers, telecommunications companies, and other online and digital platforms, Starz offers subscribers more than 7,500 distinct premium television episodes and feature films, including STARZ Original series, first-run movies and other popular programming.

Business Unit Overview

Starz

With the Company aligning its studio operations behind the growth of its streaming business, Starz has become one of the pre-eminent modern premium global streaming platforms. Offering subscribers more than 7.500 television episodes, including Starz original series and provocative documentaries, along with a broad catalogue of first-run movies, Starz is taking the lead in delivering relatable premium content that makes it the platform of choice among a wide spectrum of female, African-American and other historically underserved audiences. Its focused brand, premium content and freedom from legacy relationships position Starz at the forefront of the new bundles emerging throughout the media ecosystem, a compelling value proposition to complement virtually every kind of subscription platform.

Distinguished by its successful and focused content strategy, top programming, exceptional curation and speed to market, Starz has quickly scaled its platform to become one of the most widely distributed and fastest-growing OTT services in the world, with OTT subscribers making up more than half of its global subscriber base. The Starzplay International premium subscription service offers a “best of global SVOD” content portfolio in more than 50 countries throughout Europe, Latin America, Canada, Japan and India through a bespoke and expanding network of distribution partners. The Starzplay Arabia joint venture is one of the leading SVOD operators in the fast-growing Middle East and North Africa region. The proprietary and highly-rated Starz app, a proven hit with U.S. audiences, continues to roll out internationally.

Our Benefits

Full Coverage – Medical, Vision, and Dental
Annual discretionary bonus and merit increase
Work/Life Balance – generous sick days, vacation days, holidays, and Impact Day
401(k) company matching
Tuition Reimbursement (up to graduate degree)

Compensation

$95,400 - $122,400

EEO Statement

Lionsgate is an equal employment opportunity employer. All employees and applicants are evaluated on the basis of their qualifications, consistent with applicable state and federal laws. In addition, Lionsgate will provide reasonable accommodations for qualified individuals with disabilities. Lionsgate will consider for employment qualified applicants with criminal histories in a manner consistent with the requirements of applicable state and federal law."
HelloFresh,Senior Data Engineer,"Chicago, IL
On-site","At HelloFresh, we want to change the way people eat forever by offering our customers high quality food and recipes for different meal occasions. Over the past 10 years, we've seen this mission spread around the world and beyond our wildest dreams. Now, we are a global food solutions group and the world's leading meal kit company, active in 16 countries across 3 continents. So, how did we do it? Our weekly boxes full of exciting recipes and fresh ingredients have blossomed into a community of customers looking for delicious, healthy and sustainable options. The HelloFresh Group now includes our core brand, HelloFresh, as well as: GreenChef, EveryPlate, Chef's Plate, Factor, and Youfoodz.

Come see what's cookin' at HelloFresh!

At HelloFresh, we want to revolutionize the way we eat by making it more convenient and exciting to cook meals from scratch. We have offices all over the world and we deliver delicious meals to millions of people.

We are the industry leader in meal-kit subscription services and we're growing all the time. We have distinct meal-kit services that cater to everyone with the most menu variety in the market, which allows us to reach an incredibly wide population of people.

The HelloFresh team is diverse, high-performing, and international, and our work environment is an inspiring space where you can thrive as a result.

Job Description:

As a Senior Data Engineer at HelloFresh, you will be building and extending highly performant data products, using cutting-edge technologies. Within the Supply Chain Analytics team, you will work closely with fellow data engineers, data scientists, and analysts to teams across the Supply Chain to capture efficiencies. All in an effort to help deliver our vision, of delivering the perfect product, at the right time, to the doorstep of our customers.

You will ...

Build reusable technology that enables Supply Chain teams to capture, process, store, and serve their data products in an easy way
Build and maintain complex and scalable ETL pipelines
Work closely with the data infrastructure team and ensure and promote data quality and governance standards
Coaching and mentoring data engineers and developing internal talent


At a minimum, you have...

Extensive experience in Software Engineering and the ability to design, implement and deliver maintainable and high-quality code with Python
Expertise with Kafka, the Hadoop Ecosystem (Hadoop, Kafka, Spark)
Comfortable with cloud-based services & warehousing (AWS S3, EMR, Snowflake) and with containers (Docker, Kubernetes, ECS, EKS)
Demonstrated ability to design, build and maintain complex data products


You'll get…

Competitive Salary & 401k company match that vests immediately upon participation
Generous parental leave of 16 weeks & PTO policy
$0 monthly premium and other flexible health plans effective first day of employment
75% discount on your subscription to HelloFresh (as well as other product initiatives)
Snacks, cold brew on tap & monthly catered lunches
Company-sponsored outings & Employee Resource Groups
Collaborative, dynamic work environment within a fast-paced, mission-driven company


About HelloFresh

We believe that sharing a meal brings people of all identities, backgrounds, and cultures together. We are committed to celebrating all dimensions of diversity in the workplace equally and ensuring that everyone feels a sense of inclusion and belonging. We also aim to extend this commitment to the partners we work with and the communities we serve. We are constantly listening, learning, and evolving to deliver on these principles. We are proud of our collaborative culture. Our diverse employee population enables us to connect with our customers and turn their feedback into meaningful action - from developing new recipes to constantly improving our process of getting dinner to our customers' homes. Our culture attracts top talent with shared values and forms the foundation for a great place to work!

At HelloFresh, we embrace diversity and inclusion. We are an equal opportunity employer and do not discriminate on the basis of an individual's race, national origin, color, gender, gender identity, gender expression, sexual orientation, religion, age, disability, marital status or any other protected characteristic under applicable law, whether actual or perceived. As part of the Company's commitment to equal employment opportunity, we provide reasonable accommodations, up to the point of undue hardship, to candidates at any stage, including to individuals with disabilities."
Simon Data,Staff Data Engineer,"New York City Metropolitan Area
Hybrid","Posted by

Bianca Kaczor

Sr. Manager - Talent Acquisition - We're Hiring! at Simon Data

Send InMail

About Us

Simon Data was founded in 2015 by a team of successful serial entrepreneurs. We are an enterprise customer data platform that empowers marketers to create personalized data-driven experiences for the customers. We’re scrappy problem solvers who believe in tackling big challenges with disruptive thinking and giving our customers the support they need to deliver great next-generation experiences at scale.

Simon Data is a data-first customer experience orchestration platform, designed to disrupt the marketing technology and marketing cloud category. Simon’s platform empowers businesses to use enterprise-scale data and machine learning to power customer communications across every channel. Our unique approach allows brands to develop one-to-one relationships with their customers without building a bespoke in-house data infrastructure.

At Simon, we firmly believe that business success starts and ends with people. We all do our best work when we are surrounded by other friendly top performers who want to succeed together. This attitude is core to our values. When you trust your team, invest in their development, and give them ownership, great things happen.




The Role

We are looking for a Staff Data Engineer to help mentor and guide the Data Layer team, which is the backbone of our platform and critical to our clients’ success. You will play a pivotal role in our ability to sustainably and rapidly move the data that powers our platform to engage with hundreds of millions of customers -- sending billions of messages annually.

As a mentor, you will have a tremendous impact on the future of motivated engineers who are eager to increase their skills by leading training and presentations. This is an incredibly satisfying role as you guide early career professionals and witness their growth and success!

As a Staff Data Engineer at Simon, you’ll dive into a system of multiple streaming and batch pipelines, balancing delivery alongside system stability and scale. You’ll use your readiness to seek complete solutions with a passion for building fault-tolerant and highly available systems.




What You'll Do

Build, scale, and own data pipelines using batch and streaming tools like Python, Spark, Kinesis, Redshift, Snowflake, and Elasticsearch or using new technologies to achieve the desired outcome
Come up to speed on our existing ETL processes and create new pipelines for large datasets and features
Mentor other smart, passionate developers, fostering best practices around data processing, testing, and application/system design
Find ways to spread knowledge across the organization, leveling up engineering practices and mentoring other engineers.
Architect and develop new data products for clients like new reporting capabilities or data transformation tooling
Construct the platform and tools for our clients to self serve their data engineering needs on the Simon system
Profile and debug AWS services to define performant usage patterns
Partner daily with a group of your peers, all of whom are passionate about quality, staying ahead of the curve, and continuously improve




Qualifications

Several years of demonstrated ability crafting, deploying and owning several substantive data engineering or analysis projects with company-wide impact
Ability and desire to work with early-career engineers, offering mentoring and training in software development languages, best practices, and methodology
Ability to work with various functional owners in your company (spanning product management, program management, as well as Dev/Tech Ops)
Proficient with Python or other high level languages




What We Offer

100% coverage of medical premiums for employee AND family
Flexible PTO
Monthly Mental Health Days
Remote Work Stipend
Generous Maternity and Paternity Leave
Quarterly Wellness reimbursement
$100 per calendar year to spend towards any of our clients
Hybrid Work Environment




Visa sponsorship for this role is currently not available.

Diversity

We’re proud to be an equal opportunity employer open to all qualified applicants regardless of race, color, ancestry, religion, sex, national origin, sexual orientation, age, citizenship, marital status, disability, gender identity or expression, Veteran status, or any other legally protected status.

We will ensure that individuals with disabilities are provided reasonable accommodation to participate in the job application or interview process, to perform essential job functions, and to receive other benefits and privileges of employment. Please contact us to request accommodation.




COVID-19 vaccination is required for all employees whose position is based out of the NYC office. However, Simon will consider accommodations for disability- and religious-based reasons on a case by case basis."
"BioIntelliSense, Inc",Data Engineer,"Redwood City, CA
Remote","Description:
BioIntelliSense is ushering in a new era of continuous health monitoring and clinical intelligence for Remote Patient Monitoring (RPM) and Screening (COVID-19/Infection). We are seeing hyper growth and expanding our Data Science team! We are looking for experienced engineers who are excited about joining a fast-paced company and building something great.
Our medical-grade Data-as-a-Service (DaaS) platform seamlessly captures multi-parameter vital signs, physiological biometrics and symptomatic events through an effortless patient experience. The FDA-cleared BioSticker and BioButton devices make remote monitoring and early detection simple. Through the platform's advanced analytics, clinicians, patients, and employers will now have access to high-resolution patient trending and reporting to enable medical grade care at home, and medical grade screening in the office, school, or residence. For more information on how BioIntelliSense is redefining remote patient monitoring through medical-grade and cost-effective data services, please visit our website at BioIntelliSense.com.
Our headquarters is located at the foothills of the Rocky Mountains near historic Golden, Colorado with an additional office located in Northern California (Redwood City).
As the Data Engineer you will...
Join a team developing an enterprise data lake that supports data science, analytics, and business intelligence activities across the company. We are looking for someone to help build ELT/ETL processes, data warehousing models, and metadata management systems. The ideal candidate will have some experience with working on data platforms on the cloud; experience with internet-of-things (IoT) and healthcare data is a bonus but not a requirement. The candidate will join a global team of 5-10 high-performing engineers building a data platform to support an expanding set of internal users and a growing volume and variety of data.

This is a unique opportunity to apply your experience, roll-up-your-sleeves and significantly contribute to the rapid development, growth and expansion of BioIntelliSense in delivering clinical intelligence globally.
Responsibilities...
- Set up and manage automated data pipelines to capture IoT data, system operations data, and third-party data
- Develop cloud-based data processing pipelines using Spark, SQL, and Python
- Develop metadata management processes to enable scalable data research
- Collaborate with other data engineers and data scientists to prioritize and develop data products that serve the needs of the company
You're excited about this opportunity because you will
Join a fast-growing company and grow right along with us.
Work on challenging and interesting tech problems which reshapes the future of healthcare.
Get the chance to work on cutting-edge technologies and use world-class tooling to get the job done.
Make a large impact across the company through business deliverables and continuous innovation.
Opportunity to build solutions and organizations from 0 to 1.

.Requirements:
Minimum Experience, Skills, and Qualifications:
- BS in computer science, mathematics, engineering, information systems, or related field
- 1+ year of work experience as a data engineer or software engineer
- Experience deploying automated cloud data pipelines
- Experience with data lake or data warehouse development and management including ETL/ELT pipelines
- Strong coding skills in SQL, Spark, and preferably Python
- Familiarity with quality engineering practices including software testing and validation
- Familiarity with cloud-based data platforms and technologies such as Databricks, Apache
PI167148440"
Dice,"Python Data Engineer with Google Cloud Platform @ San Jose, California (100% Remote)","United States
Remote","Dice is the leading career destination for tech experts at every stage of their careers. Our client, HL Solutions LLC, is seeking the following. Apply via Dice today!

Job Title Python Data Engineer with Google Cloud Platform Work Location San Jose, California (100 Remote) Duration Long Term Job Description 5+ years of hands-on experience working with cloud technologies ndash Google Cloud Platform knowledge strongly preferred Required Skills are - Python, Google Cloud Platform, Data Engineer, , Data Warehousing, DB2, ETL, Go, JAVA, JSON, Kafka, NoSQL, Oracle, Scala, Teradata, XML Familiarity with technologieslanguages such as R, Scala, Java, Hive, Spark, Kafka Bachelorrsquos degree or masterrsquos degree Minimum of 8+ years of experience with Python Programming Experience in building and optimizing data pipelines, architectures and data sets. Experience in processing and extracting value from large, disconnected datasets Build the infrastructure required for optimal extraction, transformation and loading of data from a wide variety of data sources. Minimum of 5 years of experience with systems data architecture modeling tools and techniques, using standard architecture frameworks Role demands to be handrsquos on with technical exposure"
"TrueCar, Inc.",Data Engineer 3 - Remote,"San Diego, CA
Remote","Job Description:

TrueCar is on a mission to revolutionize the way that consumers engage in the vehicle purchase and ownership experience. We’re building an end-to-end consumer journey that’s uplifting, empowering, and unrivaled in the marketplace, and we’re looking for the best and brightest to help us achieve our goals. We’re on the hunt for teammates who embrace challenge, relentlessly innovate, and reject the notion that ‘it can’t be done.’

TrueCar maintains a Dynamic Workplace, allowing employees to have their primary workstations at home, with office space in Santa Monica, CA and Austin, TX to be made available to individuals and teams to use as needed. Employees enjoy excellent benefits (100% employer-paid health/vision/dental premium, 401k with contribution matching, equity for eligible roles, etc.) as well as perks like monthly credits for at-home food delivery, internet/mobile phone service coverage, fitness expenses, and Caregiver support. In short, we care deeply about our teammates and build employee-centric programs that prove it.

About the Job:

The Data Engineering team applies subject matter expertise to ingest, analyze, and validate the automotive data required from internal and 3rd party sources. Data engineers are responsible for building and maintaining highly scalable data pipelines to power the website while also providing data for our analytical engine to derive insights in a meaningful fashion.

What you'll Do:

Design and develop efficient and scalable data processing pipelines using big data technologies ( Hadoop, Spark, HBase, Kinesis, MapReduce, etc.) on large scale structured/unstructured data sets for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL/NoSQL.
Build complex workflows and orchestrate data dependencies.
Monitor and support data pipelines to honor internal and external SLA’s.
Work within standard engineering practices (i.e. SCRUM, unit/integration testing, design review, code reviews, continuous integration, etc.) to deliver product features with optimal efficiency for TrueCar customers and clients.
Closely work with product owners & analysts to understand business and functional requirements and contribute to the design and prioritization discussions.
Working with a team of engineers where mentorship is valued.
Ability to learn and adapt to continually evolving technologies in the big data ecosystem.


What you'll Need:

Bachelor degree (or Master) in Computer Science or related engineering field
3-5 years of experience programming in Java (must have)
2+ years of experience in Big Data technologies
Experience in any of big data technologies: MapReduce, Spark, HBase
Proficient in SQL and experience with RDBMS/NoSQL databases.
Experience working with Cloudera/Hortonworks/EMR distribution in AWS
Ability to self-manage tasks and be proactive in working with other teams to accomplish them while taking pride and ownership in their work
Team-player with strong collaboration and communication skills, who is able to respond positively to feedback

*** While this position is open to remote work through TrueCar's Dynamic Workplace initiative, applicants may not reside in Colorado. Colorado candidates will be required to relocate. ***


Location(s):Santa Monica, CA

"
SingleStore,Data Engineer,"San Francisco, CA","Position Overview

At SingleStore we're not just building a database company, we are defining the future of data management. As a Software Engineer on the Infrastructure Team, you will focus on continuing to build out and improve upon our product testing platform used by all software engineering teams at SingleStore, as well as being at the forefront of developing applications that will take our managed service (database as a service) product to the next level. Your expertise in building out cloud applications will drive technical decisions critical to both team and company success.

Role And Responsibilities

Build observable, high-performance applications that power our edge / hybrid cloud product, internal and external infrastructure, and our product test harness, ""Psyduck.""
Design and architect novel solutions to address cloud-scale problems, using bleeding edge container and scheduling technologies.
Automate workflows and test your own code to improve overall software quality
Manage project priorities, deadlines, and deliverables
Perform insightful code reviews for your team members
Guide broader product teams on cloud-scale practices (observability, security, performance and scalability.)

Required Skills And Experience

Experience with one or more general purpose programming languages including but not limited to: C/C++, Python, or Go.
Experience with containers with an emphasis on the Kubernetes ecosystem.
Deep interest and experience working on distributed systems, databases, networking, storage, and multi-tenant services, and Unix/Linux environments.
B.S. degree in Computer Science or a similar field

SingleStore is one platform for all data, built so you can engage with insight in every moment. Trusted by industry leaders, SingleStore enables enterprises to adapt to change as it happens, embrace diverse data with ease, and accelerate the pace of innovation. SingleStore is venture-backed and headquartered in San Francisco with offices in Portland, Seattle, Boston, Bangalore, London, Lisbon, and Kyiv. Defining the future starts with The Database of Now™.

Consistent with our commitment to diversity & inclusion, we value individuals with the ability to work on diverse teams and with a diverse range of people.

To all recruitment agencies: SingleStore does not accept agency resumes. Please do not forward resumes to SingleStore employees. SingleStore is not responsible for any fees related to unsolicited resumes and will not pay fees to any third-party agency or company that does not have a signed agreement with the Company."
Visiting Nurse Service of New York,Data Engineer,"New York City Metropolitan Area
Remote","Posted by

Christina B.

Talent Acquisition Specialist at Visiting Nurse Service of New York

Send InMail

Summary

Ensures the expansion and optimization VNSNY data and data pipeline architecture, as well as the optimization of data flow and collection for cross functional teams. Participates on data initiatives and ensures optimal data delivery architecture is consistent throughout ongoing projects. Architects and designs processes and code frameworks that enable the optimization and re-engineering of VNSNY’s data architecture systems to support the next generation of products and data initiatives. Works under general supervision.




Description

Creates and maintains optimal data pipeline architecture integrating large, complex data sets that meet functional and non-functional business requirements.
Identifies, designs, and implements internal process improvements such as automating manual processes, optimizing data delivery, and re-designing infrastructure for greater scalability.
Builds the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL and AWS cloud native technologies.
Builds analytics tools that utilize the data pipeline to provide actionable insights into customer acquisition, operational efficiency and other key business performance metrics.
Works with project stakeholders to assist with data-related technical issues and supports data infrastructure needs.
Works with data and analytics team to strive for greater functionality in our data systems.
Participates in special projects and performs other duties, as required.




Education:

Bachelor’s degree in Computer Science, Statistics, Informatics, Information Systems or a related field required.




Experience:

Minimum of five years of experience working on advanced SQL knowledge, relational databases, and query authoring (SQL) required.
Experience building and optimizing big data pipelines, architectures and data sets required.
Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement required.
Strong analytic skills related to working with structured/unstructured datasets required. Experience building processes supporting data transformation, data structures, metadata, dependency and workload management required.
Working knowledge of message queuing, stream processing, and highly scalable data stores required.
Experience with relational SQL and NoSQL databases, such as Oracle, SQL Server, MySQL, Postgres, MongoDB and Snowflake required.
Experience with data pipeline and workflow management tools such as AWS Glue, and Airflow required.
Experience with AWS cloud services such as EC2, RDS, S3 & Athena, Redshift, and Dynamo required.
Experience with stream-processing systems including PySpark, Storm, and Spark-Streaming required.
Experience with object-oriented/object function scripting languages such as Python, Java, C++, and Scala required."
Robert Half,Data Engineer,"New York, NY
On-site","Description

Automated Databases/Data Engineering has been built in house, need a Data Engineer to support the team on a consulting basis. Building pipelines to data sources, using python, SQL server, and AWS.

Build/maintain troubleshoot SSIS processes, build new API’s. Client request for custom data deliveries has increased.

Requirements

Python, SQL, API Development

Technology Doesn't Change the World, People Do.®

Robert Half is the world’s first and largest specialized talent solutions firm that connects highly qualified job seekers to opportunities at great companies. We offer contract, temporary and permanent placement solutions for finance and accounting, technology, marketing and creative, legal, and administrative and customer support roles.

Robert Half puts you in the best position to succeed by advocating on your behalf and promoting you to employers. We provide access to top jobs, competitive compensation and benefits, and free online training. Stay on top of every opportunity – even on the go. Download the Robert Half app and get 1-tap apply, instant notifications for AI-matched jobs, and more.

Questions? Call your local office at 1.888.490.4429. All applicants applying for U.S. job openings must be legally authorized to work in the United States. Benefits are available to contract/temporary professionals. Visit

© 2021 Robert Half. An Equal Opportunity Employer. M/F/Disability/Veterans. By clicking “Apply Now,” you’re agreeing to"
FaunaBio,Data Engineer,"San Francisco Bay Area
Hybrid","Posted by

Linda Goodman

Co-founder and Chief Technology Officer at FaunaBio

Send InMail

Founded in 2018, Fauna Bio is an emerging biotechnology company pioneering a novel approach to drug discovery by unlocking the secrets of animal genomics to improve human health. Fauna Bio has raised more than 12MM in seed financing and is focused on identifying genes in animals that are responsible for disease protection and that have strong connections to related human genes. The company leverages insights from extraordinary mammalian traits to develop novel therapeutics that can treat and prevent diseases in humans. Fauna Bio is particularly inspired by the extraordinary physiology of hibernation. Hibernating mammals have evolved to develop mechanisms of self-healing and tissue repair that have implications for human health problems such as heart disease, stroke, Alzheimer’s disease, diabetes, obesity, osteoporosis, and muscle loss. This makes them an excellent model system for identifying drug targets because they can transiently protect themselves from problems similar to critically important human diseases, allowing Fauna Bio to identify key protective genes that are turned on or off at medically-relevant times. These insights also translate well to human biology, as hibernating rodents share 92% of genes with humans.




In addition to creating therapeutics to treat human diseases, Fauna Bio is also pursuing gene targets to induce hibernation for long-range space travel. While the program is still in an early stage, our goal is to allow humans to hibernate safely through long journeys.







Opportunity




A key part of Fauna Bio’s strategy is to have a world-class set of rich ‘omics data from emerging model organisms that represent novel therapeutic strategies for multiple diseases with high unmet need. We have built several methods for identifying drug targets including gene network analysis, scoring-based ranking, and a knowledge graph to link diverse data sources. The data integrated in these approaches includes Fauna’s proprietary hibernation data, decades of physiologic studies on hibernators, animal models of disease, and numerous public repositories of human and animal data.




This position will play a critical role in our discovery stage programs by building and maintaining these data resources.




Key Responsibilities:




ETL pipeline management of diverse biological data, with endpoints of both long-term storage (AWS S3) and further computational analysis from SQL/noSQL databases
Optimize storage of Fauna’s genomic and transcriptomic data on AWS S3
Development of effective data validation/verification processes in collaboration with the scientific team







We’d love to hear from you if you:

Have a master’s degree in computational field, data management, OR a bachelor’s degree in one of the above areas with 3+ years of relevant work experience. A minimum of 2 years working with multimodal data repositories is preferred.
Have working knowledge of Amazon Web Services (AWS)
Are proficient in a scripting language such as Python or R.
Are proficient in relevant libraries such as Pandas, Numpy and data.table etc.
Have a good understanding and hands-on experience with databases, specifically PostgreSQL and MongoDB
Have familiarity with genomic data and standard file formats
Are highly organized and have great attention to detail


"
Amazon Web Services (AWS),"Data Engineer, AWS Marketplace","Seattle, WA","Job Summary

DESCRIPTION

AWS Marketplace is changing the way AWS customers discover, deploy, and pay for software. Any AWS customer can easily find software on the AWS Marketplace website, deploy it immediately, and be metered for usage that appears directly on their AWS bill. Buyers can quickly buy the software they need and sellers have access to millions of customers of AWS.

As a Data Engineer on the AWS Marketplace team you will work directly with Software Engineering, Business Intelligence, Data Science and Product teams to continuously improve our of data infrastructure, design, tools and pipelines. Your work will directly influence and drive organizational insights, customer facing features and machine learning models.

To be successful in this role, you should have strong database design skills, comfort with large data sets and an eagerness to invent. You should have a passion for data and analytics with the technical skills needed to build for scale and automation.

A day in the life

Collaborate with Software Engineers, Product Managers, Data Scientists and Business Intelligence Engineers to design, plan and deliver on high priority data initiatives serving internal stakeholders and AWS customers.
Build automated data processing solutions leveraging Spark, EMR, Python, Redshift and S3.
Look around corners and be creative - Continuously evaluate and improve our strategy, architecture, tooling and codebase to maximize performance, scalability and availability.

About The Hiring Group

Inclusive Team Culture

Here at AWS, we embrace our differences. We are committed to furthering our culture of inclusion. We have ten employee-led affinity groups, reaching 40,000 employees in over 190 chapters globally. We have innovative benefit offerings, and host annual and ongoing learning experiences, including our Conversations on Race and Ethnicity (CORE) and AmazeCon (gender diversity) conferences. Amazon’s culture of inclusion is reinforced within our 16 Leadership Principles, which remind team members to seek diverse perspectives, learn and be curious, and earn trust.

Work/Life Balance

Our team puts a high value on work-life balance. It isn’t about how many hours you spend at home or at work; it’s about the flow you establish that brings energy to both parts of your life. We believe striking the right balance between your personal and professional life is critical to life-long happiness and fulfillment. We offer flexibility in working hours and encourage you to find your own balance between your work and personal lives.

Mentorship & Career Growth

Our team is dedicated to supporting new members. We have a broad mix of experience levels and tenures, and we’re building an environment that celebrates knowledge sharing and mentorship. We care about your career growth and strive to assign projects based on what will help each team member develop into a better-rounded professional and enable them to take on more complex tasks in the future.

Job responsibilities


Basic Qualifications

Bachelor’s degree in Computer Science or a related field
5+ years of experience in Data Engineering / Data Warehousing
5+ years experience with SQL
2+ years experience with Big Data Technologies (Spark, EMR, Spectrum, Glue etc.)
2+ years of experience with Python, Java or similar programming languages.

Preferred Qualifications

Experience with Redshift, S3, Glue, Spectrum, EMR, DynamoDB, Kinesis.
Experience building TB-PB scalable data solutions
Experience in building large scale distributed data processing pipelines

Amazon is committed to a diverse and inclusive workplace. Amazon is an equal opportunity employer and does not discriminate on the basis of race, national origin, gender, gender identity, sexual orientation, protected veteran status, disability, age, or other legally protected status. For individuals with disabilities who would like to request an accommodation, please visit https://www.amazon.jobs/en/disability/us.


Company - Amazon Dev Center U.S., Inc.

Job ID: A1944401"
Scribd,Data Visualization Engineer - Explainable ML,"Dover, DE
Remote","At Scribd (pronounced “scribbed”), we believe reading is more important than ever. Join our cast of characters as we work to change the way the world reads by building the world’s largest and most fascinating digital library: giving subscribers access to a growing collection of ebooks, audiobooks, magazines, documents, Scribd Originals, and more. In addition to works from major publishers and top authors, our community includes over 1.5M subscribers in nearly every country worldwide.

We’re defining the workforce of the future with Scribd Flex, a program that embraces multiple perspectives while leaning into our belief that no matter where each team member is, we trust them to accomplish our shared business goals. This program lets employees, in partnership with their manager, choose where they work while creating intentional in-person meetings with co-workers that build culture and connect us personally.

Remote employees must have their primary residence in: Arizona, California, Colorado, Delaware, DC, Florida, Hawaii, Iowa, Massachusetts, Michigan, Missouri, Nevada, New Jersey, New York, Ohio, Oregon, Tennessee, Texas, Utah, Vermont, Washington, Ontario (Canada), and British Columbia (Canada).

This list may not be complete or accurate, and candidates should speak with their recruiter about their specific location for remote work.

The Explainable ML team is part of the Machine Learning & Discovery organization, which creates AI/ML powered experiences that help our subscribers discover content they love.

About The Team

We design, develop and experiment with new kinds of data visualizations that power the “unblackboxing” of our ML models.
We collaborate with Data Scientists and Machine Learning Engineers to improve model explainability and discover novel views into our data.
We build custom visual tools that give Product Managers the insights they need to make sound, data-driven decisions and deliver world-class ML-based user experiences.

About You

You have a technical background in frontend web development with a specialty in data visualization. You have a knack for creative communication and visual storytelling, a facility with exploratory data analysis, a solid design sense, and a user-focused mindset. You are a great team player, and you thrive in an environment of fast paced development and collaboration.

What You'll Do

Work through all phases of the data visualization lifecycle, from exploratory data analysis and hypothesis generation through solution design and implementation.
Develop and deliver highly interactive analytical dashboards and data visualizations that generate more than just the expected insights.

You Have

1-2 years of experience building web-based interactive analytical dashboards and visualizations.
Demonstrated technical skills in web development (esp. React.js, Node.js, Rails) and data visualization libraries (e.g. D3.js, Semiotic, Recharts, React-viz)
An intuitive sense for effective visual communication combined with the technical acumen to bring your ideas to life.
A knack for creative problem solving and a willingness to learn new technologies and skills.
Familiarity with the fundamentals of machine learning and its applications a plus.

Benefits, Perks, And Wellbeing At Scribd

Healthcare Insurance Coverage: Scribd pays for employee’s Medical, Vision, and Dental premiums and a portion of dependent premiums
401k/RSP plans provided, plus company matching with no vesting period
Professional development: generous annual budget for our employees to attend conferences, classes, and other events
Quarterly Wellness, Connectivity & Comfort Benefit
Concern mental health digital platform
Free subscription to Scribd + gift memberships for friends & family
Leaves: 12 weeks paid parental leave, company paid short-term/long-term disability plans and milestone Sabbaticals
Generous Paid Time Off: Paid Holidays, Flexible Sick Time, Volunteer Day + office closure between Christmas Eve and New Years Day
Company-wide Diversity, Equity, & Inclusion programs which include learning & development opportunities, employee resource groups, and hiring best practices.

Want to learn more? Check out our office and meet some of the team at www.linkedin.com/company/scribd/life

Scribd is committed to equal employment opportunity regardless of race, color, religion, national origin, gender, sexual orientation, age, marital status, veteran status, disability status, or any other characteristic protected by law.

We encourage people of all backgrounds to apply. We believe that a diversity of perspectives and experiences create a foundation for the best ideas. Come join us in building something meaningful.

"
Dice,Data Engineer,"South Gate, CA
On-site","Dice is the leading career destination for tech experts at every stage of their careers. Our client, Stability Technology Partners, is seeking the following. Apply via Dice today!

Essential Duties and Responsibilities Coordinate with Business Analysts and internal customers to develop business requirements and specifications documents. Develop standard reports and functional dashboards based on business requirements. Maintains business intelligence models to design, develop and generate both standard and ad-hoc reports. Create technical documents that documents SQL Server, Oracle and DB2 or other database contents, concepts, and mapping between databases. Participate in the design, development and analysis of data architecture and warehousing approaches. Create business intelligence tools and reports, such as physical data models and dimensional analyses. Design, code, test, and aggregate results from SQL queries to provide information to users. Identify and resolve data reporting issues in a timely fashion Build Data Modeling BI and Data Warehousing solutions Create and support database schemas that offer the highest standard of performance and availability. Assist in implementing effective business analytics practices that drive improved decision making, efficiency and performance. Well versed in writing SQL Queries, fine tuning and troubleshooting Translate business needs to technical specifications Develop and update technical documentation Education Required BABS in Computer Science or related field is preferable. Equivalent combination of experience, education and training will be considered. Proven experience as a BI Developer Industry experience is preferred Background in data warehouse design (e.g. dimensional modeling) and data mining In-depth understanding of database management systems, online analytical processing (OLAP) and ETL (Extract, transform, load) framework Familiarity with BI technologies (e.g. Microsoft Power BI, Oracle BI) Knowledge of SQL queries, SQL Server Reporting Services (SSRS) and SQL Server Integration Services (SSIS) Proven abilities to take initiative and be innovative Analytical mind with a problem-solving aptitude BI or SQL related certifications"
Deloitte,Sr. Data Engineer,"Arlington, Virginia, United States","In this age of disruption, organizations need to navigate the future with confidence by tapping into the power of data analytics, robotics, and cognitive technologies such as Artificial Intelligence (AI). Our Strategy & Analytics portfolio helps clients leverage rigorous analytical capabilities and a pragmatic mindset to solve the most complex of problems. By joining our team, you will play a key role in helping to our clients uncover hidden relationships from vast troves of data and transforming the Government and Public Services marketplace.

Work you'll do

The Data Engineer will have overall responsibility of planning how work within different teams will integrate into one solution. The Data Engineer will also have overall responsibility of being the primary representative on all architecture matters and the leading member of the Architecture Team. Responsibilities include mentoring and directing junior developers related to technical tasks and agile project development practices, and ensuring that all development is aligned with the program's platform and is not duplicative or designed in a way that it is not reusable.

The team

Deloitte's Government and Public Services (GPS) practice - our people, ideas, technology and outcomes-is designed for impact. Serving federal, state, & local government clients as well as public higher education institutions, our team of over 15,000+ professionals brings fresh perspective to help clients anticipate disruption, reimagine the possible, and fulfill their mission promise.

The GPS Analytics and Cognitive (A&C) offering is responsible for developing advanced analytics products and applying data visualization and statistical programming tools to enterprise data in order to advance and enable the key mission outcomes for our clients. Our team supports all phases of analytic work product development, from the identification of key business questions through data collection and ETL, and from performing analyses and using a wide range of statistical, machine learning, and applied mathematical techniques to delivery insights to decision-makers. Our practitioners give special attention to the interplay between data and the business processes that produce it and the decision-makers that consume insights.

Qualifications

Required:

8+ years of professional experience developing and testing software.
5+ years of ETL and Data Engineering experience
2+ years of experience with modern ETL tools such as Informatica, Snowflake, Talend, Databricks
2+ years of experience developing applications leveraging big data technologies including at least one of the following:
Elastic Search
Cassandra
Janus Graph
Must be legally authorized to work in the United States without the need for employer sponsorship, now or at any time in the future

Preferred:

Possess the ability to demonstrate current and at least advanced skills in front-end and middle-tier.
Possess the ability to demonstrate current and at least intermediate-advanced skills in back-end development.
Demonstrate a strong knowledge of concepts, methodologies and best practices - especially as they pertain to mitigating development risks, estimating tasks, coding standards and source control procedures.

How you'll grow

At Deloitte, our professional development plan focuses on helping people at every level of their career to identify and use their strengths to do their best work every day. From entry-level employees to senior leaders, we believe there's always room to learn. We offer opportunities to help sharpen skills in addition to hands-on experience in the global, fast-changing business world. From on-the-job learning experiences to formal development programs, our professionals have a variety of opportunities to continue to grow throughout their career."
Park Place Technologies,Data Engineer II,"Florida, United States
Remote","ALL APPLICANTS MUST CURRENTLY RESIDE IN THE U.S.

No Sponsorship is available for this position.




Who We Are:

As the global leader in third party maintenance, our 2500 Park Place Associates provide support to 21,000+ customers in more than 154+ countries. We are proud to service 90% of Fortune 500 companies and 40% of Forbes 100 clients.




Our company’s strength and success are a credit to our Associates, and Park Place Life is how we communicate and deliver our culture internally. We have been awarded as a NorthCoast 99 “Best Workplace” winner for 10 consecutive years in recognition of our employee commitment. Park Place Life is about collaboration, responsiveness, diversity, and integrity, and represents everything that makes our company great and our culture unique.




Top Rated Benefits We Offer:

We cover 100% of your Healthcare benefits!
Profit Sharing!
401K matching contributions and earnings are always 100% vested.
Plus much more!!!




Position Overview:

The Data Engineer II will help create reporting and analysis tools using SSIS, Azure Data Factory, T-SQL, SSAS, Power BI and other business intelligence applications to support our reporting platform. This position will be supporting various stages of the end-to-end data integration and report development lifecycle, with specific responsibilities related to the requirements and architecture of our reference system.




What you’ll be doing:

Implement T-SQL code (procedures and views) and tabular models in line with established plans and architectural standards for the department.
Implement, enhance, triage existing stored procedures and SSIS packages employed in the overnight or real-time integration with source systems.
Implement Azure Data Factory pipelines to support data sourcing, transformations and load into Azure Synapse and other data repositories
Implement, enhance SSAS multi-dimensional and tabular cubes.
Act as a liaison with architects, business analysts, developers and testing teams for content input and document reviews to ensure that report functionality is well documented.
Interpret requirements provided by business analysts and occasionally directly from end users
Pro-actively understand changes to project scope and make relevant changes to report design and stored procedures.
Understand relational databases and data analysis, data models, data extracts, and reporting in context.
Assist Quality Assurance team with validating reports.
Escalating issues in a timely manner and suggesting improvements.
Providing peer review support to work produced by others, confirming use of relevant coding standards; approving work produced by others.
Demonstrate understanding of the Software Development Life Cycle.




What we’re looking for:

Advanced experience with Microsoft SQL Server and T-SQL code development.
Experience in Power BI or comparable reporting tools.
Experience in ETL tools (SSIS or Azure Data Factory)
Solid understanding of database development best practices.
Knowledge of, or experience with, predictive analytics.




A Bonus If You Have:

Experience with SQL Server 2016 Enterprise, Azure Synapse, Python/R, Databricks, or C#
Web development experience is a plus.




Education:

Bachelor’s degree




Travel:

<10%

If you are a qualified individual with a disability or a disabled veteran, you may request a reasonable accommodation if you are unable or limited in your ability to access job openings or apply for a job on this site as a result of your disability. You can request reasonable accommodations by calling 1-877-778-8707.




Park Place Technologies is an Equal Opportunity Employer M/F/D/V.




Park Place Technologies has an in-house recruiting team that focuses exclusively on the hiring needs of our company. We are not currently accepting additional third-party agreements or unsolicited resumes. If you would like to be considered as a preferred partner with Park Place Technologies, please submit your detailed information to careers@parkplacetech.com. Any CVs submitted directly to hiring managers will be considered unsolicited and become the property of Park Place Technologies."
Jellyfish,Data Science Engineer,"Boston, MA","At Jellyfish, we build software that helps engineering teams measure their development efforts and improve strategic decision-making.

As a member of Jellyfish Research, you will work to build innovative new products and features that leverage our platform’s unique data about how engineers plan, develop, collaborate on, and deploy software. You will have the opportunity to create systems and services that produce rich insights, which impact how our customers understand and guide their own development teams and the software they build.

Most importantly, you will be part of a highly talented team working in a fun, challenging, and collaborative environment, building a fast-growing business and company led by experienced entrepreneurs and backed by top-tier investment partners.

As Data Science Engineer for Jellyfish Research, you will:

Collaborate with other members of the team to develop the core algorithms and inference engines that power the Jellyfish platform.
Help members of the team transition ideas and code from experiments and proofs-of-concept to real prototype features in our product.
Make improvements to how we validate, test, and deploy data science algorithms and models in production.


About you:

You are a strong programmer.
You have good fundamental skills in relevant areas of data science (e.g. statistics, machine learning, natural language processing).
You have experience with and are interested in prototyping, building, testing, and deploying advanced algorithms and technology for production applications.
You enjoy working with real-world data and are excited about testing your ideas and creations in partnership with real customers.
You love learning new things and teaching others what you know.
You have strong communication skills and enjoy working as part of a cross-functional team.


Bonus points if you have experience with:

Standard Python libraries for data science (e.g. pandas, scikit-learn, TensorFlow).
Our engineering technology stack (Python 3, Django, Postgres, Docker, AWS).


We believe that it takes a diverse team to build the best company we can. Jellyfish welcomes people from all backgrounds and especially encourages applications from members of groups underrepresented in the software industry."
Roblox,Lead Analytical Data Engineer,"San Mateo, CA
On-site","Every day, tens of millions of people from around the world come to Roblox to play, learn, work, and socialize in immersive digital experiences created by the community. Our vision is to build a platform that enables shared experiences among billions of users. This is what’s known as the metaverse: a persistent space where anyone can do just about anything they can imagine, from anywhere in the world and on any device. Join us and you’ll usher in a new category of human interaction while solving exceptional challenges that you won’t find anywhere else.

At Roblox, a deep understanding and measurement of users and creators' experience is critical to Roblox's rapid growth. The Analytical Data Engineering team is enabling Roblox's success through the development and maintenance of the Core Data model with an eye for scalability to support the analytical community and tooling to increase the speed at which we build data. As one of the founding members of the ADE team, you will define the data ontology for all of Roblox, define best practices and standards for the analytical community, define technical strategy for Roblox's ETL strategy including batch vs. streaming architecture, and influence event instrumentation.

As an Analytical Data Engineer you should be familiar with supporting Data Science and Machine Learning workflows, and should leverage that knowledge to inform your design decisions and implementations. Our team's product will act as the interface between data engineering and all other teams who will leverage the data to improve the Roblox platform and the experience of our users and creators alike.


A B.Sc. equivalent in CS or sufficient experience.
8+ years of professional experience working with scalable ETL pipelines on industry standard ETL orchestration tools (i.e. Airflow, Luigi, Prefect, Dagster, digdag.io, Google Cloud Composer, AWS Step Functions, Azure Data Factory, UC4, Control-M)
3+ years working in the Hadoop Data Ecosystem for data processing
2+ years leading data engineering development directly with business or data science stakeholders
Built, scaled, and maintained Multi-Terabyte data sets
Significant experience with at least one major cloud's suite of offerings (AWS, GCP, Azure) and willingness to learn another

Developed with Data Quality at the core of your pipelines (e.g. Great Expectations, Data Fold, etc.)
Developed or enhanced ETL orchestrations tools
Familiarity with Data Discovery tooling (e.g. Amundsen, Atlas)
Worked within standard GitOps workflow (branch and merge, PRs, CI / CD systems)
Familiarity with infrastructure configuration (IaC [e.g. Terraform], cluster parameter tuning, service parameter tuning)

Partner with science, product, and engineering to collect data requirements to define the Core Data Ontology for all of Roblox
Lead a growing team of Analytical Data Engineers to support Roblox's ever evolving data needs
Design an extensible and scalable data model to support the ever growing analytical community
Design, build, and maintain efficient and reliable data pipelines in batch and streaming to fuel the core data sets
Apply ETL Frameworks to scale and extend functionality of the frameworks.
Analyze the use cases for the data to determine appropriate SLAs
Analyze the incoming data and upstream pipelines to determine and minimize epistemological issues.
Determine appropriate relaxations to deterministic compute where appropriate and leverage probabilistic data structures (bloom filters, count min sketch)
Partner with the Data Platform Team to provide approximation algorithms (approximate nearest neighbor, etc.) for high use statistics of interest.
Determine optimal caching strategies and eviction policies to support cost effective analysis
Drive adoption of the Core Data tables and publicize new incoming datasets to ensure consistency across the organization

Industry-leading compensation package
Excellent medical, dental, and vision coverage
A rewarding 401k program
Flexible vacation policy
Roflex - Flexible and supportive work policy 
Roblox Admin badge for your avatar
At Roblox HQ: 
Free catered lunches
Onsite fitness center and fitness program credit
Annual CalTrain Go Pass"
Priceline,Data Engineer,"Dallas, TX","This role is eligible for our five day flex office work model.

Our Technology team is the backbone of our company: constantly creating, testing, learning and iterating to better meet the needs of our customers. If you thrive in a fast-paced, ideas-led environment, you’re in the right place.

Why This Job’s a Big Deal

Enjoy working with big data? We do too! In fact, data lies at the very core of our business! As a data engineer you will be working on data systems that serve and generate billions of events each day. With the most advanced big data technologies at your fingertips, you will leverage data to understand, meet, and anticipate the needs of our customers.

In This Role You Will Get To

Design and build Cloud enabled Data Analytics solutions using AWS services.
Analyze existing data ingestion process and implement efficient end to end data solutions to support business reporting needs.
Develop new capabilities and data products for Priceline Partner Services that will substantially improve the business position in the marketplace.
Expand your skill set and understanding across the entire development process from requirements through user acceptance testing.
Create new technical products to suit general and novel technical use cases.
Provide support to existing data products and processes as required.
Contribute to team best practices for all aspects of the Development Lifecycle

Who You Are

Bachelor’s degree in Computer Engineering/Computer Science or equivalent.
3+ years of work experience in Software Engineering and development
Experience in AWS data analytics platform and related services S3,AWS Glue, Redshift, Athena, Lambda etc.
Proficiency in building data pipelines to ingest unstructured data to support data lakes/data warehouses.
Good understanding of data modeling and data warehouse concepts.
Strong experience with data migration, cloud migration and ETL processes.
Knowledge with SQL and noSQL databases and applications like Redshift, MongoDB etc.
Familiarity with Data visualization tools like AWS Quicksight, Tableau or GoodData.
Illustrated history of living the values necessary to Priceline: Customer, Innovation, Team, Accountability and Trust.
The Right Results, the Right Way is not just a motto at Priceline; it’s a way of life. Unquestionable integrity and ethics is essential.

Who We Are

WE ARE PRICELINE.

Our success as one of the biggest players in online travel is all thanks to our incredible, dedicated team of talented employees. Priceliners are focused on being the best travel deal makers in the world, fueled by our passion to help everyone experience the moments that matter most in their lives. Whether it’s a dream vacation, your cousin’s graduation, or your best friend’s wedding - we make travel affordable and accessible to our customers.

Our culture is unique and inspiring (that’s what our employees tell us). We’re a grown-up, startup. We deliver the excitement of a new venture, without the struggles and chaos that can come with a business that hasn’t stabilized.

We’re on the cutting edge of innovative technologies. We keep the customer at the center of all that we do. Our ability to meet their needs relies on the strength of a workforce as diverse as the customers we serve. We bring together employees from all walks of life and we are proud to provide the kind of inclusive environment that stimulates innovation, creativity and collaboration.

Priceline is part of the Booking Holdings, Inc. (Nasdaq: BKNG) family of companies, a highly profitable global online travel company with a market capitalization of over $80 billion. Our sister companies include Booking.com, BookingGo, Agoda, Kayak and OpenTable.

If you want to be part of something truly special, check us out!

Flexible work at Priceline

When we return to the office we will be in a fully flex mode, meaning you decide when to come to the office when it makes sense for you to do so - no minimum number of days in the office. Until then we are all working from home and will return to our flex mode when it is safe to do so.

Diversity and Inclusion are a Big Deal!

To be the best travel dealmakers in the world, it’s important we have a workforce that reflects the diverse customers and communities we serve. We are committed to cultivating a culture where all employees have the freedom to bring their individual perspectives, life experiences, and values-driven passion to work.

Priceline is a proud equal opportunity employer. We embrace and celebrate the unique lenses through which our employees see the world. We’d love you to join us and add to our rich mix!

Applying for this position

We're excited that you are interested in a career with us. For all current employees, please use the internal portal to find jobs and apply.

External candidates are required to have an account before applying. When you click Apply, returning candidates can log in, or new candidates can quickly create an account to save/view applications.

"
Kforce Inc,Data Engineer,"Atlanta, GA","Responsibilities

Kforce has a client that is seeking a Data Engineer in Atlanta, GA. Responsibilities

Data Engineer will design, develop, and maintain reliable automated data solutions based on the identification, collection, and evaluation of business requirements; Including but not limited to data models, database objects, stored procedures, and views
Developing new and enhancing existing data processing (Data Ingest, Data Transformation, Data Store, Data Management, Data Quality) components
Support and troubleshoot the data environment (including periodically on call)
Document technical artifacts for developed solutions

Requirements

Advanced SQL queries, scripts, stored procedures, materialized views, and views
Focus on ELT to load data into database and perform transformations in database
Ability to use analytical SQL functions
Cloud Data Warehouse solutions experience (Snowflake, Azure DW, or Redshift); data modeling, analysis, programming
Experience with DevOps models utilizing a CI/CD tool
Work in hands-on Cloud environment in Azure Cloud Platform (ADLS, Blob)
Analyze data models
Advanced SQL queries, scripts, stored procedures, materialized views, and views
Good interpersonal skills; Comfort and competence in dealing with different teams within the organization; Requires an ability to interface with multiple constituent groups and build sustainable relationships
Versatile, creative temperament, ability to think out-of-the box while defining sound and practical solutions; Ability to master new skills
Proactive approach to problem solving with effective influencing skills
Familiar with Agile practices and methodologies
Snowflake experience a plus
Talend, Apache Airflow, Azure Data Factory, and BI tools like Tableau preferred
Kforce is an Equal Opportunity/Affirmative Action Employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, pregnancy, sexual orientation, gender identity, national origin, age, protected veteran status, or disability status.

Salary: $50 - $70 per hour"
S&P Global Ratings,"Lead I, Data Engineer","Centennial, CO","Posted by

Morgan Bergstrom 2nd

Corporate Recruiter

Send InMail
The Role: Lead I,Data Engineer

The Location: Centennial, CO (Open to hybrid options)

The Level: 10 (internal purposes)

The Team: The S&P Global Ratings Data Science team combines automation, machine learning, and data engineering to improve the delivery of Ratings data & content to our analysts and customers. The team sits within the organizations Operating Office and works in close partnership with all teams throughout the organization.

You will be a part of key transformation projects aiming to streamline and automate our data processes: sourcing, collection, tagging, linking, extraction, translation, and other manual processes.

In doing so, you will build relationships across the division to generate and unlock new and existing data for content delivery through technological advancements using intelligent automation, data science, and coding design principles.

Projects will span the entirety of the data and content value chain enhancing customer satisfaction, and driving business results and growth through the timely and accurate delivery of fundamentals, ratings and reference data.

The Impact: You will be a key member of the S&P Global Ratings Data Science team, playing a key role in developing intelligent automation solutions for our data and content generation processes. You will work with technology, business and operations teams in this role, driving and executing on automation improvement opportunities to expand our capabilities around making our data value chain processes more efficient.

Compensation/Benefits Information (US Applicants Only)

S&P Global states that the anticipated base salary range for this position is $68,300 to $141,800. Base salary ranges may vary by geographic location.

In addition to base compensation, this role is eligible for an annual incentive plan.

This role is eligible to receive additional S&P Global benefits. For more information on the benefits we provide to our employees, visit https://www.spgbenefitessentials.com/newhires.

Key Responsibilities

Execute and serve as lead and/or SME on cross-organizational and cross-divisional projects automating our data value chain processes through automation, and new technologies
Design solutions and develop automation capabilities, such sourcing, collection, ingestion, extraction, transformation, translation, linking, and tagging
Mentor junior team members on coding best practices
Support digitization of datasets and accelerate digitization of new data identified by the Content and Data Operations teams
Serve as a source of knowledge for the Data Science team for process improvement, automation (Python, Alteryx, SQL, etc), and new technologies available to enable best-in-class timeliness and data coverage
Develop skills for ‘listening to learn’ with the express goal of understanding processes and associated problems which enable efficient, automated solutions
Review business processes and rules to understand potential repeatable processes and engage affected teams to consult on automation solutions
Be a strong partner with all the teams and stakeholders your projects and work involve
Monitor market trends in the data science/automation space identifying and onboarding new technologies and solutions to address business needs
Demonstrate innovation, customer focus, and experimentation mindsets

Basic Qualifications

At least 3 years’ experience in one either of: Python, SQL, or another programming language
Bachelor's degree or equivalent experience required; advanced degree preferred
Knowledge in at least two of: DevOps/DataOps, Django, Algorithms/Algorithm design, NLP, Machine Learning, Data Mining, Docker, Linux, Process Engineering
Proven track record as a high-performing practitioner and strong executor, particularly in content-related domain
Strong domain knowledge & content expertise within Credit Rating Agencies a significant plus
Ability to quickly build credibility and relationships across multiple global teams
Ability to work effectively within and across teams, foster collaboration
Aptitude to communicate and deliver on projects, strong prioritization, and organization skills
Strong communication skills. This includes translating technical details into concepts for non-technical audiences
Strong problem-solving skills. Able to quickly identify and understand issues and drive towards effective resolution
Experience as part of a team that leverages Agile, Six Sigma Green Belt Certified a significant plus
Strong familiarity with latest technologies to accelerate data operations

About S&P Global Ratings

S&P Global Ratings is the world’s leading provider of independent credit ratings. Our ratings are essential to driving growth, providing transparency and helping educate market participants so they can make decisions with confidence. We have more than 1 million credit ratings outstanding on government, corporate, financial sector and structured finance entities and securities. We offer an independent view of the market built on a unique combination of broad perspective and local insight. We provide our opinions and research about relative credit risk; market participants gain independent information to help support the growth of transparent, liquid debt markets worldwide.

S&P Global Ratings is a division of S&P Global (NYSE: SPGI), which provides essential intelligence for individuals, companies and governments to make decisions with confidence. For more information, visit www.spglobal.com/ratings.

S&P Global has a Securities Disclosure and Trading Policy (“the Policy”) that seeks to mitigate conflicts of interest by monitoring and placing restrictions on personal securities holding and trading. The Policy is designed to promote compliance with global regulations. In some Divisions, pursuant to the Policy’s requirements, candidates at S&P Global may be asked to disclose securities holdings. Some roles may include a trading prohibition and remediation of positions when there is an effective or potential conflict of interest. Employment at S&P Global is contingent upon compliance with the Policy.

Equal Opportunity Employer

S&P Global is an equal opportunity employer and all qualified candidates will receive consideration for employment without regard to race/ethnicity, color, religion, sex, sexual orientation, gender identity, national origin, age, disability, marital status, military veteran status, unemployment status, or any other status protected by law. Only electronic job submissions will be considered for employment.

If you need an accommodation during the application process due to a disability, please send an email to: EEO.Compliance@spglobal.com and your request will be forwarded to the appropriate person.

US Candidates Only

The EEO is the Law Poster http://www.dol.gov/ofccp/regs/compliance/posters/pdf/eeopost.pdf describes discrimination protections under federal law.

Please note that S&P Global is a federal contractor, and in keeping with the US Government’s federal contractor vaccine mandate, the company plans to require all employees in the US to be fully vaccinated if and when such requirements becomes enforceable. S&P Global will also follow other state and local vaccine requirements which may apply.

#

Equal Opportunity Employer

S&P Global is an equal opportunity employer and all qualified candidates will receive consideration for employment without regard to race/ethnicity, color, religion, sex, sexual orientation, gender identity, national origin, age, disability, marital status, military veteran status, unemployment status, or any other status protected by law. Only electronic job submissions will be considered for employment.

If you need an accommodation during the application process due to a disability, please send an email to: EEO.Compliance@spglobal.com and your request will be forwarded to the appropriate person. 

US Candidates Only: The EEO is the Law Poster http://www.dol.gov/ofccp/regs/compliance/posters/pdf/eeopost.pdf describes discrimination protections under federal law.

IFTECH202.2 - Middle Professional Tier II (EEO Job Group)

Job ID: 269476

Posted On: 2022-03-15

Location: Centennial, Colorado, United States

"
Flatiron Health,"Data Reporting Engineer, Data Products","United States
Remote","We’re looking for a Data Reporting Engineer to join the Data Products initiative team at Flatiron. Here's what you need to know about the role, our team and why Flatiron Health is the right next step in your career.

What You’ll Do

In this role, you will be a data expert for OncoEMR, Flatiron’s electronic health platform. You will work directly with customers to enable data-driven decision making at hundreds of community cancer clinics across the country, leading to better care for their patients. You will help design and build a next-generation analytics platform to simplify analysis and unlock impactful insights in the future. In addition, you’ll also:

Write and edit custom SQL queries to generate customer-facing dashboards and reports for community oncology clinics designed to meet their clinical and operational needs
Analyze reporting requests to develop solutions addressing common needs across oncology practices
Build and maintain data pipelines that power parts of our analytics product
Collaborate with customers and synthesize feedback to design and prototype new reporting products
Work with platform software engineers to improve the data infrastructure that powers clinical analytics
Maintain cross-functional relationships with customer-facing teams and continually enhance team efficiency

Who You Are

You are an analytical problem solver who is fluent in SQL and has experience building data reports and visualizations. You are excited to learn about the cancer patient journey and how community oncology practices operate. You communicate effectively and empathetically with clients to understand the problem they are solving, gather requirements, and design and build an analytic solution.

You have experience with:
Database performance and interpretation of query execution plans
Database scripting and Extract, Transform and Load processes
You are organized with strong prioritization and communication skills
You love working with engineering teams to develop analytics content that complements new application features and workflows
You thrive in a cross-functional environment

Extra Credit

You have healthcare industry knowledge/context (especially oncology-specific knowledge)
You have experience with T-SQL and building reports in SQL Server Reporting Services
You have experience visualizing data in Looker dashboards
You have experience working with Visual Studio, C#, .Net, python, or spark

Why You Should Join Our Team

A career at Flatiron is a chance to work with everyone involved in the future of cancer care and research—all under one roof. Researchers, data scientists, designers, clinicians, technologists and many more all work together to improve cancer care and accelerate research.

We Offer

At Flatiron, we strive to build and maintain an environment where employees from all backgrounds are valued, respected and have the opportunity to succeed. You'll also find a culture of continuous learning, broad and inclusive employee support offerings, and a commitment to supporting our team members in all aspects of their lives—at home, at work and everywhere in between.

Flatiron University training curriculum which includes presentation skills, meeting mastery, coding languages and more
Career coaching opportunities
Hackathons for all employees (not just our engineers!)
Professional development benefit for attending conferences, industry events and external courses
Work/life autonomy via flexible work hours and flexible paid time off
Employee Resource Groups (ERGs) that encourage our employees to share their unique experiences and perspectives
Generous parental leave (16 weeks for either parent)
Back-up child care
Flatiron-sponsored fitness classes

Flatiron Health is proud to be an Equal Employment Opportunity employer.

We do not discriminate based upon race, religion, color, national origin, gender (including pregnancy, childbirth, or related medical conditions), sexual orientation, gender identity, gender expression, age, status as a protected veteran, status as an individual with a disability, or other applicable legally protected characteristics.

COVID-19: Please note that, pursuant to Flatiron's health and safety protocol relating to COVID-19, all Flatiron employees, including individuals selected for hire, are required to disclose their COVID-19 vaccination status, and must be fully vaccinated in order to access our offices or travel to customer sites. Vaccination requirements are subject to legally required exemptions for disability or sincerely-held religious beliefs, though exemptions may not be feasible for some external-facing roles."
Carvana,"Data Engineer, Operations Analytics","Phoenix, AZ
On-site","About Carvana

If you like disrupting the norm and are looking for a company revolutionizing an industry then you will LOVE what Carvana has done for the car buying experience. Buying a car the old fashioned way sucks and we are working hard to make it NOT suck. At Carvana, our customers can hop online to...

Search and browse our inventory of over 20,000 vehicles that we own and certify.
Narrow down search results using highly intelligent filtering tools/components.
View vehicle details, Carfax reports, and 360 rotating studio images for every vehicle.
Secure financing in minutes using Carvana’s in-house service or their own bank.
Interact with GUI components to easily customize loan length, down payment, and monthly payment.
Generate, upload, and eSign all documents online (no ink necessary).
Schedule front door delivery or pick up at one of our vending machines.
Trade-in their existing vehicle or just sell it to Carvana (no purchase necessary).

For more information on Carvana and our mission, sneak a peek at our company introduction video or learn more about what it’s like to work here from the people that already do.

About The Team And Position

In today’s world, data is king and this is the team with all the data. If you’re excited about understanding complex data sets from disparate sources, this is the team for you. Our data science and analytics team automates everything Carvana does from modeling consumer behavior to understanding how to make our users’ lives better.

Our Data Engineering team is the bouncer of Carvana’s data – and we have no qualms about tossing out the riff-raff. Our team partners with various internal stakeholders – from marketing to data science to engineering – to help Carvana become a strong, data-driven company.

What You’ll Be Doing

Be the connection between our supply chain systems and our data. You will have the opportunity to learn and grow working alongside a highly skilled and supportive team on exciting projects that will help shape Carvana.
Conceptualize and build an enterprise data warehouse for our Supply Chain data
Use tools like SSDT, Python and Databricks to build performant and stable data pipelines
Move and shape structured and semi-structured data between many cloud providers and data platforms, including MS SQL Server, AWS S3, Azure DataLake, Azure Blob and Azure SQL DB
Use appropriate methodologies to gather production data
Work with Engineering, Product and Analytics to identify gaps and opportunities in order to design and implement solutions that support Supply Chain business strategies and deliver value
Utilize test and production environments, adhering to change management requirements for system implementations
Participate in team design sessions and code reviews
Take part in cross-functional project teams with data scientists, product engineers and analysts, among others.
Work with end users and other IT teams to resolve operational issues and mitigate risks as applicable
Other duties as assigned.

What You Should Know

You hold (or have the equivalent experience to) a degree in Management Information Systems, Computer Science, etc.
You have an extensive background in SQL and relational databases
You have experience using Python for medium to advanced data processing tasks
You have experience using APIs as a data source
You have a strong understanding of data integration concepts, business intelligence, data warehousing and working with large data
You have worked with and feel comfortable with MS SQL Server
You have solid understanding of performance tuning and indexing concepts
You actively keep up with industry best practices
You’re a forward thinker; you will predict how future company evolution will affect processes implemented today
You are fearless of being un-knowledgeable about a particular subject area/technology; you yearn to learn and ask questions, with a strong desire to continue growing
You practice excellent written and verbal communication
Your attitude rocks
Bonus:
You have experience coding in JAVA
You have experience working with streaming data pipelines

What We’ll Offer In Return

Full-Time Salary Position with a competitive salary.
Medical, Dental, and Vision benefits.
401K with company match.
A multitude of perks including student loan payments, discounts on vehicles, benefits for your pets, and much more.
A great wellness program to keep you healthy and happy both physically and mentally.
Access to opportunities to expand your skill set and share your knowledge with others across the organization.
A company culture of promotions from within, with a start-up atmosphere allowing for varied and rapid career development.
A seat in one of the fastest-growing companies in the country.

Other Requirements

To be able to do your job at Carvana, there are some basic requirements we want to share with you.

Must be able to read, write, speak and understand English.

Of course, we’ll make any reasonable accommodations for those with disabilities to perform the essential functions of their jobs.

Legal stuff

Hiring is contingent on passing a complete background check. This role is eligible for visa sponsorship.

Carvana is an equal employment opportunity employer. All applicants receive consideration for employment without regard to race, color, religion, gender, sexual orientation, gender identity or expression, marital status, national origin, age, mental or physical disability, protected veteran status, or genetic information, or any other basis protected by applicable law. Carvana also prohibits harassment of applicants or employees based on any of these protected categories.

Please note this job description is not designed to contain a comprehensive listing of activities, duties, or responsibilities that are required of the employee for this job. Duties, responsibilities, and activities may change at any time with or without notice."
Fidelity Investments,Data Engineer - BI Developer,"Merrimack, NH","Job Description

The Role

Responsibilities With Include

We are seeking a BI Developer with with experience developing reports and dashboards using Business Intelligence software, supporting platform infrastructures, and deploying applications and data to the public cloud. You will be a collaborative member of the Analytics and BI squad within the FFIO Business Process Management & Analytics product area.

Take on key analytics challenges aimed at building and extending our platform and all aspects of solution delivery of data for integrated, actionable insights.
Work on a team of innovators to create next-level solutions that improve business operations.
Comprehend and translate business requirements into technical specifications.
Provide implementation and operational support for business intelligence platforms.
Collaborate with partners across the organization.
Develop subject matter expertise in our financial domains
Manage key partner relationships


The Expertise and Skills You Bring

5+ years of experience in IT as a Business Intelligence analyst or developer
2+ years proficiency and hands-on experience in platforms like Tableau, OBIEE, Power BI
Demonstrable experience with report development, business analytics, business intelligence or comparable data engineering role
Development and delivery of high quality, timely and maintainable reporting capability in an Agile environment which meet functional and non-functional business requirements
Proven hands-on experience with developing dashboards, reports, tables, graphs and charts in OBIEE and BI Publisher.
Experience in database analysis using Oracle PL/SQL
Understanding of Data Warehouse and Dimensional Modeling (star-schema)
Participate in the definition of the BI delivery strategy, standards and support tools
Experience with statistical analysis using R or Python scripting is a plus
Financial Services experience preferred
Bachelor's Degree in Computer Science or related subject


Certifications

Company Overview

Fidelity Investments is a privately held company with a mission to strengthen the financial well-being of our clients. We help people invest and plan for their future. We assist companies and non-profit organizations in delivering benefits to their employees. And we provide institutions and independent advisors with investment and technology solutions to help invest their own clients’ money.

Join Us

At Fidelity, you’ll find endless opportunities to build a meaningful career that positively impacts peoples’ lives, including yours. You can take advantage of flexible benefits that support you through every stage of your career, empowering you to thrive at work and at home. Honored with a Glassdoor Employees’ Choice Award, we have been recognized by our employees as a Best Place to Work in 2022. And you don’t need a finance background to succeed at Fidelity—we offer a range of opportunities for learning so you can build the career you’ve always imagined.

As a result of COVID-19, many of our associates are continuing to work remotely. When Fidelity employees eventually return to the office, our goal is for most people to work flexibly in a way that balances both personal and business needs with time onsite and offsite through what we’re calling “Dynamic Working.”

We invite you to Find Your Fidelity at fidelitycareers.com.

Fidelity Investments is an equal opportunity employer. We believe that the most effective way to attract, develop and retain a diverse workforce is to build an enduring culture of inclusion and belonging.

Fidelity will reasonably accommodate applicants with disabilities who need adjustments to participate in the application or interview process. To initiate a request for an accommodation, contact the HR Leave of Absence/Accommodation Team by sending an email to accommodations @fmr.com, or by calling 800-835-5099, prompt 2, option 2.

"
Amazon,Data Engineer - Sustainable Packaging,"Seattle, WA","Job Summary

DESCRIPTION

Are you interested in delighting customers and are passionate about promoting sustainability? Then Amazon’s Packaging Team is the place for you. We are the Customer Packaging EXperience (CPEX) team within Amazon Robotics Technology organization and we optimize Amazon’s packaging solutions. To do this across billions of shipments, we need data infrastructure, tools, software solutions that support analytics, data science, and machine learning functionalities all of which come together in building packaging decision mechanisms across millions of products.

You'll be responsible for the design, implementation, operation, and support of large-scale, performance-critical data sources that are crucial to the successful operation of our CPEX team. You will work with tenured ML scientists, software developers, business intelligence engineers, and product managers on our team as well as remote data engineering resources. You will be tasked with optimizing, managing, and supporting existing data solutions and identifying and designing our next generation.

You should be an expert in the architecture of DW, ETL solutions using multiple technologies (RDBMS, AWS, and Big Data). You should excel in the design, creation, management, and business use of extremely large datasets. You should be able to write scripts (Python, UNIX, Java etc.) and automate processes. You should have excellent business and communication skills to be able to work with business owners to develop and define key business questions, and to build data sets that answer those questions. Above all you should be passionate about working with huge data sets and someone who loves to bring data sets together to answer business questions and drive change.


Basic Qualifications

2+ years of experience analyzing and interpreting data and experience with Redshift, Oracle, NoSQL etc.
Experience with data modeling, data warehousing, and building ETL pipelines
3+ years of experience as a Data Engineer or in a similar role
Experience in working and delivering end-to-end projects independently.
Knowledge of distributed systems as it pertains to data storage and computing
Experience in at least one modern object-oriented programming language (Ruby, Python, Java)
Experience with Redshift, Oracle, NoSQL etc.
Bachelor's degree in Computer Science, Engineering, Mathematics, or a related technical discipline
3+ years of industry experience in Data Engineering, BI Engineer, or related field
Track record of data management fundamentals and data storage principles
Hands-on experience and advanced knowledge of SQL, Python etc.
Demonstrated strength in data modeling, ETL development, and data warehousing
Knowledge of distributed systems as it pertains to data storage and computing

Preferred Qualifications

5+ years of experience as a Data Engineer, BI Engineer, or Systems Analyst in a company with large, complex data sources.
Experience building/operating highly available, distributed systems of data extraction, ingestion, and processing of large data sets
Experience working with AWS big data technologies (EMR, Redshift, S3, Glue, Kinesis and Lambda)
Proven success in communicating with users, other technical teams, and senior management to collect requirements, describe data modeling decisions and data engineering strategy
Experience providing technical leadership and mentoring other engineers for best practices on data engineering
Knowledge of software engineering best practices across the development lifecycle, including agile methodologies, coding standards, code reviews, source management, build processes, testing, and operations

Amazon is committed to a diverse and inclusive workplace. Amazon is an equal opportunity employer and does not discriminate on the basis of race, national origin, gender, gender identity, sexual orientation, protected veteran status, disability, age, or other legally protected status. For individuals with disabilities who would like to request an accommodation, please visit https://www.amazon.jobs/en/disability/us.

Amazon is committed to a diverse and inclusive workplace. Amazon is an equal opportunity employer and does not discriminate on the basis of race, national origin, gender, gender identity, sexual orientation, protected veteran status, disability, age, or other legally protected status. For individuals with disabilities who would like to request an accommodation, please visit https://www.amazon.jobs/en/disability/us.


Company - Amazon.com Services LLC

Job ID: A1980646"
Hyatt Hotels Corporation,Data Engineer,"Chicago, IL","Summary

The Opportunity

At Hyatt, we’re working to Advance Care through data-driven decisions and automation. This mission serves as the foundation for every decision as we create the future of travel. We can’t do that without the best talent – talent that is innovative, curious, and driven to create exceptional experiences for our guests, customers, owners and colleagues.  

Hyatt seeks an experienced Data Engineer who will be an exceptional addition to our growing engineering team. The Data Engineer will work closely with engineering, product managers and data science teams to meet data requirements of various initiatives in Hyatt.

As a Data Engineer, you will take on big data challenges in an agile way. In this role, you will build data pipelines that enable engineers, analysts and other stakeholders across the organization. You will build data models to deliver insightful analytics while ensuring the highest standard in data integrity. You will integrate different data sources, improve the efficiency, reliability, and latency of our data system, help automate data pipelines, and improve our data model and overall architecture.

You will be part of a highly visible, collaborative and passionate data engineering team and will be working on all the aspects of design, development and implementation of scalable and reliable data products and pipelines.

Applying the latest techniques and approaches across the domains of data engineering, and machine learning engineering isn’t just a nice to have, it’s a must.

This candidate builds fantastic relationships across all levels of the organization and is recognized as a problem solver who looks to elevate the work of everyone around them.

Who We Are

Hyatt is proud to have a culture where coworkers become friends and family. Since 1957, our focus on care for our employees and our guests has served as the heart of our business and made Hyatt one of the best hospitality brands in the world, with more than 950 hotel, all-inclusive, and wellness resort properties in 67 countries across six continents.

As we continue to grow – even during the most challenging and uncertain times – we never lose sight of what’s most important: People.

Why now?

We are in a time of extraordinary transformation. Passion for personal travel combined with the explosive growth of global business underpinned our growth for years. With Covid-19, much travel has ceased - but the passion for travel remains. Now that vaccines have been introduced, one of the things people look forward to most is traveling. Hyatt is at the epicenter of the return to travel - and we are looking for passionate changemakers to be a part of our journey. When you join Hyatt, you will join a world of possibility.

How We Care For Our People

Wellbeing is the ultimate realization of our purpose — we care for people so they can be their best. We believe this focus on employees is the key to our success and we’ve earned a place on Fortune’s prestigious “100 Best Companies to Work For®” for the last seven years, ranking No. 28 in 2020.

We’re proud to offer exceptional benefits which may include:

Complimentary and discounted stays at Hyatt hotels around the world
A global family assistance policy with paid time off following the birth or adoption of a child as well as financial assistance for adoption
Work-life benefits including wellbeing initiatives such as a complimentary Headspace subscription, on-site rest and relaxation, meditation lounges and flexible schedules

Our Commitment to Diversity, Equity and Inclusion

Hyatt has made significant strides toward building a diverse, equitable and inclusive culture. With more than 127,000 colleagues across 67 countries, we embrace all cultures, races, ethnicities, genders, sexual orientations, ages, abilities, perspectives, and ways of thinking. Our culture is one that empowers every individual to be his or her best, and such authentic connection inspires the way we care for each other and for our guests.

Who You Are

We care about your qualifications, but we care more about your qualities when you join the Hyatt family. As our ideal candidate, you understand the power and purpose of our Culture of Care and share our core values of Respect, Integrity, Humility, Empathy, Creativity and Fun. Our colleagues embody our purpose of caring for people, including each other, our guests, and ultimately our owners. This commitment to genuine service and care is what differentiates us and drives guest preference.

Qualifications

The Role

Collaborate with product managers, data scientists, engineering, and program management teams to define product features, business deliverables and strategies for data products
Collaborate with business partners, operations, senior management, etc on day-to-day operational support
Support operational reporting, self-service data engineering efforts, production data pipelines, and business intelligence suite
Interface with multiple diverse stakeholders and gather/understand business requirements, assess feasibility and impact, and deliver on time with high quality
Design appropriate solutions and recommend alternative approaches when necessary
Work with high volumes of data, fine tuning database queries and able to solve complex technical problems
Contribute to multiple projects/demands simultaneously
Work in a fast paced, collaborative and iterative environment
Exercise independent judgment in methods and techniques for obtaining results
Work in an agile/scrum environment
Use state of the art technologies to acquire, ingest and transform big datasets
The ideal candidate demonstrates a commitment to Hyatt core values: respect, integrity, humility, empathy, creativity, and fun.

Qualifications

2 to 5+ years of experience within the field of data engineering or related technical work including business intelligence, analytics
Experience and comfort solving problems in an ambiguous environment where there is constant change. Have the tenacity to thrive in a dynamic and fast-paced environment, inspire change, and collaborate with a variety of individuals and organizational partners
Experience designing and building scalable and robust data pipelines to enable data-driven decisions for the business
Very good understanding of the full software development life cycle
Very good understanding of Data warehousing concepts and approaches
Experience in building Data pipelines and ETL approaches
Experience in building Data warehouse and Business intelligence projects
Experience in data cleansing, data validation and data wrangling
Hands-on experience in AWS cloud and AWS native technologies such as Glue, Lambda, Kinesis, Lake Formation, S3, Redshift
Experience using Spark EMR, RDS, EC2, Athena, API capabilities, CloudWatch, CloudTrail is a plus
Experience with Business Intelligence tools like Tableau, Cognos, ThoughtSpot, etc is a plus
Hands-on experience building complex business logics and ETL workflows using Informatica PowerCenter is preferred
Experience in one of the scripting languages: Python or Unix Scripting
Proficient in SQL, PL/SQL, relational databases (RDBMS), database concepts and dimensional modeling
Strong verbal and written communication skills
Demonstrate integrity and maturity, and a constructive approach to challenges
Demonstrate analytical and problem-solving skills, particularly those that apply to Data Warehouse and Big Data environments
Open minded, solution oriented and a very good team player
Passionate about programming and learning new technologies; focused on helping yourself and the team improve skills
Effective problem solving and analytical skills.
Ability to manage multiple projects and report simultaneously across different stakeholders
Rigorous attention to detail and accuracy
Bachelor’s degree in Engineering, Computer Science, Statistics, Economics, Mathematics, Finance, or a related quantitative field

The position responsibilities outlined above are in no way to be construed as all encompassing. Other duties, responsibilities, and qualifications may be required and/or assigned as necessary.

Reality Check (US Only)

Research shows that while men apply to jobs if they meet just 60 percent of the criteria, women, people of color and others who belong to historically-excluded groups tend to apply if they check every box. Unsure you check every box, but feeling inspired to enhance our team? Apply. We’d love to consider your unique experiences and how you could make Hyatt even better."
Under Armour,Sr. Data Platform Engineer (REMOTE),"United States
Remote","Under Armour has one mission: to make you better. We have a commitment to innovation that lies at the heart of everything we do, not just for our athletes but also for our teammates. As a global organization, our teams around the world push boundaries and think beyond what is expected. Together our teammates are unified by our values and are grounded in our vision to inspire you with performance solutions you never knew you needed but can’t imagine living without.

Position Summary

At Under Armour, we’re building the data products and services to power the future of our direct to consumer experience as well as other core aspects of our performance brand.

Performing alongside data analytics, data science, and digital marketing teams and across channels such as MapMyFitness and UA.com, we’d like you to join our cross-cutting Data Platform team in building real solutions to centralize, enrich, and activate data across the enterprise. You will learn, grow, and play in an environment that focuses on results and delivery, all backed by one of the strongest consumer brands in history.

We're looking to add a new Sr. Data Platform Engineer to join our Enterprise Data Management team, which powers the storage, processing, integration and cataloging of our data. In this role, you will work across teams to build pipelines, services, and tools that enable both UA teammates and our integrated systems with the data, information, and knowledge to fulfill Under Armour’s mission to make all athletes better.

Essential Duties & Responsibilities

Design, build, integrate, and maintain services that are critical to Under Armour’s data foundation.
Work across teams up and down the data technology and enablement stacks to understand how to improve or expand our data products and services
Contribute to the mission of the Data Platform today and build the vision for its future
Overhaul of our centralized data store for analytics and operations: evolving global data regulations require a revised approach for how we manage and respect the data of our consumers throughout the stack.
Integrate and deploy tooling for consumer identity resolution and marketing activation across our various consumer endpoints: this is a greenfield opportunity to forge a new way of operating across teams as well as data sources and destinations
Enable self-service tooling for analytics teams to publish data into our platform: grow our community of practice for data at UA by facilitating discoverability and sharing of data products across the enterprise

Qualifications (Knowledge, Skills & Abilities)

Knowledge of design patterns and data engineering best practices
Knowledge of security and privacy 'by design’ frameworks
Knowledge of data science lifecycle or test/learn models
Programming experience in SQL, Python
Experience with Snowflake, Hive, Spark, Airflow
Experience with Snowflake related services such as DBT, HVR, Snowpipe
Experience with AWS data-related services such as EMR, Glue, S3, Lambda
Experience (and enjoyment) working with emerging technologies
Preference of owning and driving projects from start (requirements gathering) to finish (production deployment)

Education And / Or Experience

Have a solid understanding of data engineering fundamentals (design patterns, common practices) and what it takes to build high-volume data products and services. You likely have a degree in Computer Science, Mathematics, Statistics or similar or equivalent real-world experience.
3+ years of data and/or software engineering experience
Are ready to take ownership and responsibility for your work
Enjoy working with emerging technologies

Other Requirements

Location: Remote
Return To Work Designation: Fully Remote

Relocation

No relocation provided

# LI-Remote

At Under Armour, we are committed to providing an environment of mutual respect where equal employment opportunities are available to all applicants and teammates without regard to race, color, religion, sex, pregnancy (including childbirth, lactation and related medical conditions), national origin, age, physical and mental disability, marital status, sexual orientation, gender identity, gender expression, genetic information (including characteristics and testing), military and veteran status, and any other characteristic protected by applicable law. Under Armour believes that diversity and inclusion among our teammates is critical to our success as a global company, and we seek to recruit, develop and retain the most talented people from a diverse candidate pool.

Learn more about Under Armour’s COVID-19 response and Teammate vaccination policies here.

"
Meta,Data Engineer,"United States
Remote","How would Facebook scale to the next billion users? The Infrastructure Strategy group is responsible for the strategic analysis to support and enable the continued growth critical to Facebook’s infrastructure organization.We are looking for a Data Engineer to not only build data pipelines but also extend the next generation of our data tools. As a Data Engineer, you will develop a clear sense of connection with our organization and leadership - as Data Engineering is the eyes through which they see the product.This is a partnership-heavy role. As a member of Infrastructure Strategy Data Engineering, you will belong to a centralized Data Science/Data Engineering team who partners closely with teams in Facebook’s Infrastructure organization. Through the consulting-nature of our team, you will contribute to a variety of projects and technologies, depending on partner needs. Projects include analytics, ML modeling, tooling, services, and more. The broad range of partners equates to a broad range of projects and deliverables: ML Models, datasets, measurements, services, tools and process.

Data Engineer Responsibilities:


Partner with leadership, engineers, program managers and data scientists to understand data needs.
Design, build and launch extremely efficient and reliable data pipelines to move data across a number of platforms including Data Warehouse, online caches and real-time systems.
Communicate, at scale, through multiple mediums: Presentations, dashboards, company-wide datasets, bots and more.
Educate your partners: Use your data and analytics experience to ‘see what’s missing’, identifying and addressing gaps in their existing logging and processes.
Leverage data and business principles to solve large scale web, mobile and data infrastructure problems.
Build data expertise and own data quality for your areas.


Minimum Qualifications:


Bachelor's degree in Computer Science, Computer Engineering, relevant technical field, or equivalent practical experience.
2+ years of Python development experience.
2+ years of SQL experience.
2+ years of experience with workflow management engines (i.e. Airflow, Luigi, Prefect, Dagster, Digdag, Google Cloud Composer, AWS Step Functions, Azure Data Factory, UC4, Control-M).
2+ years experience with Data Modeling.
2+ years experience in custom ETL design, implementation and maintenance.
Experience understanding requirements, analyzing data, discovering opportunities, addressing gaps and communicating them to multiple individuals and stakeholders.
Experience working with cloud or on-prem Big Data/MPP analytics platform (i.e. Netezza, Teradata, AWS Redshift, Google BigQuery, Azure Data Warehouse, or similar).


Preferred Qualifications:


Experience with more than one coding language.
Experience with designing and implementing real-time pipelines.
Experience with data quality and validation.
Experience with SQL performance tuning and E2E process optimization.
Experience with anomaly/outlier detection.
Experience with notebook-based Data Science workflow.
Experience with Airflow.
Experience querying massive datasets using Spark, Presto, Hive, Impala, etc.


Facebook is proud to be an Equal Opportunity and Affirmative Action employer. We do not discriminate based upon race, religion, color, national origin, sex (including pregnancy, childbirth, or related medical conditions), sexual orientation, gender, gender identity, gender expression, transgender status, sexual stereotypes, age, status as a protected veteran, status as an individual with a disability, or other applicable legally protected characteristics. We also consider qualified applicants with criminal histories, consistent with applicable federal, state and local law.Facebook is committed to providing reasonable accommodations for candidates with disabilities in our recruiting process. If you need any assistance or accommodations due to a disability, please let us know at accommodations-ext@fb.com."
Lokavant,Data Engineer,"United States
Remote","Posted by

Ann Marie Bjerke

Operations Associate @ Lokavant | Corporate Leadership Development | Technical Recruitment | OKR Management | Cross-Functional Team Building

Send InMail

About Lokavant

Lokavant is a clinical intelligence platform that improves the time, cost, and quality of trial execution through data-driven analytics applications. Lokavant’s platform aggregates and integrates real-time data from disparate trial data sources, and powers advanced analytics enabled by its compendium of proprietary trial data. The suite of applications built on the platform allows study teams to proactively manage their studies, and surfaces insights, driving efficiencies in all scientific and operational use cases. For more information, please visit www.lokavant.com.







About the Opportunity

How often are you given the opportunity to build something from the ground up, with an abundance of resources at your disposal; to be part of a team of people accomplished in diverse scientific and engineering disciplines, focused on using the best of what lies at the forefront of technology to address complex, real-world problems that have a positive impact on potentially millions of peoples' lives? This is that kind of opportunity.

We are seeking a thoughtful, hands-on technology enthusiast with a strong aptitude for data engineering to join the rapidly growing Lokavant team in our New York City headquarters. The Data Engineer will work very closely with our front-end developers, back-end developers, development operations engineers, and data scientists. Our platform is fully cloud-based and is being built around modern tools and frameworks in an incredibly fast-moving agile environment.







Key Responsibilities

Design, develop, and implement data infrastructure and pipelines that ingest and transform data from various external sources, storing it in highly optimized database systems, and making it useful to our application and reporting layers
Create automation systems and tools to configure, monitor, and orchestrate data infrastructure and pipelines
Create data integration services to help onboard new customers as quickly as possible
Maintain ongoing reliability, performance, and support of the data infrastructure, providing solutions based on application needs and anticipated growth
Participate in creating and maintaining strict compliance, data privacy and security measures
Develop robust and production-level code to implement new product features in collaboration with other engineers and subject matter experts
Identify and resolve performance and scalability issues, troubleshoot problems, and improve product quality
Collaborate with the Front-End Development team to thread the right information through to forward-facing applications
Interface with the Development Operations colleagues to evaluate and implement methodologies and workflows to facilitate the frequent and continuous release of high-quality software
Work closely with Data Science colleagues to implement descriptive and predictive algorithms and models using the latest technologies
Keep up to date on emerging technology solutions, particularly those on AWS, for continuous improvements in data engineering
Help recruit highly capable engineers to the team from diverse backgrounds
Mentor and be mentored by engineers of varied experience levels and subject matter areas




Minimum Requirements

3+ years relevant experience with data engineering
Strong proficiency with Python and SQL
Experience with AWS S3, EC2, EMR, or an equivalent cloud-hosted infrastructure
Experience with cloud-hosted database/data warehouse architecture (e.g. Snowflake, Redshift, etc.)
Experience writing complex data transformations in SQL and/or related frameworks (e.g. dbt)
Interest in building distributed computing and orchestration frameworks (e.g. Airflow)
Experience working in an Agile software development environment
Exceptional written and verbal communication skills
Strong attention to detail and highly organized, with effective multi-tasking and prioritization skills
Proactive, self-motivated and self-directed, with the ability to learn quickly and autonomously
Comfortable with ambiguity
Superior problem-solving and troubleshooting skills
Ability to work as part of a collaborative cross-functional team in a fast-paced environment
Sincere interest in working at a rapidly changing start-up and scaling with the company as we grow
Bachelor’s degree with strong academic performance in Computer Science, Software Engineering, Applied Science, or equivalent field







Employee Benefits

Competitive salary and annual performance bonus
Full medical, dental, and vision benefits, including mental health and telehealth
Employee stock options
Free membership to One Medical
401(k) retirement savings plan with company match
Flexible paid time off policy
Flexible remote work policy
Generous paid parental leave policy
Short and long-term disability insurance
Basic and supplemental life and AD&D insurance
Health Advocate program
Employee Assistance Program (EAP)
Healthcare & dependent care Flexible Spending Accounts (FSA)
Discounts on auto, home, and pet insurance
Commuter benefits
Great NYC office located in the heart of Times Square
Team-building events and outings
Learning and professional development opportunities
Employee referral program







Lokavant is an equal opportunity employer, indiscriminate of race, color, religion, ethnicity, ancestry, national origin, sex, gender, gender identity, sexual orientation, age, marital status, veteran status, disability, medical condition, or any other protected characteristic. We celebrate diversity and are committed to creating an inclusive environment for all employees."
Amazon Web Services (AWS),Data Engineer - AWS Networking,"Seattle, WA","Description

How often have you had an opportunity to build a big business, solving significant customer problems through innovative technology, from the beginning? The AWS Network organization is looking for passionate, hard-working, and talented individuals to join our fast paced, start-up environment to help invent the future.

As a Data Engineer, you will get the exciting opportunity to work on very large data sets in one of the world's largest and most complex data warehouse environments. You will work closely with the business and technical teams in analysis on many non-standard and unique business problems and use creative-problem solving to deliver actionable output.

The ideal candidate has expertise gathering customer needs and insights, designing logical schema's that organise data in a meaningful, efficient way, and knows how to build scalable and maintainable solutions. They are an expert at data modeling, building end to end data pipelines to stream data from multiple sources, ETL design, have hands-on knowledge of databases such as Redshift, and experience developing user interfaces. They are self-starter, comfortable with ambiguity, with strong attention to detail. They are motivated to achieve result in a fast-paced and ever-changing environment with ability to work effectively with cross-functional teams. A successful candidate will have a track record of partnering closely with customers to invent solutions to complex data problems. They have built trusting relationships with business partners and have experience generating insights that enable senior leaders to make critical business decisions.


Basic Qualifications

Bachelor’s degree in computer science, Data Science, engineering, mathematics, information systems, or a related technical discipline
5+ years of relevant experience in data engineering roles
Detailed knowledge of data warehouse technical architectures, data modelling, infrastructure components, ETL/ ELT and reporting/analytic tools and environments, data structures and hands-on SQL coding
Proficient in at least one or more programming languages: Java, Python, Ruby, Scala
Experienced with AWS services such as Redshift, S3, EC2, Lambda, Athena, EMR, AWS Glue
Experience developing data visualisation and reporting with tools such as Amazon QuickSight, Metabase, Tableau, or similar software
Up to speed on recent advances in distributed systems (e.g. MapReduce, MPP architectures, and NoSQL databases).
Experience building metrics deck and dashboards for KPIs including the underlying data models.
Understand how to design, implement, and maintain a platform providing secured access to large datasets

Preferred Qualifications

Master’s degree in computer science, Data Science, engineering, mathematics, information systems, or a related technical discipline
5+ years of work experience with ETL, Data Modelling, and Data Architecture.
Experience or familiarity with newer analytics tools such as AWS Lake Formation, Sagemaker, DynamoDB, Lambda, ElasticSearch.
Experience with Data streaming service e.g Kinesis Kafka
Ability to develop experimental and analytic plans for data modeling processes, use of strong baselines, ability to accurately determine cause and effect relations
Proven track record partnering with business owners to understand requirements and developing analysis to solve their business problems
Proven analytical and quantitative ability and a passion for enabling customers to use data and metrics to back up assumptions, develop business cases, and complete root cause analysis
Meets/exceeds Amazon’s leadership principles requirements for this role
Meets/exceeds Amazon’s functional/technical depth and complexity for this role

Amazon is committed to a diverse and inclusive workplace. Amazon is an equal opportunity employer and does not discriminate on the basis of race, national origin, gender, gender identity, sexual orientation, protected veteran status, disability, age, or other legally protected status. For individuals with disabilities who would like to request an accommodation, please visit https://www.amazon.jobs/en/disability/us.


Company - Amazon Data Services, Inc.

Job ID: A1596615"
Dice,Data Engineer,"Durham, NC
On-site","Dice is the leading career destination for tech experts at every stage of their careers. Our client, SPECTRAFORCE TECHNOLOGIES Inc., is seeking the following. Apply via Dice today!

Data Engineer Durham, NC ndash Initially remote till pandemic 6+ Months W2 Only (No C2C) Skill Set Modelling Lead Ideal candidate would have designed variety of Semantic layers across many BI tools - PowerBI is what we are using Engagement Lead Modelling BI Layer work Semantic layer work 3rd Party semantic layer work ndash Preferred Engagement Lead ndash Leadership Process oriented. PM skills preferred The Expertise and Skills You Bring Advanced working SQL knowledge and experience working with relational databases, query authoring (SQL) as well as working familiarity with a variety of databases. Proven project management, analysis, and organizational skills. Knowledge of agile methods is a plus. Experience performing root cause analysis on internal and external data and processes to answer specific business questions and seek out opportunities for improvement. Strong analytic skills related to working with unstructured datasets. Experience building and optimizing lsquobig datarsquo data pipelines, architectures, and data sets Preferable Experience in cloud services such as AWS EMR, EC2 etc. Successful history of manipulating, processing, and extracting value from large disconnected datasets Experience supporting and working with multi-functional teams in a dynamic environment. Candidate should also have experience using the following softwaretools Solid experience in BI Tools such as Power-BI, Tableau, OBIEE etc. Preferable experience and familiarity in Semantic layers Multidimensional modeling tools like AtScale Experience with project management tools (agile tools), Office tools presentation skills Experience with relational SQL and NoSQL databases, including Postgres databases Bachelors in Computer Science or related field with relevant experience is desired. Other Pointers This is not necessarily an all-inclusive list of job-related responsibilities, duties, skills, efforts, requirements or working conditions. While this is intended to be an accurate reflection of the current job, SPECTRAFORCE and the assigned client reserve the right to revise the job or to require that other or different tasks be performed as assigned. Benefits SPECTRAFORCE offers ACA compliant health benefits as well as dental, vision, accident, and hospital indemnity insurances. Additional benefits SPECTRAFORCE offers to the eligible employees include commuter benefits, 401K plan with matching and a referral bonus program. SPECTRAFORCE offers unpaid leave as well as paid sick leave when required by law. Equal Opportunity Employer SPECTRAFORCE is an equal opportunity employer and does not discriminate against any employee or applicant for employment because of race, religion, color, sex, national origin, age, sexual orientation, gender identity, genetic information, disability or veteran status, or any other category protected by applicable federal, state, or local laws. Please contact Human Resources at NAhrspectraforce.com mailtoNAhrspectraforce.com if you require a reasonable accommodation"
Walmart,Data Engineer III,"Bentonville, AR
On-site","Position Summary... What You'll Do...

What you'll do...

Here at Walmart, we’re driven by an intellectual curiosity that keeps us on the cutting-edge of user design and a seamless customer experience. We’re intrigued by the opportunity to engineer the most optimal approach that drives conversions and generates consumer loyalty across every touchpoint of the digital journey.

As a Data Engineer III you will support the understanding of the priority order of requirements and service level agreements. Help identify the most suitable source for data that is fit for purpose. Perform initial data quality checks on extracted data. Analyze complex data elements, systems, data flows, dependencies, and relationships to contribute to conceptual, physical, and logical data models. Develop the Logical Data Model and Physical Data Models including data warehouse and data mart designs. Define relational tables, primary and foreign keys, and stored procedures to create a data model structure. Evaluate existing data models and physical databases for variances and discrepancies. Develop efficient data flows. Analyzes data-related system integration challenges and proposes appropriate solutions. Create training documentation and trains end-users on data modeling. Oversee the tasks of less experienced programmers and stipulates system troubleshooting supports. Write code to develop the required solution and application features by determining the appropriate programming language and leveraging business, technical, and data requirements. Create test cases to review and validate the proposed solution design. Create proofs of concept. Test the code using the appropriate testing approach. Deploy software to production servers. Contribute code documentation, maintains playbooks, and provides timely progress updates.

What you will do...

Establishes modify, and document data governance projects and recommendations. Implement data governance practices in partnership with business stakeholders and peers. Interpret company and regulatory policies on data. Educate others on data governance processes, practices, policies, and guidelines. Provide recommendations on needed updates or inputs into data governance policies, practices, or guidelines.

Demonstrate up-to-date expertise and applies this to the development, execution, and improvement of action plans by providing expert advice and guidance to others in the application of information and best practices; supporting and aligning efforts to meet customer and business needs; and building commitment for perspectives and rationales.
Provide and support the implementation of business solutions by building relationships and partnerships with key stakeholders; identifying business needs; determining and carrying out necessary processes and practices; monitoring progress and results; recognizing and capitalizing on improvement opportunities; and adapting to competing demands, organizational changes, and new responsibilities.

Minimum Qualifications...

As permitted by applicable law, provide evidence of full vaccination as defined by CDC guidelines OR secure approval of medical or religious accommodation for the vaccination mandate.,

Option 1: Bachelor’s degree in Computer Science and 3 years' experience in software engineering or related field.
Option 2: 5 years’ experience in software engineering or related field.
Option 3: Master's degree in Computer Science and 1 year’s experience in software engineering or related field. 2 years' experience in data engineering, database engineering, business intelligence, or business analytics.

Preferred Qualifications...

Data engineering, database engineering, business intelligence, or business analytics, ETL tools and working with large data sets in the cloud, Master’s degree in Computer Science or related field and 3 years' experience in software engineering

The above information has been designed to indicate the general nature and level of work performed in the role. It is not designed to contain or be interpreted as a comprehensive inventory of all duties, responsibilities and qualifications required of employees assigned to this job. The full Job Description can be made available as part of the hiring process.

Benefits & Perks

Beyond competitive pay, you can receive incentive awards for your performance. Other great perks include 401(k) match, stock purchase plan, paid maternity and parental leave, PTO, multiple health plans, and much more.

Equal Opportunity Employer

Walmart, Inc. is an Equal Opportunity Employer – By Choice. We believe we are best equipped to help our associates, customers, and the communities we serve live better when we really know them. That means understanding, respecting, and valuing diversity- unique styles, experiences, identities, ideas, and opinions – while being inclusive of all people.

About Global Tech

Imagine working in an environment where one line of code can make life easier for hundreds of millions of people and put a smile on their face. That’s what we do at Walmart Global Tech. We’re a team of 15,000+ software engineers, data scientists and service professionals within Walmart, the world’s largest retailer, delivering innovations that improve how our customers shop and empower our 2.2 million associates. To others, innovation looks like an app, service, or some code, but Walmart has always been about people. People are why we innovate, and people

Minimum Qualifications...

Outlined below are the required minimum qualifications for this position. If none are listed, there are no minimum qualifications.

Option 1: Bachelor’s degree in Computer Science and 2 years' experience in software engineering or related field. Option 2: 4 years’ experience in

software engineering or related field. Option 3: Master's degree in Computer Science. Preferred Qualifications...

Outlined below are the optional preferred qualifications for this position. If none are listed, there are no preferred qualifications.

Data engineering, database engineering, business intelligence, or business analytics, Master’s degree in Computer Science or related field and 2 years' experience in software engineering or related field Primary Location...805 SE MOBERLY LN, BENTONVILLE, AR 72712, United States of America"
Luxoft,Data Engineer,"Los Angeles, CA","Project Description

Our Data Strategy and Operations team is focused on implementing and evolving client's investment data strategy to enable the full range of business objectives; data for minimizing risk, data for efficient processes, and data for growth and innovation. It also delivers a set of capabilities to source, automate, and curate data assets to enable and enrich core investment and distribution processes, quantitative analytics, reporting and analysis, and advanced analytic efforts such as artificial intelligence.

Responsibilities

Work with complex data structures and provide innovative ways to a solution for complex data delivery requirements
Evaluate new and alternative data sources and new integration techniques
Contribute to data models and designs for the data warehouse
Establish standards for documentation and ensure your team adheres to those standards
Influence and develop a thorough understanding of standards and best practices used by your team

Skills

Must have

Advanced knowledge of SQL
Knowledge of data management and automation technologies, such as Python (preferred)
AWS experience is a must
Experience with business intelligence, analytics, reporting, and data transformation
Advanced knowledge of scripting tools for automation such as R, Python, Alteryx, etc.

Nice to have

Financial background

Languages

English: C1 Advanced"
Retina AI,Data Engineer (Remote),"Santa Monica, CA
Remote","At Retina, our team (former Facebook, PayPal, and Stanford alums) has built a product that can predict individual customer lifetime value at the earliest point of customer contact and changed how brands view their relationships with their customer. We use data science to predict future buying behavior of consumers and recommend actions that businesses can take around those predictions.

Our APIs connect directly to a company’s data warehouse and start to make recommendations on how they should move marketing, customer service and retention dollars towards the more profitable customers. In customer acquisition alone, Retina has the opportunity to optimize a $389 billion industry spend.

We recently raised our series A funding and work with brands like Dollar Shave Club, Nestlé, Madison Reed, and Capital One. We have more than 10x our revenue in the last year and we’re looking for the brightest minds who believe in data driven decision making at scale to join our growing team!

Overview:

As a Data Engineer, you will be working closely with the founding CTO to leverage the best technologies and techniques to build out DataOps at Retina. That is to automate fast, available, and accurate data to super-charge the data science we deliver to our clients.

The perfect candidate will love automating via code, be comfortable with AWS services, and have had experience with Big Data systems such as Apache Spark. We look for self-starters focused on results, who have a proven track record of success with multiple technologies and data sets. Data is at the core of what we do at Retina; and this position is an opportunity to build and innovate the way we do data.

Responsibilities:

Own the data lifecycle. From ingest and automated quality checks, to discovery and usage.
Automate and enhance data and machine learning pipelines in Databricks using Spark / Python (PySpark)
Empower Retina team members with fast self-service access to data for ad-hoc analysis
Advise and assist our clients in integrating with Retina
Innovate on bringing the best technologies and tools to empower data at Retina


Qualifications:

2-3 years professional working experience with analytics, databases, and data systems
1+ years experience working with Apache Spark
Bachelors in Engineering, Computer Science, or related technical degree
Strong proficiency in SQL, Python, and AWS
Experience with large relational data in various formats
Experience in at least one big data relational database technology such as Snowflake, Cassandra, Redshift, BigQuery


We are a people-oriented company; this means taking care of our team members is critical to our success. We believe that if we hire the smartest people and invest in them, a high-performing team -- and subsequently a high performing company -- will flourish.

Here's why you'll love working at Retina:

Ownership:

Take ownership of products, client relationships, and the trajectory of your role. Working in a startup means you’re a vital member of a small team — and your work will definitely be recognized.

Exciting challenges:

Our product is solving business challenges in new and exciting ways. You’ll deliver capabilities companies don’t even know they need yet, and learn amazing things along the way.

Innovation:

Your coworkers will be the type of people who take risks, welcome new perspectives, and live for great new ideas. Learn how to approach problems differently and with agility.

Unlimited opportunities:

Encounter endless opportunities that fall outside of your wheelhouse. At Retina, you will try out new skills and gain valuable experience that puts you on a career path you love.

Great culture:

You can always rely on your team. Our culture is all about learning, growing, and working together to build something great. Find out what it’s like to work with the best.

We also have some amazing benefits and perks that include:

Health Coverage: 99% health coverage for employees and 75% for dependents. (BlueShield PPO Platinum)

Vacation: Unlimited vacation and ample sick leave

Setup Your Own Kit: Buy what you need to get a comfortable work environment (Pick your laptop, headphones and any other accessories needed)

Conferences: 1 Paid conference and unlimited conferences where you present)

Health and Education: $250 per month towards Gym, Books, Audiobooks, or Safari Books online

Meal & Coffee Card: $250 Debit card each month for meals, coffee, etc."
Fivetran,"Data Engineer, Analytics","Oakland, CA
Hybrid","From Fivetran’s founding until now, our mission has remained the same: to make access to data as simple and reliable as electricity. With Fivetran, customer data arrives in their warehouses, canonical and ready to query, with no engineering or maintenance required. We’re proud that more organizations continue to leverage our technology every day to become truly data-driven.




We’re honored to be valued at over $5.6 billion, but more importantly, we’re proud of our core values of 1 Team, 1 Dream, Get Stuck in, and Do the right thing. To learn more about Fivetran’s culture, watch this video. You can also learn more about why Fivetran Analytics is the best place to work for data talents in this video here.




About the role:

We believe that in the future, with the work of writing and maintaining ELT pipelines offloaded to Fivetran, the role of a data engineer opens up into a wider problem space focused on performance, reliability, and making data teams and data consumers more efficient. Fivetran’s internal Analytics team are happy Fivetran customers, and we are looking for a data engineer who is ready to step into that future with us.




This person will own the development and maintenance of tools and systems leveraged by our analytics team, bringing a “DevOps/DataOps” mindset to how we think about the efficiency, reliability and ease of use of our stack. You will work on designing and building highly automated operational flows for monitoring, QAing, deploying and integrating analytics outputs. You will be an integral technical leader within the Analytics team.

This is a full-time position based out of our Oakland office.




Technologies You’ll Use:

We emphasize using no-nonsense tools, and take great pride in the simplicity and effectiveness of the systems we build. Our Modern Data Stack is made up primarily of Python, BigQuery, dbt, Looker, Slack, Google Cloud Functions, and of course, Fivetran!




What You’ll Do:

Design and maintain tools and systems to empower analysts and other data consumers
Support the growth of analytics through streamlining our onboarding and deployment processes
Assess and optimize existing workflows to make them faster, cheaper, more secure and easier to use
Bring the curiosity of an analyst and take a proactive approach toward identifying pain points and solving problems




Skills We’re Looking For:

Deep familiarity with advanced SQL in a modern data warehouse environment (BigQuery, Redshift, Snowflake, etc)
Applied knowledge of a scripting language, preferably Python, in a data engineering context
Experience with serverless computing (AWS Lambda or Google Cloud Functions)
Experience with version control (git)
Experience writing production code, and debugging and deciphering code written by others
Interest in becoming a sought after voice in the data analytics community.




Perks and Benefits:

100% paid Medical, Dental, Vision and Basic Life Insurance. Benefits begin on your first day!
Option of Health Savings Account (HSA) or Flexible Savings Account (FSA)
Generous paid time off (PTO) plus paid sick time, holidays, parental leave, and volunteer days off
401k match program
Eligible donation match program
Monthly cell phone stipend
Work-from-home equipment reimbursement for your home office setup!
Professional development and training opportunities
Company virtual happy hours and fun team building activities
Pet Insurance -- and yes, you can bring your well-behaved fur babies to work
Commuter benefits to help with transit and parking costs"
"Take-Two Interactive Software, Inc.",Data Engineer (Python/AWS),"New York, NY
On-site","Who We Are:

Take-Two develops and publishes some of the world's biggest games. Our Rockstar label creates Grand Theft Auto and Red Dead Redemption, two of the most critically acclaimed gaming franchises in history. Our 2K label creates games like NBA 2K, WWE 2K, Bioshock, Borderlands, Evolve, XCOM and the beloved Sid Meier's Civilization. Our Private Division label publishes Kerbal Space Program, Ancestors and The Outer Worlds.

While our offices (physical and virtual) are casual and inviting, we are deeply committed to our core tenets of creativity, innovation and efficiency, and individual and team development opportunities. Our industry and business are continually evolving and fast-paced, providing numerous opportunities to learn and hone your skills. We work hard, but we also like to have fun, and believe that we provide a great place to come to work each day to pursue your passions.

The Challenge:

The data engineering team at Take-Two Interactive is looking for a Data Engineer with knowledge of data architectures, APIs, and the delivery and transformation of data in a reliable way. As a data engineer on this team, you will apply knowledge of Python and AWS technologies to solve exciting problems. You will work closely with the business and technical teams in analysis on many non-standard and unique business problems and use creative problem solving to deliver actionable output.

The ideal candidate is passionate about both developing software and working with data. He or she must be a team player, always willing to collaborate with others. We are looking for an individual who has a high sense of ownership and s constantly passionate about customer delight & business impact / end-result and ‘gets it done’ in business time.

What You’ll Take On:

Develop and manage stable, scalable data pipelines that cleanse, structure and integrate disparate data sets into a readable and accessible format for end user analyses and targeting using stream and batch processing architectures.
Develop and improve the current data architecture, data quality, monitoring and data availability.
Develop data quality framework to ensure delivery of high-quality data and analyses to stakeholders.
Develop and support continuous integrations processes which use Jenkins, Docker, Git, etc.
Define and implement monitoring and alerting policies for data solutions.

What You Bring:

3+ years of Data Engineering experience of designing, developing and automating scalable ETL/ELT solutions that transform data into accurate and actionable business information
3+ years of Python and PySpark experience
3+ years of SQL experience using advanced SQL queries (analytical functions), writing and optimizing highly efficient queries
2+ years of experience working with AWS technologies stack like S3, EMR, etc.
Experience building API’s (highly desirable)
Comfort in working with business customers to gather requirements and gain a deep understanding of varied datasets
Experience in testing and monitoring data for anomalies and rectifying them
Knowledge of software coding practices across the development lifecycle, including agile methodologies, coding standards, code reviews, source management, build processes, testing, and operations
Bachelor’s degree or equivalent in an engineering or technical field such as Computer Science, Information Systems, Statistics, Engineering, or similar

Preferred Qualifications

AWS Certification
Python
Spark
SQL
Understanding of EMR and related compute engines
Build and deployment tools (for example Jenkins, Maven, SBT)
Git
Developing solutions using Docker
Developing solutions on Kubernetes clusters
Data modeling for data warehousing

What We Offer You:

Great Company Culture. Ranked as one of the most creative and innovative places to work, creativity, innovation, efficiency, diversity and philanthropy are among the core tenets of our organization and are integral drivers of our continued success.
Growth: As a global entertainment company, we pride ourselves on creating environments where employees are encouraged to be themselves, to be inquisitive and collaborative and to grow within and around the company.
Work Hard, Play Hard. Our employees bond, blow-off steam, and flex some creative muscles – through corporate boot camp classes, company parties, game release events, monthly socials, and team challenges.
Benefits. Medical (HSA & FSA), dental, vision, 401(k) with company match, employee stock purchase plan, commuter benefits, in-house wellness program, broad learning & development opportunities, a charitable giving platform with company match and more!
Perks. Fitness allowance, employee discount programs, free games & events, stocked pantries and the ability to earn up to $500+ per year for taking care of yourself and more!

Take-Two Interactive Software, Inc. (“T2”) is proud to be an equal opportunity employer, which means we are committed to creating and celebrating diverse thoughts, cultures, and backgrounds throughout our organization. Employment at T2 is based on substantive ability, objective qualifications, and work ethic – not an individual’s race, creed, color, religion, sex or gender, gender identity or expression, sexual orientation, national origin or ancestry, alienage or citizenship status, physical or mental disability, pregnancy, age, genetic information, veteran status, marital status, status as a victim of domestic violence or sex offenses, reproductive health decision, or any other characteristics protected by applicable law."
Simplex.,Data Engineer,"Austin, Texas Metropolitan Area
Remote","Posted by

Johnny Chang 2nd

Owner and Technical Recruiter

Send InMail

This role is 100% Remote based anywhere in the United States.

Our client, an Agile Transformation Services firm based in Austin, TX is looking for a charismatic and driven Client Success Manager to join our team. We are a rare kind of company that strives to live and breathe our core values of openness, transparency, ownership, continuous improvement, collaboration, and fun. If you want to feel like you truly are operating as part of a team with autonomy in your job then come help us guide teams to accelerate agility within their organization.




The Role

The Client Success Manager is the owner of designated high-profile large customer relationships, working directly with our Senior Director of Client Success. They are the trusted advisors to our customers and will succeed by enabling their customers to succeed.

You will be responsible for establishing exceptional strategic business relationships with customers (from the team level to the C-suite) and ensuring their needs are being met. Another key responsibility is uncovering new engagement opportunities.




Enterprise Client Success Manager Responsibilities:

Manages Client Relationships and becomes a trusted advisor to the client
Assist with client kickoff meetings (agenda preparation, client communication, follow up tracking)
Conduct recurring client satisfaction checks and monthly business reviews to ensure the highest level of service is being provided
Own all contractual stages (initial, extension, expansion) as well as responsibilities around billing questions from the client
Ensure Customer Satisfaction
Manage engagement status, risks/issues, progress communication with the client
Communicate budget progress and status to the client
Perform bi-weekly billing tasks




Qualifications

Trusted Consultant/Coach mentality with a strong focus on client success
4+ years experience as a client-facing project or engagement manager
Experience managing and growing enterprise level accounts
Ability to connect with executive-level partners
Excellent written and verbal communication skills
Ability to create and deliver highly professional presentations
Proven ability to collaborate to deliver holistic client proposals
Very strong organization skills
Up to 20% travel"
Xiartech Inc.,Data Engineer,"United States
Remote","Posted by

Rupesh Kumar

Technical Recruiter

Role: Senior Data Engineer

Location: Remote

Client: Innovaccer

Full-Time




What You Need




● 5+ years of experience in a Data Engineering role, Graduate degree in Computer Science, Statistics,

Informatics, Information Systems or another quantitative field.

● Experience working with relational databases like Snowflake, Redshift or Postgres.

● Proficiency in SQL programming

● Proficiency in at least one programming language (like Python, R, Scala) and experience writing code to

handle data manipulation, scheduling, event-based triggering and automation tasks

● Experience working with AWS including services like EC2, EMR, RDS, Redshift, S3 and more

● US Healthcare Data experience preferably in Value-Based Care and strong healthcare data background -

clinical, claims, FHIR, HL7, X12, CCDA etc.

● Data Analytics and Visualization (using tools like PowerBI)


"
Northern Trust Corporation,Sr. Data Engineer,"Chicago, IL","Data and Analytics – Senior Data Engineer

We have an exciting opportunity in Asset Management Technology team for a Sr. Data Engineer to build and support Data and Analytics initiatives. The Sr. Data Engineer will design and implement architectures to support advanced analytics solutions and enrich data in the platform, creating data pipelines for a wide variety of internal and external data types and ensuring the quality and consistency of current and future data flows.

We’re looking for a candidate with 5-7 years of with strong expertise into cloud data engineering using any of the following public clouds – Azure and Snowflake

Knowledge/ Skills

Excellent written and verbal communication skills, along with an ability to effectively interface with both business partners and with technical staff. Ability to prioritize multiple tasks and work within time lines to meet project expectations. Strong client management, interpersonal and decision making skills and competencies, attention to detail and the ability to be proactive, prudent and confident in pressure situations are a must.

Qualifications

Expertise in designing and implementing a fully operational solution on Snowflake Data Warehouse or Azure Synapse,
Experience working with Azure services like – Azure Data Factory, Blob, etc.
Monitoring health, tuning, and growth of such on-premise and cloud databases,
Experience in managing security and maintenance of Snowflake or Azure Synapse databases,
Manage and monitor user access to the Azure Synapse and Snowflake databases,
Experience in ETL tools from designing and implementing data ingestion and processing pipelines using any big data technologies,
Advanced knowledge of big data querying tools, experience with SQL, Python,
Good understanding of Snowflake Internals and integration of Snowflake with other data processing and reporting technologies is preferred,
Design and Develop ETL pipelines in and out of data warehouse using combination of Python and APIs,
Experience researching new technologies that align with business strategy and improve the user experience,
Experience working with Agile methodologies,
Exceptional problem solving / analytical thinking and skills.

Required

Experience

Bachelor’s Degree in a related field and 5-7 years of equivalent work experience required.

Additional Information"
Motion Recruitment,Senior Data Engineer / FinTech,"San Francisco, CA","Help shape a leading FinTech company that works with thousands of businesses to handle the point-of-sale contact with customers, along with providing cloud platform services for merchants, developers and service providers.

The VP of Data Fabric is building a brand new Data Infrastructure team and, as continued growth creates more client demand, will be hiring a Software Engineer who will be manipulating highly scalable data. This involves building a new infrastructure for the data pipelines to assist in clients’ data analysis and developing new strategies for data insights and identify technical needs in order to assist the business team.

You’ll help develop new strategies for data insights and identify technical needs in order to assist the business team.

Required Skills & Experience
Understanding of distributed systems
Experience with:
SQL databases
Hadoop
MapReduce
HDFS
Degree in Computer Engineering, Computer Sciences or related field
Desired Skills & Experience
Knowledge of:
NoSQL/Cassandra
Hive, pig
Cloud Platforms: AWS, GCP, Rackspace
Experience with highly scalable distributed systems
What You Will Be Doing

Tech Breakdown

50% Data Manipulation
30% Design and Architecture
20% Coding/Scripting
The Offer

Competitive Salary: Up to $180,000/year, DOE

You Will Receive The Following Benefits

Medical Insurance & Health Savings Account (HSA)
401(k)
Paid Sick Time Leave
Pre-tax Commuter Benefit

Posted By: Caitlin Carrion"
Dice,Data Engineer,"Rancho Cordova, CA
On-site","Dice is the leading career destination for tech experts at every stage of their careers. Our client, The Ascent Services Group, is seeking the following. Apply via Dice today!

This is an onsite role at Rancho Cordova, CA Bachelor's degree in Computer Science, Mathematics, Statistics, or other related technical field. Experience in ETL tool, SAP BusinessObjects Data Services preferred Experience in Python or Java development. Experience in SQL (No-SQL experience is a plus). Experience in data virtualization, Tibco Data Virtualization preferred. Experience in Cloud platforms. Snowflake is preferred. The Senior Data Engineer will design, build, and implement data integration solutions, including data pipelines, data API's, and ETL jobs, to meet the data needs of applications, services, micro services, data assets, and business intelligence and analytical tools. Working with data architects, application development teams, data analytics teams, business analytics, product managers, and the data governance COE, the Senior Data Engineer will design and develop interfaces between applications, databases, data assets, external partners, and third-party systems in a combination of cloud and on-premise platforms. Develops and maintains scalable data pipelines and builds out new integrations to support continuing increases in data volume and complexity Designs and develops scalable ETL packages for point to point integration of data between source systems, extraction and integration of data into various data assets, including data warehouse and fit for purpose data repositories, both on prem and cloud Designs and develops scalable data APIs to provide data as a service to microservices, applications, and analytical tools Designs and develops data migrations in support of enterprise application and system implementations from legacy systems Writes functional specifications for data pipelines and APIs and writes and performs unitintegration tests Performs data analysis required to troubleshoot data related issues and assist in the resolution of data issues. Assists in planning, coordinating, and executing engineering projects Supports and collaborates with other Engineers through evaluation, design analysis, and development phases Maintains knowledge, ensures competency and compliance with policies and procedures, in order to be the technical expert while collaborating with cross-functional teams This list is not all-inclusive and you are expected to perform other duties as requested or assigned Develops and maintains scalable data pipelines and builds out new integrations to support continuing increases in data volume and complexity Designs and develops scalable ETL packages for point to point integration of data between source systems, extraction and integration of data into various data assets, including data warehouse and fit for purpose data repositories, both on prem and cloud Designs and develops scalable data APIs to provide data as a service to microservices, applications, and analytical tools Designs and develops data migrations in support of enterprise application and system implementations from legacy systems Writes functional specifications for data pipelines and APIs and writes and performs unitintegration tests Performs data analysis required to troubleshoot data related issues and assist in the resolution of data issues. Assists in planning, coordinating, and executing engineering projects Supports and collaborates with other Engineers through evaluation, design analysis, and development phases Maintains knowledge, ensures competency and compliance with policies and procedures, in order to be the technical expert while collaborating with cross-functional teams Please contact Meera Makam at or email for any questions"
State Farm ®,Data Engineer,"Phoenix, AZ","Overview

We are not just offering a job but a meaningful career! Come join our passionate team!

As a Fortune 50 company, we hire the best employees to serve our customers, making us a leader in the insurance and financial services industry. State Farm embraces diversity and inclusion to ensure a workforce that is engaged, builds on the strengths and talents of all associates, and creates a Good Neighbor culture.

We offer competitive benefits and pay with the potential for an annual financial award based on both individual and enterprise performance. Our employees have an opportunity to participate in volunteer events within the community and engage in a learning culture. We offer programs to assist with tuition reimbursement, professional designations, employee development, wellness initiatives, and more!

Visit our Careers page for more information on our benefits, locations and the process of joining the State Farm team!

Responsibilities

As a Data Engineer you will:

Work closely with data scientists and business experts to develop modeling solutions for actuarial and underwriting business problems
Building and maintaining data pipelines for the development, implementation, execution, validation, monitoring, and improvement of data science solutions
Establish business domain knowledge for State Farm data sources
Investigate, recommend, and initiate acquisition of new data resources from internal and external data sources
Identify critical and emerging technologies, techniques, tools, data sources, and platforms in the data engineering field, including cloud-based solutions, that support and extend quantitative analytic deployment solutions

Success In This Job Requires

Up-to-date expertise in data engineering practices. Ability to provide solutions for the identification, acquisition, cleansing, profiling, and ETL (extracting, transformation, and loading) of data used in data science discovery and deployment solutions
Conformance with State Farm data management and governance policies
An understanding of the basics of predictive analytics/statistical models/machine learning models
Developing and maintaining an effective network of both scientific and business contacts/knowledge
Strong business acumen and the technical ability to acquire, transform and interpret complex data
Excellent communication skills and the ability to work with multiple, diverse stakeholders across business areas and leadership levels
Willingness to learn and adapt in agile development environment
Ability to learn and share new technical concepts quickly

Qualifications

We are looking for Candidates who have:

A minimum of 3 years’ relevant work experience
Bachelor’s Degree in Computer Science or a related field
AWS Certification(s)
Hands on experience working in AWS/Cloud Concepts
Experience working with P&C data
Experience with version control (e.g., GitHub, GitLab)
Hands on GitOps experience
Familiarity with one of the following programming languages: SAS, Python, R
Experience working in Hadoop, or LINUX
Experience with gathering and creating analytic business requirements, researching potential data sources (both internal and external sources), designing, developing and maintaining data assets
Familiarity with building SQL and No-SQL queries
Linux Based Containers/Dockers experience and knowledge
Kubernetes experience (deploying/hosting runners, applications)
Infrastructure as code (e.g. Terraform)
CI/CD and integration with code dependency scans
Knowledge of version control and DevOps tools such as GitLab
Applicants are required to be eligible to lawfully work in the U.S. immediately; employer will not sponsor applicants for U.S. work authorization (e.g. H-1B visa) for this opportunity*****

Office Location: Corporate office located in Bloomington, IL, OR State Farm Hubs: Richardson, TX; Dunwoody, GA; Phoenix AZ.

SFARM

#JOA

"
XLMedia PLC,Senior Data Engineer,"Austin, TX
Hybrid","Posted by

Brenda Sherwood - PRC CIR ACIR CDR CMVR CSMR CSSR 2nd

Talent Acquisition Manager at XLMedia PLC via Join Talent

Send InMail

XLMedia is seeking a Senior Data Engineer to launch and grow the US team during an exciting period of rapid growth.




Operating globally across a variety of vertical marketplaces including online gambling, sports betting, personal finance and more, XLMedia uses proprietary tools and methodologies to generate high value users for our customers in return for performance-based payment models. We own content-rich websites in 18 languages, which alongside proprietary technology and exclusive consumer data, make us one of the strongest players in the industry.




Sponsorship is not available.




Locations:

Austin, TX or Princeton, NJ

Hybrid role: remote with occasional office meetings.




Role Purpose:

Data operations management, QA / Testing, Orchestration, Automation




Areas of Responsibility & Accountability

Assist the Data Engineer Team Lead in the overall design and implementation of the data architecture, first party data and single customer view platforms, and continuous development of the Single Source of Truth.
Expanding and optimizing the current data pipeline as well as data workflows.
Building and improving data systems and ETL data for cross-functional teams.
Work with the MLOps engineer to productize models created by the data science team.
Guiding a team of Data Engineers and delivering on requirements in a timely manner.
Collaborate with the Data Team to achieve common goals, such as product development.




What the successful candidate will produce:

Data lake and warehouse
ETL pipelines
Consumable Data for Data Analysts and Data Scientists




Measures of success:

Developing and maintaining data platforms and engineering best practices
Resilient and secure data structures
Collaboration with other technical teams




This role interacts with:

Engineering, Product, and Delivery teams
Other Data teams




Experience & Qualifications

Cloud computing, Big Data platforms, analytics platforms, real-time event sourcing.
2+ years in similar capacity
Degree in Computer Science, Info tech or other technical disciplines




Knowledge & Skills

Cloud functions, Data warehouse, ETL, orchestration, complex SQL/NoSQL, Message Queues, Analytic Platforms, Python, JS.

Preferred skills:

AWS, Snowflake




#XLMedia #hiring #hiringnow #Austin #Princeton #data #bigdata #dataengineer #cloud #datawarehouse #aws #etl #sql #nosql #python #js #javascript #snowflake"
Amazon Web Services (AWS),Data Engineer - AWS FinTech,"Seattle, WA","Job Summary

DESCRIPTION

Are you passionate about data? Does the prospect of dealing with massive volumes of data excite you? Do you want to build data engineering solutions that process billions of records a day in a scalable fashion using AWS technologies? Do you want to create the next-generation tools for intuitive data access? If so, AWS Finance Technology (AWS FinTech) is for you!

AWS FinTech is seeking a Data Engineer to join the team that is shaping the future of the finance data platform. The team is committed to building the next generation big data platform that will be one of the world's largest finance data warehouse to support Amazon's rapidly growing and dynamic businesses, and use it to deliver the BI applications which will have an immediate influence on day-to-day decision making. Amazon has culture of data-driven decision-making, and demands data that is timely, accurate, and actionable. Our platform serves Amazon's finance, tax and accounting functions across the globe.

As a Data Engineer, you should be an expert with data warehousing technical components (e.g. Data Modeling, ETL and Reporting), infrastructure (e.g. hardware and software) and their integration. You should have deep understanding of the architecture for enterprise level data warehouse solutions using multiple platforms (RDBMS, Columnar, Cloud). You should be an expert in the design, creation, management, and business use of large data-sets. You should have excellent business and communication skills to be able to work with business owners to develop and define key business questions, and to build data sets that answer those questions. The candidate is expected to be able to build efficient, flexible, extensible, and scalable ETL and reporting solutions. You should be enthusiastic about learning new technologies and be able to implement solutions using them to provide new functionality to the users or to scale the existing platform. Excellent written and verbal communication skills are required as the person will work very closely with diverse teams. Having strong analytical skills is a plus. Above all, you should be passionate about working with huge data sets and someone who loves to bring data-sets together to answer business questions and drive change.

Our ideal candidate thrives in a fast-paced environment, relishes working with large transactional volumes and big data, enjoys the challenge of highly complex business contexts (that are typically being defined in real-time), and, above all, is a passionate about data and analytics. In this role you will be part of a team of engineers to create world's largest financial data warehouses and BI tools for Amazon's expanding global footprint.

Key job responsibilities

Design, implement, and support a platform providing secured access to large datasets.
Interface with tax, finance and accounting customers, gathering requirements and delivering complete BI solutions.
Model data and metadata to support ad-hoc and pre-built reporting.
Own the design, development, and maintenance of ongoing metrics, reports, analyses, dashboards, etc. to drive key business decisions.
Recognize and adopt best practices in reporting and analysis: data integrity, test design, analysis, validation, and documentation.
Tune application and query performance using profiling tools and SQL.
Analyze and solve problems at their root, stepping back to understand the broader context.
Learn and understand a broad range of Amazon’s data resources and know when, how, and which to use and which not to use.
Keep up to date with advances in big data technologies and run pilots to design the data architecture to scale with the increased data volume using AWS.
Continually improve ongoing reporting and analysis processes, automating or simplifying self-service support for datasets.
Triage many possible courses of action in a high-ambiguity environment, making use of both quantitative analysis and business judgment.


Basic Qualifications

3+ years of experience as a Data Engineer or in a similar role
Experience with data modeling, data warehousing, and building ETL pipelines
Experience in SQL
Bachelor’s degree in CS or related technical field.
5+ years of work experience with ETL, Data Modeling, and Data Architecture.
3+ years experience using big data technologies (Parquet, Spark, Hadoop, Presto, EMR, etc.)
Excellent knowledge of SQL and Linux OS
Proficiency in at least one modern programming language such as Java, Scala, or Python
Excellent understanding of software development life cycle and/or agile development environment with emphasis on BI practices.

Preferred Qualifications

Master’s degree in Information Systems or a related field.
Experience using business intelligence reporting tools (Tableau, Business Objects, Cognos, etc.)
Knowledge of data management fundamentals and data storage principles
Knowledge of distributed systems as it pertains to data storage and cloud computing
Strong problem-solving skills and ability to prioritize conflicting requirements.
Excellent written and verbal communication skills and ability to succinctly summarize key findings.
Experience working with AWS Big Data Technologies (EMR, Redshift, S3)
Strong organizational and multitasking skills with ability to balance multiple priorities.
Experience providing technical leadership and mentor other engineers for the best practices on the data engineering space.
An ability to work in a fast-paced environment where continuous innovation is occurring and ambiguity is the norm.

Amazon is committed to a diverse and inclusive workplace. Amazon is an equal opportunity employer and does not discriminate on the basis of race, national origin, gender, gender identity, sexual orientation, protected veteran status, disability, age, or other legally protected status. For individuals with disabilities who would like to request an accommodation, please visit https://www.amazon.jobs/en/disability/us.

Amazon is committed to a diverse and inclusive workplace. Amazon is an equal opportunity employer and does not discriminate on the basis of race, national origin, gender, gender identity, sexual orientation, protected veteran status, disability, age, or other legally protected status. For individuals with disabilities who would like to request an accommodation, please visit https://www.amazon.jobs/en/disability/us.


Company - Amazon.com Services LLC

Job ID: A1806209"
AArete,Data Engineer,"Los Angeles, CA","Description

Data Engineer

AArete is one of a kind when it comes to consulting firm culture.

Why AArete? We are a fast-growing strategy, operations and technology consulting firm – due to this the bar is set high at AArete. AArete is guided by our deeply embedded principles: Excellence, Passion, Loyalty to Clients, Stewardship, Family, Community, Sustainability, and Inclusion. We strive to continually improve our work environment to allow for high team member engagement and the ability for all team members to be successful.

AArete builds custom software and data analytics for Global 1000 clients in healthcare, transportation & logistics, retail, financial services, and other industries. We hire talented technologists who share our drive to extract insight from information.

A career at AArete provides variety, challenge, and opportunity for advancement. You’ll be entrusted with the success of our strategic clients and the company’s growth each and every day. We believe that any organization can succeed by enriching and empowering its people.

Exciting variety of clients and business challenges – from the largest firms in the world to the largest health plans in the country to colleges and universities
A collaborative culture focused on career and professional development, abundant opportunities with meaningful and impactful experiences.
Competitive Pay and Benefits – competitive salary, unlimited PTO, 401K match, Employee Stock Ownership Plan

At AArete we’re successfully embracing working in a flexible work environment. We have offices across the globe in Chicago, New York, Los Angeles, Dallas, Denver, DC, London and India. Based on preference, AAretians have the opportunity to work remotely, visit nearby offices or client locations.

AArete is proud to have earned a Great Place to Work Certification™. We are named in Vault’s Top 50 Firms to Work For, Crain's Chicago Business Fast 50 for a 3rd year, Inc 5000’s Fastest Growing Firms list for 4 consecutive years, Consulting Magazine's Fastest Growing Firms for the 4 consecutive years, and Forbes Magazine’s Best Consulting Firms in America list of 2021.

Work you’ll do

Responsible for discussing data extraction needs and ensuring that data extracted meets the needs of the application in order to process it
Responsible for validating that data ingested by connectors created by Java/Kafka Developers is ingested correctly and makes sense from a business perspective
Work with the account teams to ensure that after initial data is ingested, that data being displayed in the application appears appropriate and explores any anomalies that may exist
Design and document data pipelines/transformations required for each unique account connector

Requirements

An ability to learn and understand data schema and how data is processed
An understanding of how data designs work well with product
An ability to interact with and understand data extraction pipelines, to suggest changes to meet product requirement needs
Jupyter Labs
Python
OpenAPI
JUnit
Pytest
JMeter
SQL

Nice to Have

PySpark
ServiceNow
Experience with design and documentation of ETL pipelines"
Dice,Python Data Engineer,"Portland, OR
On-site","Dice is the leading career destination for tech experts at every stage of their careers. Our client, Nextogen Inc., is seeking the following. Apply via Dice today!

Role Python Data Engineer Location Portland, OR (Remote till Covid) Duration Long Term Job Description Python Data Engineer with pipeline development and testing. Experience AWS and Jenkins, and comfort with CICD. ETL experience is strongly preferred. Exposure to ML or Data scienceAnalytics is desirable. Thanks Regards. SureshLead RecruiterNextogen Inc Direct Email mailto Web www.nextogen.com httpwww.nextogen.com"
NCR Corporation,Data Visualization Engineer,"Atlanta, GA","About NCR

NCR Corporation (NYSE: NCR) is a leading software- and services-led enterprise provider in the financial, retail and hospitality industries. NCR is headquartered in Atlanta, Georgia, with 36,000 employees globally. NCR is a trademark of NCR Corporation in the United States and other countries.

Data Visualization Engineer

Locations – Atlanta, GA, Omaha, NE Dallas,TX Or Redwood City, CA

NCR D3 is searching for a highly innovative, enthusiastic and results-driven Data Visualization Engineer who has built data visualizations and data analytics systems at scale using the AWS and GCP architectures. This individual should be from a development background in data engineering and not an infrastructure background and should have experience in the financial and banking markets. Someone who has a strong familiarity working in an AWS or GCP cloud environment to implement enterprise data visualizations. Comfortable working with full stack engineers, product managers and product delivery teams.

Position Summary & Key Areas Of Responsibility

The Digital Banking team at NCR is looking for Data Visualization Engineers to develop our next generation Digital Banking Data Reporting Platform.

You will build visualization application using new generation tools and technologies like Looker, Power BI, React, and d3.js to induct data from various systems to provide efficient reporting and analytics capability.

You will translate complex business requirements into scalable technical solutions, and design dashboards or visualization using BI tools to perform data analysis and to support business.

Collaborate with multiple cross functional teams such as product management, solution architectures, security, and software engineering.

NCR's Digital Banking solutions are a leading Software-as-a-Service (""SaaS"") platform for financial institutions in the United States. The Digital Banking team in Redwood City, California is looking for a Data Visualization Engineer to participate in the development of our next generation Digital Banking Data Platform.

As a Data Visualization Engineer, you will build and design highly performant visualization platform using new generation tools and technologies like Looker inducting data from BigQuery in Google Cloud Platform to provide efficient reporting and analytics capability. You will translate complex business requirements into scalable technical solutions, and design dashboards or visualization using BI tools to perform data analysis and to support the business. You will collaborate with multiple cross functional teams such as Product Management, Solution Architecture, Security, and Software Engineering.

Basic Qualifications
Bachelor’s degree in a technical discipline or equivalent work experience
2+ years experience in designing and developing visualization using BI tools like Looker, Tableau, or PowerBI
Strong understanding of data modeling, data structures and algorithms
Experience with all aspects of software development life cycle (source control, continuous integration, deployments, etc.)
Use exploration and analytic tools like Jupyter Notebooks to probe and validate data
Participate in and follow Agile methodology best practices
Collaborate with product owners and stakeholders to plan and define requirements
Ability to translate data needs into detailed functional and technical designs for development, testing and implementation
Keep up to date with technology and apply new knowledge
Experience with the following software/tools is required
Github, Jenkins, Gitflow, Github Projects
One or more of the following: ReactJS, Power BI, Tableau, Looker and D3 JS
Experience with the following software/tools is a plus
Preferred Qualifications
Knowledge of React and d3.js is a plus
AWS Services:
RDS, AWS Lambda, AWS Glue, Apache Spark, Kafka, Kinesis, Firehose
SQL and NoSQL databases like MySQL, Oracle, Postgres, Elasticsearch, MongoDB Atlas, Athena, DynamoDB
GCP Services:
DataProc, DataFlow, Cloud Composer, Apache Beam, Apache Spark, Kafka, PubSub
SQL and NoSQL databases:
MySQL, Oracle, Postgres, Elasticsearch, MongoDB Atlas, BigQuery, BigTable Strong programming skills in at least one of the following
Ideal Qualifications

Bachelor’s degree in computer science, software engineering or proven work experience in related field
2+ years, preferred 4+ years production AWS and or GCP experience
AWS development with a firm understanding of the use of AWS Glue, GCP DataProc, DataFlow, Kafka Firehose, Kinesis, AWS Lambda, S3, DynamoDB, BigQuery, BigTable, MongoDB Atlas, Kafka
Solid understanding of Microservice and big data architectures for the use of security, machine learning and analytics
Experience implementing and achieving PCI compliance in a cloud environment
Solid Python and or Java programming language experience
Relational and NoSQL database experience is required
Strong complex problem solving and troubleshooting skills
Ability to learn quickly and manage time effectively
Proven written and oral communication skills
Experience in the financial industry a plus

Offers of employment are conditional upon passage of screening criteria applicable to the job.

Full Time Employee Benefits Include

Medical Insurance
Dental Insurance
Life Insurance
Vision Insurance
Short/Long Term Disability
Paid Vacation
401k

EEO Statement

Integrated into our shared values is NCR's commitment to diversity and equal employment opportunity. All qualified applicants will receive consideration for employment without regard to sex, age, race, color, creed, religion, national origin, disability, sexual orientation, gender identity, veteran status, military service, genetic information, or any other characteristic or conduct protected by law. NCR is committed to being a globally inclusive company where all people are treated fairly, recognized for their individuality, promoted based on performance and encouraged to strive to reach their full potential. We believe in understanding and respecting differences among all people. Every individual at NCR has an ongoing responsibility to respect and support a globally diverse environment.

Statement to Third Party Agencies

To ALL recruitment agencies: NCR only accepts resumes from agencies on the NCR preferred supplier list. Please do not forward resumes to our applicant tracking system, NCR employees, or any NCR facility. NCR is not responsible for any fees or charges associated with unsolicited resumes."
Fitch Ratings,Senior Data Engineer,"New York, NY
On-site","At Fitch, we have an open culture where employees are able to exchange ideas and perspectives, throughout the organization, irrespective of their seniority. Your voice will be heard allowing you to have a real impact. We embrace diversity and appreciate authenticity encouraging an environment where employees can be their true selves. Our inclusive and progressive approach helps us to keep a balanced perspective. Fitch is also committed to supporting its employees by advancing conversations around diversity, equity and inclusion. Fitch’s Employee Resource Groups (ERGs) have been established by employees who have joined together as a workplace community based on similar backgrounds or life experiences. Fitch’s ERGs are available to connect employees with others within the organization to offer professional and personal support.

With our expertise, we are not only creating data and information, but also producing timely insights from every angle to influence decision making in this ever changing and highly competitive market. We have a relentless hunger to innovate and unlock the power of human insights and to drive value for our customers. There has never been a better time to make an impact and we invite you to join us on this journey.

Fitch Ratings is a leading provider of credit ratings, commentary and research. Dedicated to providing value beyond the rating through independent and prospective credit opinions, Fitch Ratings offers global perspectives shaped by strong local market experience and credit market expertise. The additional context, perspective and insights we provide have helped fund a century of growth and enables you to make important credit judgments with confidence.

Job Summary

The Fitch Group Chief Data Office (CDO) is seeking a Data Engineer to be part of a team supporting all business and support functions across Fitch for a wide range of cloud based big solutions. This is a formative opportunity to influence and innovate as part of the new CDO function. In this role, the solution architect will partner across business, technology, enterprise architecture and operations to design and act as a subject matter expert for Fitch’s new cutting edge cloud data platform.

Responsibilities

Create and maintain optimal data pipeline architecture.
Assemble large, complex data sets that meet functional / non-functional business requirements.
Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.
Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL and AWS ‘big data’ technologies.
Build analytics tools that utilize the data pipeline to provide actionable insights into customer acquisition, operational efficiency and other key business performance metrics.
Work with stakeholders including the Executive, Product, Data and Design teams to assist with data-related technical issues and support their data infrastructure needs.
Keep our data separated and secure across national boundaries through multiple data centers and AWS regions.
Create data tools for analytics and data scientist team members that assist them in building and optimizing our product into an innovative industry leader.
Work with data and analytics experts to strive for greater functionality in our data systems.

Qualifications
Advanced working SQL knowledge and experience working with relational databases, query authoring (SQL) as well as working familiarity with a variety of databases.
Experience building and optimizing ‘big data’ data pipelines, architectures and data sets.
Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement.
Strong analytic skills related to working with unstructured datasets.
Build processes supporting data transformation, data structures, metadata, dependency and workload management.
A successful history of manipulating, processing and extracting value from large disconnected datasets.
Working knowledge of message queuing, stream processing, and highly scalable ‘big data’ data stores.
Experience supporting and working with cross-functional teams in a dynamic environment.
We are looking for a candidate with 5+ years of experience in a Data Engineer role, who has attained a Graduate degree in Computer Science, Statistics, Informatics, Information Systems or another quantitative field. They should also have experience using the following software/tools:
Experience with big data tools: Hadoop, Spark, Kafka, etc.
Experience with relational SQL and NoSQL databases, including Postgres and Cassandra.
Experience with data pipeline and workflow management tools: Azkaban, Luigi, Airflow, etc.
Experience with AWS cloud services: EC2, EMR, RDS, Redshift
Experience with stream-processing systems: Kafka, Spark-Streaming, etc.
Experience with object-oriented/object function scripting languages: Python, Java, C++, Scala, etc.

#ZR

#DICE

Fitch Group is a global leader in financial information services with operations in more than 30 countries. Wholly owned by the Hearst Corporation, we are comprised of three main businesses: Fitch Ratings | Fitch Solutions | Fitch Learning.

For more information please visit our websites: | |

Fitch is committed to providing global securities markets with objective, timely, independent and forward-looking credit opinions. To protect Fitch’s credibility and reputation, our employees must take every precaution to avoid conflicts of interests or any appearance of a conflict of interest. Should you be successful in the recruitment process at Fitch Ratings you will be asked to declare any securities holdings and other potential conflicts prior to commencing employment. If you, or your immediate family, have any holdings that may conflict with your work responsibilities, you may be asked to divest yourself of them before beginning work.

Fitch is an equal opportunity employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, disability, protected veteran status, sexual orientation, gender expression, gender identity or any other characteristic protected by law.

UNITED STATES ONLY: As part of its continued efforts to maintain a safe workplace for employees, Fitch requires that all employees who receive a written offer of employment on or after October 4, 2021 be fully vaccinated (as defined by the CDC) against the coronavirus by the first day of employment as a condition of employment, to the extent permitted by applicable law. Fitch will consider requests for reasonable accommodations due to medical and/or religious reasons on an individual basis in accordance with applicable legal requirements.

"
Nabors Industries,Senior Data Science Engineer,"Houston, TX
On-site","

Company Overview

Nabors is a leading provider of advanced technology for the energy industry. With operations in about 20 countries, Nabors has established a global network of people, technology and equipment to deploy solutions that deliver safe, efficient and responsible hydrocarbon production. By leveraging its core competencies, particularly in drilling, engineering, automation, data science and manufacturing, Nabors aims to innovate the future of energy and enable the transition to a lower carbon world.

Nabors is committed to providing equal employment opportunities to all employees and applicants and prohibiting discrimination and harassment of any type without regard to race, religion, age, color, sex, national origin, disability status, genetics, protected veteran status, sexual orientation, gender identity or expression, or any other characteristic protected by federal, state or local laws. This applies to all terms and conditions of employment including recruiting, hiring, placement, promotion, termination, layoff, recall, transfer, leaves of absence, compensation, and training. To learn more about our Fair Employment practices, please refer to the Nabors Code of Conduct.

JOB SUMMARY

Sr Data Science Engineer will help discover the information hidden in vast amounts of data and help make smarter decisions to deliver even better products. The primary focus will be in applying data mining techniques, doing statistical analysis, and building high quality prediction systems integrated with drilling products. The ideal candidate is adept at using large data sets to find opportunities for product and process optimization and using models to test the effectiveness of different courses of action in the drilling and/or oil gas industry. Candidate must have strong experience using a variety of data mining/data analysis methods, using a variety of data tools, building and implementing models, using/creating algorithms and creating/running simulations. Candidate must have a proven ability to drive business results with their data-based insights. Candidate must be comfortable working with a wide range of stakeholders and functional teams. The right candidate will have a passion for discovering solutions hidden in large data sets and working with stakeholders to improve business outcomes.

DUTIES AND RESPONSIBILITIES


Work with stakeholders throughout the organization to identify opportunities for leveraging company data to drive business solutions.
Mine and analyze data from company databases to drive optimization and improvement of product development and business strategies.
Develop custom data models and algorithms to apply to data sets.
Use predictive modeling and machine learning algorithms to optimize business processes and thereby generate revenue for the company.
Assess and enhance data collection procedures to include information that is relevant for building analytic systems
Doing ad-hoc analysis and presenting results in a clear manner
Creating automated anomaly detection systems and constant tracking of its performance
Develop company A/B testing framework and test model quality.
Co-ordinate with different functional teams to implement models and monitor outcomes.
Develop processes and tools to monitor and analyze model performance and data accuracy.Develop applications (programming, coding)
Ability to multitask and work well independently and in a team environment
Ability to generate research ideas


Desired Skills and Experience


MINIMUM QUALIFICATIONS/SKILLS


6-8 years of experience manipulating data sets and building Machine Learning models
Master’s or PHD in Statistics, Mathematics, Computer Science or another quantitative field, and is familiar with the following software/tools:
Coding knowledge and experience with several languages: Python, C, C++, Java, JavaScript, etc.
Knowledge and experience in statistical and data mining techniques: GLM/Regression, Random Forest, Boosting, Trees, text mining, social network analysis, etc.
Knowledge of a variety of machine learning techniques (clustering, decision tree learning, artificial neural networks, etc.) and their real-world advantages/drawbacks.
Experience querying databases and using statistical computer languages: R, Python, etc. o Experience working with Oil and Gas drilling data and time series data is preferable.
Experience using web services: REST API, SOAP, WCF etc.
Experience creating and using advanced machine learning algorithms and statistics: regression, simulation, scenario analysis, modeling, clustering, decision trees, neural networks, etc.
Proficiency using query languages like SQL, Hive etc.
Experience with distributed data/computing tools: Hadoop, Hive, Spark, Oracle, MySQL, Cosmos DB, Mongo DB etc.
Experience visualizing/presenting data for stakeholders using MicroStrategy, StreamBase, SpotFire, PowerBI etc
Strong problem solving skills with an emphasis on product development.
Experience using statistical computer languages (R, Python, etc.) to manipulate data and draw insights from large data sets.
Knowledge of advanced statistical techniques and concepts (regression, properties of distributions, statistical tests and proper usage, etc.) and experience with applications.
Excellent written and verbal communication skills for coordinating across teams.
A drive to learn and master new technologies and techniques.



PHYSICAL REQUIREMENTS / WORKING CONDITIONS


Office environment
Occasional travel to rig yards and field
While performing the duties of this job, the employee is frequently required to sit. The employee is occasionally required to stand, walk and use hands. The employee may occasionally lift and/or move up to 20 pounds. Specific vision abilities required by this job include close vision and distance vision"
"KCF Technologies, Inc.",Data Analytics Engineer,"State College, PA
Remote","
KCF Technologies is an exciting and rapidly growing technology company dedicated to putting innovative solutions to work in the Industrial World. SmartDiagnostics, our IIOT Technology, fuels our mission to solve the worlds machine health problems to drive safety and sustainability for our communities. To accomplish this epic technical revolution, we integrate our core values into our everyday actions and consistently challenge the status quo! Learn more at www.kcftech.com.
At KCF, we value autonomy, working smart, and doing things a little bit differently. If youre an A-player who wants to be part of one of the most important companies revitalizing machine health across the world, KCF is the place for you. If our values resonate with you, please keep reading!
Core Values:
Smarts: We are humbly aggressive lifelong learners.
Grit: We are scrappy, proactive problem solvers who dont stop until the job is done.
Drive: We demonstrate an insatiable hunger to serve others.
Responsibility: We do the right thing and contribute to the greater good.
Autonomy: We own our work and define how we do it, while aligned with the greater mission.


Where You Come In:
KCF Technologies is looking for a talented and motivated Data Analytics Engineer to help develop computer algorithms and models trained to ingest sensor and machine process data and diagnose fault conditions, prognose time-to-failure, provide recommended actions, and perform root-cause analysis.
We are looking for an individual who not only has an aptitude for data analytics, statistical analysis, and pattern recognition, but also has a passion for learning and understanding how things work. You consider yourself to be independently motivated and an analytically deductive problem solver, with a factual and to-the-point communication style. Youre a case matter expert on things that draw your interest, in addition to being extremely disciplined with perfectionist tendencies.
Our ideal candidate exemplifies our cultural values of Smarts, Grit, and Drive, and considers him or herself to be:
Self-motivated
Passionate for solving real-world problems
Hard-working
Adaptable
Detail-oriented
Accepting of the unknown


If this sounds like you, we encourage you to keep reading!

Essential Functions:
Design and implement algorithms tailored to industrial machine health diagnostics, prognostics, and root-cause analysis
Produce models for edge and/or cloud (AWS) execution
Integrate analytics solutions within the KCF SMARTdiagnostics platform
Construct data mining tools for acquiring training and validation data sets



Qualifications:
Bachelors degree in Computational Science; Statistics; Computer, Mechanical, or Industrial Engineering; or similar field of study
Strong mathematical background (linear algebra, calculus, probability and statistics)
Machine Learning experience (regression and classification, supervised, and unsupervised learning)
At least 2 years of experience in machine health
Strong algorithm design skills
Programming experience with Python (Numpy, Scipy, Scikit-Learn, Matplotlib, TensorFlow, etc.), MATLAB, R, or similar languages
Communicates verbally and in writing in a clear and professional manner
Able to work in a rapid-paced environment, managing and tracking multiple tasks with speed and accuracy
Highly service-oriented disposition with aptitude in problem-solving
Must exemplify the following KCF cultural values: Smarts, Grit, Drive
Strong organizational, time management, and prioritization abilities
Should be able to deal with difficult, sensitive, and confidential issues




Perks & Benefits:
At KCF, we are committed to providing best-in-class benefits, engaging development opportunities, and powerful perks that are focused on bringing out the best in you:
100% company-paid Medical, Dental, and Vision premiums
Health Savings Account with a generous annual employer contribution
Hybrid work model for most positions, work from home, work from anywhere
Competitive compensation & bonus opportunities
Four weeks PTO; Paid Holidays
401(k) with KCF match
Wellness Perk- including annual reimbursement program
Monthly cell and office expense stipend
Learning Culture committed to growth and continuous development



At KCF, we are an equal opportunity employer. The only things we require for employment, compensation, advancement and benefits are performance and a good team attitude. No one will be denied opportunities or benefits, and no employment decisions will be made, on the basis of race, religion/creed, national origin, ancestry, sex, sexual orientation, gender, gender identity, age, disability that does not prohibit performance of essential job functions, protected veteran status, medical condition, marital status, pregnancy, genetic information, possession of a general education development certificate (GED) as compared to a high school diploma, or any other characteristic protected by applicable federal or state laws. KCF complies with applicable state and local laws governing nondiscrimination in employment in every location in which KCF has facilities.
PI168213348"
OrionIG,Data Engineer,"Salt Lake City, UT
Remote","Position: Data Engineer

org: Engineering

Target Salary: DOE + great benefits

Term: Full Time - Remote




We are a SaaS company, is part of the family of industry-leading information exchange and communication management

solutions for healthcare. We give dental practices and insurance plans and payers the ability to exchange health

information in an efficient manner to help improve their revenue cycle management processes.




The Data Engineer will support our product, sales, support, leadership and marketing teams with insights gained from analyzing company data. Has a passion for discovering solutions hidden in large data sets and working with stakeholders to improve business outcomes.




Primary Job Responsibilities:

Work with stakeholders throughout the organization to identify opportunities for leveraging company data to drive business solutions.
Mine and analyze data from company databases to drive optimization and improvement of product development, marketing techniques and business strategies.
Assess the effectiveness and accuracy of new data sources and data gathering techniques.
Develop custom data models and algorithms to apply to data sets.
Use predictive modeling to increase and optimize customer experiences, revenue generation, ad targeting and other business outcomes.
Other projects and duties as assigned.







Education, Knowledge and Skills

Bachelor’s Degree in a Technical or Business Field.
3+ years of Data Analysis, Business Intelligence, or Report Writing Experience.
Strong problem-solving skills with an emphasis on product development.
Strong experience using SQL and statistical computer languages (R, Python, Ruby, etc.) to manipulate data and draw insights from large data sets.
Experience working with and creating data architectures.
Knowledge of a variety of machine learning techniques (clustering, decision tree learning, artificial neural networks, etc.) and their real-world advantages/drawbacks.
Knowledge of advanced statistical techniques and concepts (regression, properties of distributions, statistical tests and proper usage, etc.) and experience with applications.
Excellent written and verbal communication skills for coordinating across teams."
Vail Systems,Data Engineer II,"Deerfield, IL
Hybrid","Data Engineer II




Who You Are




Interested in building a data transformation layer into the architecture of a product to aggregate and calculate items more efficiently?

As a Data Engineer, you will support and maintain the infrastructure that blends our Call Detail Record (CDR) system’s data with customer data for key clients. You’ll focus on data aggregation and transformation within the context of our overall data pipeline by utilizing machine learning techniques, Python data transformations, and messaging and queueing with Apache Kafka. You would also support a Microsoft SQL Server environment.




Develop, construct, test, and maintain data architectures to align with business requirements provided by clients including creating data tables and partitions
Write code in Python to perform data transformations and maintain automate systems
Create and deploy sophisticated analytics programs utilizing machine learning and other statistical methods for data analysis to resolve internal and customer requests
Develop data set processes for automation and report generation
Refine and automate other processes, track issues, troubleshoot, and document changes
Identify methods to improve data reliability, efficiency, and quality
Prepare and clean data for predictive and prescriptive modeling
Configure, monitor, and maintain database servers and processes
Assist software engineers with query tuning and schema refinement
Provide occasional on-call support for critical production systems
Present project status updates and insights to stakeholders based on analytics in weekly team meetings




Qualifications:

We encourage you to apply if you think your experience may be a match, even if you do not meet all of the qualifications.

A Bachelor’s degree in Computer Science, or a related degree
3+ years of experience in a database administration role (SQL or MySQL)
Proficient at developing T-SQL procedures
Experience with performance tuning and optimization
Experience with data transfer with SFTP, file compression and encryption
Knowledge of high availability and disaster recovery for SQL Server
Excellent verbal and written communication skills
Experience working with engineers in a highly technical environment
Knowledge of data archiving techniques, preferred
Experience with data visualization tools (e.g., Tableau), preferred
Experience with NoSQL technology, such as MongoDB and Hadoop, preferred
Experience with other programming/scripting languages, such as Java, Python, preferred
Familiarity with machine learning methods and techniques, preferred




Who We Are

At Vail, we believe in the unique power of voice interactions to create more expressive, more intimate, and more efficient interpersonal interactions. Using Vail technology, we make millions of voice interactions better every day. We process around 10% of all toll-free call traffic in the U.S; 1 in 10 times when someone calls a customer support hotline Vail is routing or interacting with that call.




We are rapidly growing across multiple dimensions, including our customer base, the scope of products we offer, and the size of our team. Now is the right time for a strong candidate to join and grow with us. We have a supportive culture where employees are encouraged to achieve both personal and team goals because we believe growth leads to both business impact and personal fulfillment.

We offer competitive compensation and affordable benefits with flexibility and choice to meet individual and family (including Domestic Partnerships) needs including:

Multiple medical, dental, and vision plan options
Company-paid life insurance, short and long-term disability
401(k) retirement savings plan with company match every pay period (50% on first 6% of employee contribution)
30 days PTO
Annual Bonus Program
Paid maternity and paternity leave
Relocation allowance
Employee referral bonus
Gym membership
Access to LinkedIn Learning




We are striving to implement and sustain an inclusive and equitable work environment for all employees by sourcing underrepresented groups and continually empowering those individuals within our organization to further enrich Vail’s communication solutions. We recognize that equitable and unique individuals benefit our teams’ problem-solving, innovation and development efforts.




COVID-19

Vail’s offices are located in Deerfield, IL, and Chicago, IL. We will continue to interview and onboard employees remotely during this unprecedented time. We are not requiring employees to come into our offices at this time. Once the pandemic ends, we will observe a hybrid work arrangement giving employees continued flexibility to come into the office based on personal, team, and business needs."
Cisco,Data Engineer,"Austin, TX","Posted by

Peter A. Ernst

Technical Recruiter at Cisco

Send InMail
Who You Are

Data Engineers are passionate about enabling a data driven approach to optimization by sourcing, maintaining and ensuring the availability of data used to drive full life cycle marketing insights to optimize CX’s marketing investments and the customer experience. They integrate both batch and streaming approaches to support standard business intelligence, as well as decision automation and machine learning requirements. Data engineers help make data much easier to understand and consume for others and have managed to adjust to new technologies quickly.

What you’ll do

Provide leadership and support for development of a coordinated Customer Experience data foundation that will enable extensive business intelligence and machine learning for CX Platform and extended user communities.
Work with CX business and IT teams to ensure high quality, timely deliverables that meet usability, scalability, quality and performance standards.
Provide hands-on technical support for development, research and quality assurance testing.
Apply a variety of big data technologies to ingest, transform, index, aggregate, correlate, provide API's, visualize and enable a spectrum of organizations across Cisco.
Analyze structural requirements for data storage solutions and software
Design architectures for technical systems as to ensure robustness, scalability, and completeness


We are seeking high energy and qualified candidates who possess the following skills and experience

Minimum Bachelor’s in computer science, Mathematics, Engineering, Statistics, or related field
Preferred Masters/PhD in Computer science, Mathematics, Engineering, Statistics, or related field
4-6 years of experience working with very large datasets, ETL processing and Machine learning pipelines
Minimum 3 years’ experience in SQL/NoSQL required
Experience in Hadoop, Python/R, Spark and Data Warehousing required atleast 2 years
Experience with Map Reduce, AWS/Azure/Cloud experience preferred
Who you'll work with

You will work cross functionally with the Customer Experience, Sales, Marketing, and IT organizations, playing a leadership role in redefining Cisco by developing and implementing analytic models and intelligent automation to drive us toward a data-driven digital organization.

DX’s digital expertise makes us uniquely qualified to address the evolving expectations of today’s connected customers and partners, along with Cisco sellers. Using real-time connected data, machine learning, and automation; the team enables Cisco sellers and partners to deliver a powerful, personalized experience—throughout the entire customer lifecycle. DX is passionate about providing customers with an immersive digital experience with Cisco. This in turn drives improvements in recurring revenue, cost savings and sales efficiency for Cisco and its partners.

Why Cisco

We connect everything people, processes, data, and things. We innovate everywhere, taking ambitious risks to craft the technologies that give us smart cities, connected cars, and handheld hospitals. And we do it in style with outstanding personalities who aren’t afraid to change the way the world works, lives, plays and learns.

We are leaders with vision, tech enthusiasts, pop culture aficionados, and we even have a few purple haired hard workers. We celebrate the creativity and diversity that fuels our innovation. We are futurists and we are doers.

We Are Cisco

"
Dice,Data Engineer,"Plano, TX","Our client, a leader in their industry, needs a Data Engineer to join their team in Plano.

We can facilitate w2 and corp-to-corp consultants. For our w2 consultants, we offer a great benefits package that includes Medical, Dental, and Vision benefits, 401k with company matching, and life insurance.

Responsibilities of the Data Engineer

Support adherence to data quality management principles

Apply open sourcedigital technologies to mine data from various sources and platforms

Build data solutions, tools, and capabilities to enable self-service frameworks for data consumption

Build and execute tools to monitor and report on data quality

Collaborate with technical teams to manage security and data access governance

Requirements of the Data Engineer

Understanding of SOA, HTTP, and REST

Scripting experience in Python, R, Spark, and SQL

Strong desire and experience with data in various forms - data warehousesSQL, unstructured data

Experience utilizing and developing within AWS services

Java, Spring, Maven

Microservices development

Job ID 365788

About Eliassen Group

Eliassen Group provides strategic consulting and talent solutions to drive our clients' innovation and business results. Our purpose is to positively impact the lives of our employees, clients, consultants, and the communities in which we operate. Leveraging over 30 years of success, our expertise in talent solutions, life sciences consulting, Agile consulting, cloud services, risk management, business optimization, and managed services enables us to partner with our clients to execute their business strategy and scale effectively. Headquartered in Reading, MA, and with offices from coast to coast, Eliassen Group offers local community presence and deep networks, as well as national reach.

Eliassen Group is an Equal OpportunityAffirmative Action Employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, pregnancy, sexual orientation, gender identity, national origin, age, protected veteran status, or disability status.

Don't miss out on our referral program! If we hire a candidate that you refer us to then you can be eligible for a 1,000 referral check!"
Dice,Data Engineer,"Dallas, TX
On-site","Dice is the leading career destination for tech experts at every stage of their careers. Our client, Clearbridge Technology Group, is seeking the following. Apply via Dice today!

Our client, a large manufacturing company with offices in Dallas TX, and Burlington, MA is in need of a Data Analytics Platform Engineer to work onsite for a 3-month contract to hire position. The Platform Engineer is a key role on the Analytics team, with a focus on evaluating, analyzing and maintaining the tools and technologies necessary to support analytics reporting solutions for various business groups across our clientrsquos organization. This person will be using cloud data warehousing technologies like Snowflake, data modelling tools (such as Atscale) and ETL tools like Informatica Cloud. This person will provide analytical and technical support to project teams and cross-functional business partners, acting as the technical point of contact for Strategic Insights Platform analytics tools and technologies. This person will quickly learn new technologies in analytics and business processes and should collaborate well with the business and technical teams in rolling out analytics solutions across the organization. Other duties Provide Analytics thought leadership in Project and Support paradigm to technical teams and key business stakeholders in defining technology solutions and services to enable the business strategy and goals to be achieved for the supported functional business area. Analyze Analytics solutionrsquos design and architecture and suggest improvements as necessary. Document, develop and implement best practices for Analytics platform in collaboration with delivery and support teams. Ensure delivery teams follow our clientrsquos best practices. Plan and implement Analytics Data platform strategy and roadmaps. Define and deliver the roadmap for continuous improvement of Analytics Data platform tools, technologies and processes. Work closely with 3rd party consulting partners to ensure high-quality on-time deliverables. Ensure architecture review and change control processes are being followed and all approvals and applicable documentation have been complete before production rollout. Required Skills At least 5 years of Data Analytics Platform Experience Experience with Informatica Intelligent Cloud Services (IICS) Prior experience with Snowflake Strong SQL coding, datawarehousing and ETL skills. Proven ability to develop both technical and leadership skills within a team framework. Broad exposure to emerging Analytics trends and technologies. Strong ability to communicate complex technical concepts in simple terms, per both verbally and in writing. A team player with the ability to sell change effectively and influence others with a can-do attitude and strong presentation skills."
Roblox,Search and Discovery - Principal Data Engineer,"San Mateo, CA
On-site","Every day, tens of millions of people from around the world come to Roblox to play, learn, work, and socialize in immersive digital experiences created by the community. Our vision is to build a platform that enables shared experiences among billions of users. This is what’s known as the metaverse: a persistent space where anyone can do just about anything they can imagine, from anywhere in the world and on any device. Join us and you’ll usher in a new category of human interaction while solving exceptional challenges that you won’t find anywhere else.

As an S&D Software Engineer (focused on Data) you will work on various big data systems used by S&D. These include batch and streaming pipelines, feature stores, recommendation systems, search engine indexing. The systems we build together will help Roblox do Search and Discovery better which will make users happier. Additionally, we will grow the team and our systems together.

People in this role should expect challenges requiring a variety of skillsets including: building microservices and pipelines, managing data infrastructure, data analysis, and scaling machine learning workloads.

We Will:
Define and build the Data Infra for S&D workflows
Build tooling to interact with S&D data systems
Build batch and streaming pipelines that populate S&D data stores such as the Signal Platform
Develop complex backend systems used in serving search and recommendations

You Are

9+ years experience working on data heavy applications.
Fluent in SQL and another modern programming language such as Java, Python, C#, and more.
An expert on big data tools/frameworks such as Hadoop, Spark, Hive, Kafka, Flink, and more.
An expert on building and scaling microservices.
Someone who will report into the Search and Discovery Organization.

You May Have

Worked with ETL orchestration tools like Airflow, Luigi, etc.
Familiarity with Elasticsearch.
Worked with Terraform or similar tools to manage IaC.

Equal Employment Opportunity statement below.

You’ll Love

Industry-leading compensation package
Excellent medical, dental, and vision coverage
A rewarding 401k program
Flexible vacation policy
Roflex - Flexible and supportive work policy
Roblox Admin badge for your avatar
At Roblox HQ:
Free catered lunches
Onsite fitness center and fitness program credit
Annual CalTrain Go Pass"
STAND 8 Technology Services,Data Engineer,"Los Angeles County, CA
Remote","We are hiring a Data Engineer. This is a great opportunity to work with an iconic entertainment brand. You will be comfortable working with structured and unstructured datasets and automating data pipelines to ingest, analyze, validate, normalize and clean data in a hybrid multi-cloud environment. The position will require a strong background in cloud architecture, data modeling, quality assurance, data governance, and experience with the tools to implement these solutions.
STAND 8 provides end to end IT solutions to enterprise partners across the United States and with offices in LA, Atlanta, New York, Raleigh, and more.
This is the perfect opportunity to become a part of a fast paced and innovative team that leverages data, reports, dashboard and machine learning to solve real world problems and drive business value.
Key Roles And Responsibilities

Design, develop, and maintain reliable automated data solutions based on the identification, collection and evaluation of business requirements. Including but not limited to data models, database objects, stored procedures and views
Automate data pipelines to ingest, transform, analyze, validate, normalize and clean data.
Implements statistical data quality procedures on new data sources.
Conduct system monitoring across cloud and on-prem environments
Develop and maintain data engineering best practices and provides thought leadership.
Work with Security teams and Legal to create data policies and develop interfaces and retention models which requires synthesizing or anonymizing data.
Develop basic reports, charts, tables, and other visual aids in support of findings.
Implementation of RESTful API’s supporting system integrations
Advise business partners with regards to patterns and relationships in data to recommend business direction or outcomes.
Experience

Preferred: SQL, Snowflake, Vertica, Teradata, AWS and Azure.
Bonus: Python, Databricks (Spark), Hortonworks
Good understanding of internal business segments or stakeholders and good presentation skills.
3-5 years related experience in data engineering/automation, relational database structures, data modeling and data quality.Education
Preferred advanced college degree or masters in Computer Science and or Statistics, Mathematics.
This is a remote position that can be located anywhere in the United States"
"Akraya, Inc.",Data Engineer (Scala Programming),"San Mateo, CA
On-site","Posted by

Aman G.

Happy Recruiter :) | Technology Enthusiast | Coffee Lover | Good listener | To know more lets connect | Expert in Creative Hiring for UX Researcher Quantitative/ Qualitative/ accessibility/ Usability/ DEI/ UI Designer

Primary Skills: Java, Scala, SQL, RDBMS, ETL pipelines

ContractType: W2 / C2C

Duration: 6+ month extendable

Location: Remote




To follow up with any questions, please contact Aman at 408-907-3219




JOB DESCRIPTION

Good programming skills in java, Scala preferred.
Strong in algorithms, data structures
Experience in spark, hive and parquet
Experience in Object Oriented Programming.
Experience in SQL, RDBMS
Strong grasp of design patterns, coding best practices and be able to write unit-testable code.
Create and maintain optimal data pipeline architecture,
Assemble large, complex data sets that meet functional / non-functional business requirements.

JOB REQUIREMENTS

Either Java / Scala / Python (Scala over Java over Python)
System design experience , especially in data applications.
Ability to solve data problems – creating ETL pipelines using Spark, Hive, RedShift data store
Ability to deliver project end to endusing data skills
SQL scripting

Level of experience – 4 to 5 years of relevant data engineering experience"
Unum,Data Engineer II,"Portland, ME","Job Posting End Date: April 14

For more than 75 years, Colonial Life & Accident Insurance Company has had one mission: to help America’s workers preserve and protect the vitally important things they work so hard to build.

Headquartered in Columbia, South Carolina, we offer a wide range of financial protection options, helping more than 3.7 million people in over 86,000 companies. In addition to our personal benefits counseling expertise, we’re a pioneer of payroll deduction and innovator of enrollment technology. Our end-to-end capabilities and commitment to going above and beyond enable us to provide unmatched customer service. With more than 10,000 sales representatives and more than 1,000 home office professionals, we’re here to help during life's most challenging times.

General Summary

General Summary

This position for an experienced data Engineer who will design, build and maintain data systems and pipelines to allow data to be accessed and utilized optimally for analytics or visualization/insight generation purposes. The role requires significant technical expertise in the fields of computer programming, database management and data architecture; and relies on increasing business knowledge. With minimal review by a manager, this individual will take initiative in identifying and executing data engineering approaches to support business priorities.

Principal Duties And Responsibilities

Employ a variety of languages and tools to develop, construct, test and maintain data pipelines, ensuring they support the requirements of the business.
Integrate large volume of data from different sources (including DB2, SQL Server, Web API and Teradata).
Apply validation, aggregation, and reconciliation techniques to create a rich data framework.
Work closely with the data scientists and business partners to understand the business problem they are trying to solve and the analytics solutions they plan to apply. Use this understanding to create appropriate data structures tailored for the specific problem.
Create data assets that conform to scalability, extensibility, performance and maintainability requirements for the problem at hand. Promotes development of solutions following appropriate engineering process that is fit to purpose for different use scenarios.
Understand and contribute to the evolution of the enterprise data architecture including the application of current and emerging data frameworks and tools (eg hosting data in Cloud).
Efficiently prepares results for interpretation and/or visualization and communicates findings and potential value to influence manager and leadership decision making.
Support integration of solutions within existing business processes using automation techniques.
Understand theory and application of current and emerging software engineering practices.
Provides support, training and/or mentorship to lower level Data Engineer peers.
Perform other related duties as assigned

Job Specifications

Bachelor’s degree in quantitative field is required, Master’s is preferred
4 years of professional experience or equivalent relevant work experience preferred
Core Data Engineer Capabilities: Knowledge in all of the following skillsets and deep expertise in at least two:
Software Engineering: Expertise in at least one relevant object-oriented programing language (i.e. Java/Scala, Python). Experience in DevOps best practice including CI/CD, process automation and optimization.
Data Architecture and Infrastructure: Good understanding of data architecture principles and related infrastructure requirements, covering on-prem and Cloud platforms.
Holistic Data Preparation: The ability to understand and present data in the appropriate context, including a good understanding how the data will build towards a business solution.
Data Extraction, Transform & Load: Preferred skills include: Expertise in writing complex SQL queries that join multiple tables/databases. Independently explore databases/tables or other legacy data content to identify best data sources to solve business problems. Demonstrates ability to troubleshoot complex SQL queries with little guidance. Demonstrates ability to create logical data models by combining data from multiple sources including internal and external data
Core business capabilities: Demonstrated communication skills, experience in financial services, leadership experience working with senior management and executive leadership, and attention to detail while effectively and independently prioritizing work and managing multiple projects simultaneously. Demonstrated ability to understand and explain a problem and identify and communicate an appropriate solution.
Leadership Capabilities: Demonstrated ability to coach or mentor team members, ability to commit quickly and positively to change. Viewed as a promoter of change management and leads proof of concept work and prototyping when necessary
Preferred characteristics: Entrepreneurial self-starter, a thorough, results-oriented problem-solver, and a lifelong learner with voracious curiosity AND intermediate understanding of their organization

~IN1

Headquartered in Columbia, South Carolina, we offer a wide range of financial protection options, helping more than 3.7 million people in over 86,000 companies. In addition to our personal benefits counseling expertise, we’re a pioneer of payroll deduction and innovator of enrollment technology. Our end-to-end capabilities and commitment to going above and beyond enable us to provide unmatched customer service. With more than 10,000 sales representatives and more than 1,000 home office professionals, we’re here to help during life's most challenging times.

Colonial Life is an equal opportunity employer, considering all qualified applicants and employees for hiring, placement, and advancement, without regard to a person's race, color, religion, national origin, age, genetic information, military status, gender, sexual orientation, gender identity or expression, disability, or protected veteran status.

Company

Unum

In accordance with Colorado’s Equal Pay For Equal Work Act, if you are a resident of the state of Colorado and wish to review the salary information for a role posted in Colorado, please fill out the form below and our Unum HR representative will reply to your inquiry within three business days.

Colorado’s Equal Pay For Equal Work Act

In accordance with New York City Human Rights Law, if you wish to review the salary information for a role posted in New York City, please fill out this form and our Unum HR representative will reply to your inquiry within three business days.

New York City Human Rights Law

"
Nesco Resource,Data Analytics Engineer,"New York, NY","Job Description

As a Data Analytics Engineer you will:

Be responsible for collaboratively building data analytics pipelines and dashboards for multiple stakeholders. This includes

Translating high-level product requirements into technical requirements
Identifying new data we need and developing ingestion processes
Clarifying business rules and implementing them in code
Transforming data using SQL to power our analytics products
Writing automated tests, documenting your work, and reviewing others' work
Building Tableau visualizations to make data actionable
Teaching stakeholders how to use your products and incorporating their feedback

You Have

Experience using git, complex SQL, and AWS to build and deploy data products
Experience creating visualizations in Tableau
Ability to work cross-functionally to iteratively develop data products and processes
Experience analyzing data from digital marketing, CRM, SIS or LMS tools
Experience using dbt and AWS Athena (Bonus)

Nesco Resource and affiliates (Lehigh G.I.T Inc, and Callos Resource, LLC) is an equal employment opportunity employer and does not discriminate on the basis of race, color, religion, sex, sexual orientation, gender identity, national origin, disability, age, or veteran status, or any other legally protected characteristics with respect to employment opportunities."
Zoetis Inc.,Data Engineer- Sales and Marketing Data Specialist,"Parsippany, NJ","Role Description



This role has been created to US Commercial teams. The candidate will be responsible for shaping the core data capabilities that drive critical use cases within the organization, such as revenue generation tactics and strategies to detect and prevent potential customer defection. This candidate will also support data quality and governance initiatives such as master data management and assist the data engineering team in applying advanced analytics to both internal and external data sources to solve complex business problems. The role will provide the candidate with an exceptionally broad view of every aspect of Zoetis' business, while serving as critical support for the US Petcare division.



To be successful, the ideal candidate will demonstrate an advanced technical expertise, along with a thirst for knowledge and natural curiosity to enable rapid learning of the Zoetis business model and data infrastructure at a scale to support a growing $2b region. The position involves working with structured and unstructured real-world data, requiring a candidate who is comfortable thinking independently about solving business problems. Candidates must be self-motivated, detail-oriented, can work with limited supervision, and must be comfortable in an environment of changing priorities. Candidates should have a strong business acumen, with evidence of delivered business value, preferably in a commercial setting







POSITION RESPONSIBILITIES



Design, develop, test, and implement database solutions related to optimal data pipeline architecture and infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL Server, Oracle, Sales Force and Big Data technologies.
Develop, construct, test and maintain NGSE commercial and customer data layers to enable modeling
Collaborate with and enable data scientists on the team to rapidly iterate on model building and deployment
Assists in data quality, integrity, and documentation efforts in support of data governance initiatives
Automate processes that increase productivity for the NGSE team
Develop and implement controls to ensure data integrity and regulatory compliance (SOX).
Package and support deployment of releases of new code within our Azure environment.
Drive innovation within the group in areas including development efficiencies within Databricks, and leading-edge industry concepts and developments.
Mentor and guide other team members on data engineering skillsets
Monitor the production schedule and provide support to remediate job failures.
Provide support to team members when they need a solution to a complex issue.
Provide production support to business users.
Administer the Tableau environments.





EDUCATION AND EXPERIENCE



*Master's from an accredited college/university in Computer Science, Information Technology, Electrical Engineering, or related field



*Minimum of 2 years of professional experience working as a Data Engineer



*Deep experience building deployed data pipelines / ETL



*Expertise in designing, building and maintaining large-scale databases



*Deep acuity in leveraging the latest in big data and cloud infrastructure methodologies



*Demonstrated ability to learn and apply new concepts and technologies



*Experience working directly with data scientists to build and deploy predictive and optimization models, including assisting in feature engineering



*Excellent interpersonal, written, and verbal communication skills; demonstrated ability to effectively communicate complex ideas to both technical and non-technical audiences



*Fluency in English required



*Expertise in database design, creation, and maintenance







TECHNICAL SKILLS REQUIREMENTS



*Knowledge of latest data infrastructure technologies, including cloud-based (e.g. Amazon S3, Redshift, Azure Storage) and distributed file systems (HDFS)



*Deep knowledge of SQL; experience with SQL Server a plus



*Experience in large-scale/distributed computing and analytics (Hadoop, Spark)



*Familiarity with predictive model building in Python (Pandas/PySpark) / R



*Experience with source control and collaboration tools (e.g. Git)



*Software development / engineering experience a plus



*Experience in scaled model testing, integration, and deployment (DevOps) a plus



*Experience working in Microsoft tech stack preferred

"
Dice,Data Engineer,"San Francisco, CA
On-site","Dice is the leading career destination for tech experts at every stage of their careers. Our client, Agile Datapro, Inc, is seeking the following. Apply via Dice today!

Hiring Data Engineer for Fortune Biotech Client Immediate Interview Start W2 Only NO C2C Role Data Engineer Location San Francisco, CA(Remote) Duration Long Term Description The ideal candidate will have a general knowledge of the underlying scientific principles applied to the development and manufacture of biopharmaceuticals. They will have a keen interest in learning bioprocess operations. Some knowledge of statistical process control and data analysis techniques Proficiency in data analysis techniques and a willingness to learn new techniques is desired. Experience with the following technologies Distributed Processing (Spark, Hadoop, EMR), traditional RDBMS (MS SQL Server, Oracle, MySQL, PostgreSQL), MPP (AWS Redshift, Teradata), NoSQL (MongoDB, DynamoDB, Cassandra, Neo4J, Titan) Experience translating scientific or engineering workflows into automated systems. 3 years of work experience with Python, C, Javascript, or other programming language. 1 years of work experience with SQL and data manipulation. Ability to work across structured, semi-structured, and unstructured data, extracting information and identifying linkages across disparate data sets Experience and interest in Cloud platforms Experience in traditional data warehousing and deploying ETL processes with Python. Familiarity with agile ways of working e.g. in SCRUM-teams. If you are interested to pursue this opportunity, please send your updated resume to mailto"
Tanium,Data Engineer,"Emeryville, CA","The Basics: 

As a Data Engineer at Tanium, you’ll be responsible and accountable for all aspects of Tanium’s data warehouse and associated data pipelines. Along these lines, you will be directly involved in data governance and business enablement as well as strategic planning for data architecture and underlying tooling strategies. This also includes the design, development and upkeep of engagement strategies and technical resources necessary to support decentralized BI functions operating across Tanium. 

What you’ll do: 

Own and be accountable for the health, architecture, and usage of Tanium’s Enterprise Data Warehouse (Snowflake)
Design, maintain, configure, scale, and optimize optimal data pipelines and related infrastructure, ensuring reliable & optimal extraction, transformation and loading of data from various data sources 
Identify, design, and implement internal process improvements including optimizing data delivery, automating manual processes, and improving governance and data quality 
Work with leadership to develop, enhance and support a self-service model for decentralized analytics teams 
Work with key stakeholders to polish and enhance the collection and calculation of key business performance metrics, in addition to addressing data-related technical issues 
Maintain a culture of transparency, accountability, and efficiency within the Business Systems organization and with all stakeholders 
Align and execute compliance mandates as necessary, including GDPR, CMMC and execution of ISO27001 policies and procedures 
Conduct research and make recommendations on products and services 

We’re looking for someone with: 

Education 

Bachelor’s Degree in Computer Science, Engineering or Business or equivalent degree and experience

Experience  

5+ years in a Data Engineering role with exposure to data architecture and pipeline design, management and/or integration responsibilities 
Expertise in Snowflake 
5+ years of experience with relational NoSQL and SQL databases
Expertise with workflow management and pipeline tools 
Minimum of 3 years’ experience with data warehousing and data modelling
Attention to detail and strong understanding of how data solves for analytical and operational use cases in Marketing, Sales, Finance, Product development Skilled in collecting, documenting, and prioritizing requirements in an Agile framework, and responding to input from multiple sources (customers, testers, engineering, competitive products, management, etc.)

These Will Be a Plus

Hands-on experience with Fivetran, Dell Boomi, DBT/DBT Cloud and/or Airflow 
Experience with AWS and AWS tools/products
Hands-on experience with PowerBI
Familiarity with Kanban and Agile methodology 
Google CPDE or IBM CDE certifications are a plus 

About Tanium  

Tanium offers an endpoint management and security platform built for the world’s most demanding IT environments. Many of the world’s largest and most sophisticated organizations —  including nearly half of the Fortune 100, top retailers and financial institutions, and multiple branches of the U.S. Armed Forces — rely on Tanium to make confident decisions, operate efficiently, and remain resilient against disruption. Tanium has been named to the Forbes Cloud 100 list of “Top 100 Private Companies in Cloud Computing” for five consecutive years and ranks 4th on FORTUNE’s list of the “Best Workplaces in Technology 2020.”  

On a mission. Together.  

At Tanium, we are stewards of a culture that emphasizes the importance of collaboration, respect, and diversity. In our pursuit of revolutionizing the way some of the largest enterprises and governments in the world solve their most difficult IT challenges, we are strengthened by our unique perspectives and by our collective actions.    

We are an organization with stakeholders around the world and it’s imperative that the diversity of our customers and communities is reflected internally in our team members. We strive to create a diverse and inclusive environment where everyone feels they have opportunities to succeed and grow because we know that only together can we do great things.  

Taking care of our team members  

Each of our team members has 5 days set aside as volunteer time off (VTO) to contribute to the communities they live in and give back to the causes they care about most. "
Booz Allen Hamilton,Data Engineer,"Arlington, VA
On-site","Job Number: R0135955

Data Engineer

The Challenge:

Do you want to work at the forefront of advanced technology and solve complex data challenges? You know that data yields pivotal insights when it’s gathered from disparate sources and organized. As a data engineer, you have the chance to develop and deploy the pipelines and platforms that make this data meaningful. What’s more, you’ll have the chance to help grow Booz Allen’s DataOps capabilities while working with a multi-disciplinary team of analysts, data engineers, data scientists, developers, and data consumers in a fast-paced, agile environment. We’re looking for someone like you to work with our clients and meet their mission by keeping the warfighter safe, supporting national security, and protecting civilians.

This is an opportunity to implement data engineering activities on some of the most mission-driven projects in the industry. Supporting advanced analytics efforts, you’ll have the chance to architect data systems, build out ETL pipelines, write custom code, and perform data ingestion. From sharing your skills in analytical exploration and examination of data to supporting the assessment, design, building, and maintenance of scalable platforms, you’ll work with our clients to solve their most pressing challenges.

Ready to help drive innovation using cutting-edge data tools and techniques?

Join us. The world can’t wait.

You Have:

2+ years of experience within data engineering, software engineering, data science, or machine learning
1+ years of experience in developing and deploying data ETL systems on and with AWS technologies
Experience with object-oriented programming languages, including Java or Python
Experience in working with IC data sets and NoSQL databases, including ElasticSearch or HBase
Experience with using AWS datastores, including RDS Postgres, S3, or DynamoDB
Experience in working with pub or sub messaging technology, including Apache Kafka
Secret clearance
Bachelor's degree


Nice If You Have:

Experience with data engineering tools, including Scrapy, Kubernetes, or UI Path
TS/SCI clearance
Master's degree


Clearance:

Applicants selected will be subject to a security investigation and may need to meet eligibility requirements for access to classified information; Secret clearance is required.

Build Your Career:

At Booz Allen, we know the power of analytics and we’re dedicated to helping you grow as a data analysis professional. When you join Booz Allen, you’ll have the chance to:

access online and onsite training in data analysis and presentation methodologies, and tools like Hortonworks, Docker, Tableau, and Splunk
change the world with the Data Science Bowl—the world’s premier data science for social good competition
participate in partnerships with data science leaders, like our partnership with NVIDIA to deliver Deep Learning Institute (DLI) training to the federal government


You’ll have access to a wealth of training resources through our Analytics University, an online learning portal specifically geared towards data science and analytics skills, where you can access more than 5000 functional and technical courses, certifications, and books. Build your technical skills through hands-on training on the latest tools and state-of-the-art tech from our in-house experts. Pursuing certifications that directly impact your role? You may be able to take advantage of our tuition assistance, on-site bootcamps, certification training, academic programs, vendor relationships, and a network of professionals who can give you helpful tips. We’ll help you develop the career you want as you chart your own course for success.

We’re an EOE that empowers our people—no matter their race, color, religion, sex, gender identity, sexual orientation, national origin, disability, veteran status, or other protected characteristic—to fearlessly drive change."
