{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "16241f46",
   "metadata": {},
   "source": [
    "# Scrape LinkedIn's Job Postings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "72788493",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import re\n",
    "from dotenv import load_dotenv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "import pyautogui\n",
    "import time\n",
    "import copy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27593d94",
   "metadata": {},
   "source": [
    "### Load username and password from your .env file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b1154319",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Save LinkedIn username and password to variables\n",
    "linkedin_user=os.environ['LINKEDIN_USER']\n",
    "linkedin_pass=os.environ['LINKEDIN_PASS']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "380134fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "====== WebDriver manager ======\n",
      "Current google-chrome version is 99.0.4844\n",
      "Get LATEST chromedriver version for 99.0.4844 google-chrome\n",
      "Driver [/Users/WonderWolff/.wdm/drivers/chromedriver/mac64/99.0.4844.51/chromedriver] found in cache\n"
     ]
    }
   ],
   "source": [
    "# Open browser to LinkedIn.com\n",
    "browser = webdriver.Chrome(service=Service(ChromeDriverManager().install()))\n",
    "browser.get(\"https://www.linkedin.com\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2f72314",
   "metadata": {},
   "source": [
    "### Login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3cb65cb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Auto-input username and password on login page\n",
    "username = browser.find_element(By.ID, \"session_key\")\n",
    "username.send_keys(linkedin_user)\n",
    "password = browser.find_element(By.ID, \"session_password\")\n",
    "password.send_keys(linkedin_pass)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "38e5d859",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Click login button\n",
    "login_button = browser.find_element(By.CLASS_NAME, \"sign-in-form__submit-button\")\n",
    "login_button.click()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "682bcd94",
   "metadata": {},
   "source": [
    "### Define our helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "25cd4405",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 8 µs, sys: 3 µs, total: 11 µs\n",
      "Wall time: 16.9 µs\n"
     ]
    }
   ],
   "source": [
    "# Scroll through all job search results to \"load\" them.\n",
    "# This is necessary to grab their details below, as LinkedIn\n",
    "# does not load all 25 job openings at once, \n",
    "# only once you scroll through the page.\n",
    "def load_full_page():\n",
    "    \"\"\"\n",
    "    LinkedIn needs a user to scroll to load all 25 jobs.\n",
    "    This accomplishes the loading automatically. \n",
    "    \"\"\"\n",
    "    i = 1\n",
    "    while i < 25:\n",
    "        try:\n",
    "            element = browser.find_element(By.CLASS_NAME, \"global-footer-compact\")\n",
    "        except NoSuchElementException:\n",
    "            element = browser.find_element(By.CLASS_NAME, \"jobs-search-two-pane__pagination\")\n",
    "        browser.execute_script(\"arguments[0].scrollIntoView();\", element)\n",
    "        time.sleep(0.1)\n",
    "\n",
    "        job_lists = browser.find_element(By.CLASS_NAME, \"jobs-search-results__list\")\n",
    "        jobs = job_lists.find_elements(By.CLASS_NAME, 'job-card-list__title')\n",
    "        every_other_5_list = jobs[::i]\n",
    "        for element in every_other_5_list:\n",
    "            browser.execute_script(\"arguments[0].scrollIntoView();\", element)\n",
    "            time.sleep(0.3)\n",
    "\n",
    "        i += 3\n",
    "        \n",
    "    return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f53cb8f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get company name\n",
    "def get_company_name(browser, company_names):\n",
    "    \"\"\"\n",
    "    Get the 25 company names listed on \n",
    "    the LinkedIn jobs page.\n",
    "    \"\"\"\n",
    "    # Save the <ul> HTML tag holding the job openings by\n",
    "    # searching for its specific class name\n",
    "    company_lists = browser.find_element(By.CLASS_NAME, \"jobs-search-results__list\")\n",
    "    \n",
    "    # Get an iterable list of all <li> tags holding\n",
    "    # each individual job opening\n",
    "    companies = company_lists.find_elements(By.CLASS_NAME, 'artdeco-entity-lockup__subtitle')\n",
    "    \n",
    "    # Iterate through each job opening\n",
    "    for i in companies:\n",
    "        # Append each company name to a list\n",
    "        company_names.append(i.text)\n",
    "    \n",
    "    # Print results\n",
    "    print(\"Company Names:\")\n",
    "    print(company_names, \"\\n\")\n",
    "    print(\"Length of list:\", len(company_names))\n",
    "    print()\n",
    "    \n",
    "    return company_names\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d858b29c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get job titles\n",
    "def get_job_titles(browser, job_title):\n",
    "    \"\"\"\n",
    "    Get the 25 job titles listed on \n",
    "    the LinkedIn jobs page.\n",
    "    \"\"\"\n",
    "    # Save the <ul> HTML tag holding the job openings by\n",
    "    # searching for its specific class name\n",
    "    job_lists = browser.find_element(By.CLASS_NAME, \"jobs-search-results__list\")\n",
    "    \n",
    "    # Get an iterable list of all <li> tags holding\n",
    "    # each individual job opening\n",
    "    jobs = job_lists.find_elements(By.CLASS_NAME, 'job-card-list__title')\n",
    "    \n",
    "    # Iterate through each job opening\n",
    "    for i in jobs:\n",
    "        # Append each job title to a list\n",
    "        job_title.append(i.text)\n",
    "    \n",
    "    # Print results\n",
    "    print(\"Job Titles:\")\n",
    "    print(job_title, \"\\n\")\n",
    "    print(\"Length of list:\", len(job_title))\n",
    "    print()\n",
    "    \n",
    "    return job_title\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e058b384",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get locations\n",
    "def get_location(browser, location):\n",
    "    \"\"\"\n",
    "    Get the 25 job locations listed on \n",
    "    the LinkedIn jobs page.\n",
    "    \"\"\"\n",
    "    # Save the <ul> HTML tag holding the job openings by\n",
    "    # searching for its specific class name\n",
    "    location_lists = browser.find_element(By.CLASS_NAME, \"jobs-search-results__list\")\n",
    "\n",
    "    # Get an iterable list of all <li> tags holding\n",
    "    # each individual job opening\n",
    "    each_item = location_lists.find_elements(By.CLASS_NAME, 'jobs-search-results__list-item')\n",
    "    \n",
    "    # Iterate through each job opening\n",
    "    for item in each_item:\n",
    "        # For each job posting, grab the first element containing\n",
    "        # the class name below, which will give us location\n",
    "        i = item.find_element(By.CLASS_NAME, 'job-card-container__metadata-wrapper')\n",
    "        \n",
    "        # Append each location to a list\n",
    "        location.append(i.text)\n",
    "    \n",
    "    # Print results\n",
    "    print(\"Location:\")\n",
    "    print(location, \"\\n\")\n",
    "    print(\"Length of list:\", len(location))\n",
    "    print()\n",
    "    \n",
    "    return location\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c708744a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get job descriptions\n",
    "def get_descriptions(browser, description):\n",
    "    \"\"\"\n",
    "    Get the 25 job descriptions listed on \n",
    "    the LinkedIn jobs page.\n",
    "    \"\"\"\n",
    "    # Save the <ul> HTML tag holding the job openings by\n",
    "    # searching for its specific class name\n",
    "    description_lists = browser.find_element(By.CLASS_NAME, \"jobs-search-results__list\")\n",
    "    \n",
    "    # Get an iterable list of all <li> tags holding\n",
    "    # each individual job opening\n",
    "    job_descriptions = description_lists.find_elements(By.CLASS_NAME, 'jobs-search-results__list-item')\n",
    "    \n",
    "    # Iterate through each job opening\n",
    "    for i in job_descriptions:\n",
    "        # Click on an individual job opening to \n",
    "        # display its job description\n",
    "        ac = ActionChains(browser)\n",
    "        ac.move_to_element_with_offset(i, 2, 2).click().perform()\n",
    "        time.sleep(0.2)\n",
    "        \n",
    "        # Scrape the job description\n",
    "        element = browser.find_element(By.CLASS_NAME, 'jobs-description__content')\n",
    "        \n",
    "        # Append job description to list\n",
    "        description.append(element.get_attribute(\"innerText\"))\n",
    "        \n",
    "    # Print results\n",
    "    print(\"Description list length:\")\n",
    "    print(len(description))\n",
    "    print()\n",
    "    \n",
    "    return description"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "459457ad",
   "metadata": {},
   "source": [
    "### Now define our looping scraper function to scrape the first 10 pages of a position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2ca59bb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scrape first 10 pages\n",
    "def repeat_scrape_until_10th_page(browser, position):\n",
    "    \"\"\"\n",
    "    Scrape first 10 pages of a position's job openings.\n",
    "    \"\"\"\n",
    "    # Define our lists\n",
    "    company_names=[]\n",
    "    job_title=[]\n",
    "    location=[]\n",
    "    description=[]\n",
    "    \n",
    "    # Navigate to /jobs/ page\n",
    "    browser.get(f\"https://www.linkedin.com/jobs/search/?keywords={position}&location=united%20states\")\n",
    "    \n",
    "    page = 1\n",
    "    \n",
    "    # Loop through first 10 pages\n",
    "    for i in range(1, 11):\n",
    "        \n",
    "        # Rest between page loads so the server \n",
    "        # doesn't shut us out\n",
    "        time.sleep(i)\n",
    "        if i != 1:\n",
    "            page = (i-1)*25\n",
    "            browser.get(f'https://www.linkedin.com/jobs/search/?keywords={position}&location=united%20states&start={page}')\n",
    "        \n",
    "        # Give page 2 seconds to load\n",
    "        time.sleep(2)\n",
    "        load_full_page()\n",
    "        company_names = get_company_name(browser, company_names)\n",
    "        job_title = get_job_titles(browser, job_title)\n",
    "        location = get_location(browser, location)\n",
    "        description = get_descriptions(browser, description)\n",
    "        \n",
    "    scraped_list = [company_names, job_title, location, description]\n",
    "    \n",
    "    return scraped_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "926ad78e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "410a6cee",
   "metadata": {},
   "source": [
    "### Begin looking for jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "436d1999",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List the \"job search terms\" in HTML format \n",
    "# to iterate through\n",
    "search_terms = [\n",
    "    \"data%20scientist\",\n",
    "    \"machine%20learning%20engineer\",\n",
    "    \"data%20engineer\",\n",
    "    \"web%20developer\",\n",
    "    \"frontend%20developer\",\n",
    "    \"backend%20developer\", \n",
    "    \"devops\",\n",
    "    \"software%20engineer\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c982290",
   "metadata": {},
   "source": [
    "### Collect jobs and store in dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e2e9aa5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Company Names:\n",
      "['Amazon Web Services (AWS)', 'Meta', 'Criteria Corp', 'US Tech Solutions', 'SecureLink', 'Amazon Web Services (AWS)', 'Autodesk', 'Google', 'The Guitar Center Company', 'nCino, Inc.', 'Houzz', 'Hinge', 'DAOHQ', 'Relativity Space', 'Eventbrite', 'Instagram', 'Amazon', 'Levvel, an Endava company', 'HelloFresh', 'Reddit, Inc.', 'Blockchain.com', 'Dice', 'Benco Dental', 'RPX Corporation', 'iQor'] \n",
      "\n",
      "25\n",
      "Job Titles:\n",
      "['Senior Data Scientist', 'Data Scientist, Analytics', 'Data Scientist', 'Data Scientist - Optimization', 'Data Scientist', 'Data Scientist, ProServe', 'Senior Data Scientist / Machine Learning Engineer, eCommerce', 'Data Scientist, Revenue Acceleration, Google Cloud', 'Data Scientist II', 'Sr. Data Scientist', 'Product Data Scientist', 'Data Scientist', 'Data Scientist', 'Data Scientist', 'Staff Data Scientist', 'Data Scientist, Instagram Creator Monetization', 'Data Scientist', 'Python Developer- Data Scientist', 'Senior Data Scientist, Forecasting', 'Senior Data Scientist, Experimentation', 'Data Scientist', 'Data Scientist', 'Data Scientist/Senior Data Scientist', 'Principal Data Scientist, NLP', 'Data Scientist'] \n",
      "\n",
      "25\n",
      "Location:\n",
      "['Seattle, WA', 'Fremont, CA', 'West Hollywood, CA\\nRemote', 'United States\\nRemote', 'Nashville, TN\\nRemote', 'Seattle, WA', 'Los Angeles, CA', 'Los Angeles, CA\\nOn-site', 'Westlake Village, CA', 'Salt Lake City Metropolitan Area\\nHybrid', 'Iowa, United States', 'New York, NY', 'Miami, FL\\nRemote', 'Long Beach, CA', 'United States\\nRemote', 'San Francisco, CA', 'Seattle, WA', 'Charlotte, NC\\nRemote', 'New York, NY\\nOn-site', 'Chicago, IL\\nRemote', 'San Francisco, CA', 'Denver, CO\\nOn-site', 'United States\\nRemote', 'San Francisco Bay Area\\nHybrid', 'St Petersburg, FL\\nRemote'] \n",
      "\n",
      "25\n",
      "Description list length:\n",
      "25\n",
      "Company Names:\n",
      "['Amazon Web Services (AWS)', 'Meta', 'Criteria Corp', 'US Tech Solutions', 'SecureLink', 'Amazon Web Services (AWS)', 'Autodesk', 'Google', 'The Guitar Center Company', 'nCino, Inc.', 'Houzz', 'Hinge', 'DAOHQ', 'Relativity Space', 'Eventbrite', 'Instagram', 'Amazon', 'Levvel, an Endava company', 'HelloFresh', 'Reddit, Inc.', 'Blockchain.com', 'Dice', 'Benco Dental', 'RPX Corporation', 'iQor', 'Amazon Web Services (AWS)', 'Houzz', 'Meta', 'Dice', 'Amgen', 'Lenovo', 'IQVIA', 'Alkami Technology', 'Tesla', 'Meta', 'Rivian', 'HelloFresh', 'Amazon Web Services (AWS)', 'Deloitte', 'Glocomms', 'DUPR (Dreamland Universal Pickleball Rating)', 'Deloitte', 'HelloFresh', 'Madison Square Garden Sports Corp.', 'Microsoft', 'Pfizer', 'Whiterock.ai', 'Altus Group', 'Meta', 'Collabera Inc.'] \n",
      "\n",
      "50\n",
      "Job Titles:\n",
      "['Senior Data Scientist', 'Data Scientist, Analytics', 'Data Scientist', 'Data Scientist - Optimization', 'Data Scientist', 'Data Scientist, ProServe', 'Senior Data Scientist / Machine Learning Engineer, eCommerce', 'Data Scientist, Revenue Acceleration, Google Cloud', 'Data Scientist II', 'Sr. Data Scientist', 'Product Data Scientist', 'Data Scientist', 'Data Scientist', 'Data Scientist', 'Staff Data Scientist', 'Data Scientist, Instagram Creator Monetization', 'Data Scientist', 'Python Developer- Data Scientist', 'Senior Data Scientist, Forecasting', 'Senior Data Scientist, Experimentation', 'Data Scientist', 'Data Scientist', 'Data Scientist/Senior Data Scientist', 'Principal Data Scientist, NLP', 'Data Scientist', 'Data Scientist - AWS Infrastructure', 'Product Data Scientist', 'Research Data Scientist', 'Applied Data Scientist', 'Data Scientist', 'Data Scientist', 'Data Scientist', 'Data Scientist', 'Data Scientist/Software Engineer, Reliability Engineering', 'Data Scientist, Product Analytics - VR Devices (FRL)', 'Senior Data Scientist', 'Senior Data Scientist, Forecasting', 'AWS AI Labs - Applied Scientist and Research Engineers', 'NLP Data Scientist - Python / R - Top Secret', 'Senior Data Scientist', 'Data Scientist', 'NLP Data Scientist - Python / R - Top Secret', 'Senior Data Scientist, Forecasting', 'Basketball Data Scientist (remote opportunity)', 'Data & Applied Scientist', 'Statistical Data Scientist', 'Data Scientist', 'Data Scientist', 'Research Data Scientist (University Grad)', 'Data Scientist'] \n",
      "\n",
      "50\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [27]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Loop through all search terms\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m position \u001b[38;5;129;01min\u001b[39;00m search_terms:\n\u001b[1;32m      3\u001b[0m     \n\u001b[1;32m      4\u001b[0m     \u001b[38;5;66;03m# Find and scrape positions\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m     scraped_list \u001b[38;5;241m=\u001b[39m \u001b[43mrepeat_scrape_until_10th_page\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbrowser\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mposition\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;66;03m# Define dataframe-creating function\u001b[39;00m\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate_linkedin_dataframe\u001b[39m(scraped_list):\n",
      "Input \u001b[0;32mIn [21]\u001b[0m, in \u001b[0;36mrepeat_scrape_until_10th_page\u001b[0;34m(browser, position)\u001b[0m\n\u001b[1;32m     27\u001b[0m     company_names \u001b[38;5;241m=\u001b[39m get_company_name(browser, company_names)\n\u001b[1;32m     28\u001b[0m     job_title \u001b[38;5;241m=\u001b[39m get_job_titles(browser, job_title)\n\u001b[0;32m---> 29\u001b[0m     location \u001b[38;5;241m=\u001b[39m \u001b[43mget_location\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbrowser\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocation\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     30\u001b[0m     description \u001b[38;5;241m=\u001b[39m get_descriptions(browser, description)\n\u001b[1;32m     32\u001b[0m scraped_list \u001b[38;5;241m=\u001b[39m [company_names, job_title, location, description]\n",
      "Input \u001b[0;32mIn [19]\u001b[0m, in \u001b[0;36mget_location\u001b[0;34m(browser, location)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m each_item:\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;66;03m# For each job posting, grab the first element containing\u001b[39;00m\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;66;03m# the class name below, which will give us location\u001b[39;00m\n\u001b[1;32m     15\u001b[0m     i \u001b[38;5;241m=\u001b[39m item\u001b[38;5;241m.\u001b[39mfind_element(By\u001b[38;5;241m.\u001b[39mCLASS_NAME, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mjob-card-container__metadata-wrapper\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 16\u001b[0m     location\u001b[38;5;241m.\u001b[39mappend(\u001b[43mi\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtext\u001b[49m)\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLocation:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28mprint\u001b[39m(location, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Bloomtech/underdog-devs-ds-a/venv/lib/python3.8/site-packages/selenium/webdriver/remote/webelement.py:77\u001b[0m, in \u001b[0;36mWebElement.text\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;129m@property\u001b[39m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtext\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[1;32m     76\u001b[0m     \u001b[38;5;124;03m\"\"\"The text of the element.\"\"\"\u001b[39;00m\n\u001b[0;32m---> 77\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mCommand\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mGET_ELEMENT_TEXT\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[0;32m~/Bloomtech/underdog-devs-ds-a/venv/lib/python3.8/site-packages/selenium/webdriver/remote/webelement.py:710\u001b[0m, in \u001b[0;36mWebElement._execute\u001b[0;34m(self, command, params)\u001b[0m\n\u001b[1;32m    708\u001b[0m     params \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m    709\u001b[0m params[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_id\n\u001b[0;32m--> 710\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_parent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Bloomtech/underdog-devs-ds-a/venv/lib/python3.8/site-packages/selenium/webdriver/remote/webdriver.py:423\u001b[0m, in \u001b[0;36mWebDriver.execute\u001b[0;34m(self, driver_command, params)\u001b[0m\n\u001b[1;32m    420\u001b[0m         params[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msessionId\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msession_id\n\u001b[1;32m    422\u001b[0m params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_wrap_value(params)\n\u001b[0;32m--> 423\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcommand_executor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdriver_command\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    424\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m response:\n\u001b[1;32m    425\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39merror_handler\u001b[38;5;241m.\u001b[39mcheck_response(response)\n",
      "File \u001b[0;32m~/Bloomtech/underdog-devs-ds-a/venv/lib/python3.8/site-packages/selenium/webdriver/remote/remote_connection.py:333\u001b[0m, in \u001b[0;36mRemoteConnection.execute\u001b[0;34m(self, command, params)\u001b[0m\n\u001b[1;32m    331\u001b[0m data \u001b[38;5;241m=\u001b[39m utils\u001b[38;5;241m.\u001b[39mdump_json(params)\n\u001b[1;32m    332\u001b[0m url \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_url\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mpath\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 333\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand_info\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Bloomtech/underdog-devs-ds-a/venv/lib/python3.8/site-packages/selenium/webdriver/remote/remote_connection.py:355\u001b[0m, in \u001b[0;36mRemoteConnection._request\u001b[0;34m(self, method, url, body)\u001b[0m\n\u001b[1;32m    352\u001b[0m     body \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    354\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkeep_alive:\n\u001b[0;32m--> 355\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    356\u001b[0m     statuscode \u001b[38;5;241m=\u001b[39m resp\u001b[38;5;241m.\u001b[39mstatus\n\u001b[1;32m    357\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/Bloomtech/underdog-devs-ds-a/venv/lib/python3.8/site-packages/urllib3/request.py:74\u001b[0m, in \u001b[0;36mRequestMethods.request\u001b[0;34m(self, method, url, fields, headers, **urlopen_kw)\u001b[0m\n\u001b[1;32m     71\u001b[0m urlopen_kw[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrequest_url\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m url\n\u001b[1;32m     73\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m method \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_encode_url_methods:\n\u001b[0;32m---> 74\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest_encode_url\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     75\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfields\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfields\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43murlopen_kw\u001b[49m\n\u001b[1;32m     76\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     78\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequest_encode_body(\n\u001b[1;32m     79\u001b[0m         method, url, fields\u001b[38;5;241m=\u001b[39mfields, headers\u001b[38;5;241m=\u001b[39mheaders, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39murlopen_kw\n\u001b[1;32m     80\u001b[0m     )\n",
      "File \u001b[0;32m~/Bloomtech/underdog-devs-ds-a/venv/lib/python3.8/site-packages/urllib3/request.py:96\u001b[0m, in \u001b[0;36mRequestMethods.request_encode_url\u001b[0;34m(self, method, url, fields, headers, **urlopen_kw)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m fields:\n\u001b[1;32m     94\u001b[0m     url \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m?\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m urlencode(fields)\n\u001b[0;32m---> 96\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mextra_kw\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Bloomtech/underdog-devs-ds-a/venv/lib/python3.8/site-packages/urllib3/poolmanager.py:375\u001b[0m, in \u001b[0;36mPoolManager.urlopen\u001b[0;34m(self, method, url, redirect, **kw)\u001b[0m\n\u001b[1;32m    373\u001b[0m     response \u001b[38;5;241m=\u001b[39m conn\u001b[38;5;241m.\u001b[39murlopen(method, url, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n\u001b[1;32m    374\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 375\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mu\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest_uri\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    377\u001b[0m redirect_location \u001b[38;5;241m=\u001b[39m redirect \u001b[38;5;129;01mand\u001b[39;00m response\u001b[38;5;241m.\u001b[39mget_redirect_location()\n\u001b[1;32m    378\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m redirect_location:\n",
      "File \u001b[0;32m~/Bloomtech/underdog-devs-ds-a/venv/lib/python3.8/site-packages/urllib3/connectionpool.py:703\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[1;32m    700\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_proxy(conn)\n\u001b[1;32m    702\u001b[0m \u001b[38;5;66;03m# Make the request on the httplib connection object.\u001b[39;00m\n\u001b[0;32m--> 703\u001b[0m httplib_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    704\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    705\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    706\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    707\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    708\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    709\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    710\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    711\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    713\u001b[0m \u001b[38;5;66;03m# If we're going to release the connection in ``finally:``, then\u001b[39;00m\n\u001b[1;32m    714\u001b[0m \u001b[38;5;66;03m# the response doesn't need to know about the connection. Otherwise\u001b[39;00m\n\u001b[1;32m    715\u001b[0m \u001b[38;5;66;03m# it will also try to release it and we'll have a double-release\u001b[39;00m\n\u001b[1;32m    716\u001b[0m \u001b[38;5;66;03m# mess.\u001b[39;00m\n\u001b[1;32m    717\u001b[0m response_conn \u001b[38;5;241m=\u001b[39m conn \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m release_conn \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Bloomtech/underdog-devs-ds-a/venv/lib/python3.8/site-packages/urllib3/connectionpool.py:449\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    444\u001b[0m             httplib_response \u001b[38;5;241m=\u001b[39m conn\u001b[38;5;241m.\u001b[39mgetresponse()\n\u001b[1;32m    445\u001b[0m         \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    446\u001b[0m             \u001b[38;5;66;03m# Remove the TypeError from the exception chain in\u001b[39;00m\n\u001b[1;32m    447\u001b[0m             \u001b[38;5;66;03m# Python 3 (including for exceptions like SystemExit).\u001b[39;00m\n\u001b[1;32m    448\u001b[0m             \u001b[38;5;66;03m# Otherwise it looks like a bug in the code.\u001b[39;00m\n\u001b[0;32m--> 449\u001b[0m             \u001b[43msix\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_from\u001b[49m\u001b[43m(\u001b[49m\u001b[43me\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    450\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (SocketTimeout, BaseSSLError, SocketError) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    451\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_raise_timeout(err\u001b[38;5;241m=\u001b[39me, url\u001b[38;5;241m=\u001b[39murl, timeout_value\u001b[38;5;241m=\u001b[39mread_timeout)\n",
      "File \u001b[0;32m<string>:3\u001b[0m, in \u001b[0;36mraise_from\u001b[0;34m(value, from_value)\u001b[0m\n",
      "File \u001b[0;32m~/Bloomtech/underdog-devs-ds-a/venv/lib/python3.8/site-packages/urllib3/connectionpool.py:444\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    441\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m    442\u001b[0m     \u001b[38;5;66;03m# Python 3\u001b[39;00m\n\u001b[1;32m    443\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 444\u001b[0m         httplib_response \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    445\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    446\u001b[0m         \u001b[38;5;66;03m# Remove the TypeError from the exception chain in\u001b[39;00m\n\u001b[1;32m    447\u001b[0m         \u001b[38;5;66;03m# Python 3 (including for exceptions like SystemExit).\u001b[39;00m\n\u001b[1;32m    448\u001b[0m         \u001b[38;5;66;03m# Otherwise it looks like a bug in the code.\u001b[39;00m\n\u001b[1;32m    449\u001b[0m         six\u001b[38;5;241m.\u001b[39mraise_from(e, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.8/http/client.py:1347\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1345\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1346\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1347\u001b[0m         \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbegin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1348\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m:\n\u001b[1;32m   1349\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.8/http/client.py:307\u001b[0m, in \u001b[0;36mHTTPResponse.begin\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    305\u001b[0m \u001b[38;5;66;03m# read until we get a non-100 response\u001b[39;00m\n\u001b[1;32m    306\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 307\u001b[0m     version, status, reason \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_read_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    308\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m status \u001b[38;5;241m!=\u001b[39m CONTINUE:\n\u001b[1;32m    309\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.8/http/client.py:268\u001b[0m, in \u001b[0;36mHTTPResponse._read_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    267\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_read_status\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 268\u001b[0m     line \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_MAXLINE\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miso-8859-1\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    269\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(line) \u001b[38;5;241m>\u001b[39m _MAXLINE:\n\u001b[1;32m    270\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m LineTooLong(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstatus line\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.8/socket.py:669\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    667\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    668\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 669\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    670\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[1;32m    671\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Loop through all search terms\n",
    "for position in search_terms:\n",
    "    \n",
    "    # Find and scrape positions\n",
    "    scraped_list = repeat_scrape_until_10th_page(browser, position)\n",
    "\n",
    "    # Define dataframe-creating function\n",
    "    def create_linkedin_dataframe(scraped_list):\n",
    "        \"\"\"\n",
    "        Create a dataframe from the scraped text.\n",
    "        \"\"\"\n",
    "        linkedin_jobs = pd.DataFrame(scraped_list, \n",
    "            index=[\"company\",\"job_title\",\"location\",\"description\"]).T\n",
    "        \n",
    "        return linkedin_jobs\n",
    "\n",
    "    # Create dataframe from results\n",
    "    linkedin_jobs = create_linkedin_dataframe(scraped_list)\n",
    "\n",
    "    # Look at number of unique jobs to account for duplicate postings\n",
    "    print(f\"Number of unique jobs found for {position}:\", \n",
    "          linkedin_jobs['description'].nunique())\n",
    "\n",
    "    # Begin filtering and saving work\n",
    "    saving_jobs = copy.deepcopy(linkedin_jobs)\n",
    "    \n",
    "    # Drop duplicates\n",
    "    saving_jobs = saving_jobs.drop_duplicates(['description']).reset_index(drop=True)\n",
    "    \n",
    "    # Remove jobs scraped without descriptions\n",
    "    saving_jobs = saving_jobs[saving_jobs['description'].notna()]\n",
    "    display(saving_jobs)\n",
    "\n",
    "    # Rename position variable\n",
    "    position = position.replace(\"%20\",\"_\")\n",
    "\n",
    "    # Save unique jobs\n",
    "    saving_jobs.to_csv(f'individual_jobs/{position}_jobs.csv', encoding='utf-8', index=False)\n",
    "    print(f'Successfully saved {position} jobs!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d467c66f",
   "metadata": {},
   "source": [
    "### Merge our saved datasets into one super dataset with all job titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e990f62c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "web_developer_jobs.csv\n",
      "individual_jobs/web_developer_jobs.csv\n",
      "frontend_developer_jobs.csv\n",
      "individual_jobs/frontend_developer_jobs.csv\n",
      "data_scientist_jobs.csv\n",
      "individual_jobs/data_scientist_jobs.csv\n",
      "data_engineer_jobs.csv\n",
      "individual_jobs/data_engineer_jobs.csv\n",
      "machine_learning_engineer_jobs.csv\n",
      "individual_jobs/machine_learning_engineer_jobs.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>company</th>\n",
       "      <th>job_title</th>\n",
       "      <th>location</th>\n",
       "      <th>description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Pinckney Marketing</td>\n",
       "      <td>Junior Web Developer</td>\n",
       "      <td>Charlotte, NC\\nOn-site</td>\n",
       "      <td>Role: Junior Web Developer\\n\\n\\n\\n\\nRole Overv...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The Lukens Company</td>\n",
       "      <td>Web Developer &amp; Designer</td>\n",
       "      <td>Arlington, VA\\nHybrid</td>\n",
       "      <td>The Lukens Company (TLC) is an award-winning f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Esri</td>\n",
       "      <td>Back End Web Developer - ArcGIS StoryMaps</td>\n",
       "      <td>Redlands, CA\\nRemote</td>\n",
       "      <td>Overview\\n\\nJoin our team and build next gener...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Apex Systems</td>\n",
       "      <td>Web Developer</td>\n",
       "      <td>Denver Metropolitan Area\\nHybrid</td>\n",
       "      <td>Apex Systems is currently looking for multiple...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Foodable Tech</td>\n",
       "      <td>Web Developer</td>\n",
       "      <td>San Francisco Bay Area\\nRemote</td>\n",
       "      <td>This role is particularly well-suited for some...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>932</th>\n",
       "      <td>PatientPoint®</td>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>Cincinnati, OH\\nRemote</td>\n",
       "      <td>Position: Data Scientist\\n\\nLocation: Cincinna...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>933</th>\n",
       "      <td>Meta</td>\n",
       "      <td>Research Data Scientist (University Grad)</td>\n",
       "      <td>Bellevue, WA</td>\n",
       "      <td>The Infrastructure Data Science and Engineerin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>934</th>\n",
       "      <td>Oracle</td>\n",
       "      <td>Principal Data Scientist - Machine Learning En...</td>\n",
       "      <td>United States\\nRemote</td>\n",
       "      <td>Posted by\\n\\nGreg Amorose\\n\\nSenior Recruiting...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>935</th>\n",
       "      <td>Amazon</td>\n",
       "      <td>Data Scientist I</td>\n",
       "      <td>Cupertino, CA</td>\n",
       "      <td>Job Summary\\n\\nDESCRIPTION\\n\\nAre you customer...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>936</th>\n",
       "      <td>Just Slide Media</td>\n",
       "      <td>Data Engineer</td>\n",
       "      <td>Los Angeles Metropolitan Area\\nHybrid</td>\n",
       "      <td>Posted by\\n\\nSteven Sesar 2nd\\n\\nGrowth is my ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>937 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                company                                          job_title  \\\n",
       "0    Pinckney Marketing                               Junior Web Developer   \n",
       "1    The Lukens Company                           Web Developer & Designer   \n",
       "2                  Esri          Back End Web Developer - ArcGIS StoryMaps   \n",
       "3          Apex Systems                                      Web Developer   \n",
       "4         Foodable Tech                                      Web Developer   \n",
       "..                  ...                                                ...   \n",
       "932       PatientPoint®                                     Data Scientist   \n",
       "933                Meta          Research Data Scientist (University Grad)   \n",
       "934              Oracle  Principal Data Scientist - Machine Learning En...   \n",
       "935              Amazon                                   Data Scientist I   \n",
       "936    Just Slide Media                                      Data Engineer   \n",
       "\n",
       "                                  location  \\\n",
       "0                   Charlotte, NC\\nOn-site   \n",
       "1                    Arlington, VA\\nHybrid   \n",
       "2                     Redlands, CA\\nRemote   \n",
       "3         Denver Metropolitan Area\\nHybrid   \n",
       "4           San Francisco Bay Area\\nRemote   \n",
       "..                                     ...   \n",
       "932                 Cincinnati, OH\\nRemote   \n",
       "933                           Bellevue, WA   \n",
       "934                  United States\\nRemote   \n",
       "935                          Cupertino, CA   \n",
       "936  Los Angeles Metropolitan Area\\nHybrid   \n",
       "\n",
       "                                           description  \n",
       "0    Role: Junior Web Developer\\n\\n\\n\\n\\nRole Overv...  \n",
       "1    The Lukens Company (TLC) is an award-winning f...  \n",
       "2    Overview\\n\\nJoin our team and build next gener...  \n",
       "3    Apex Systems is currently looking for multiple...  \n",
       "4    This role is particularly well-suited for some...  \n",
       "..                                                 ...  \n",
       "932  Position: Data Scientist\\n\\nLocation: Cincinna...  \n",
       "933  The Infrastructure Data Science and Engineerin...  \n",
       "934  Posted by\\n\\nGreg Amorose\\n\\nSenior Recruiting...  \n",
       "935  Job Summary\\n\\nDESCRIPTION\\n\\nAre you customer...  \n",
       "936  Posted by\\n\\nSteven Sesar 2nd\\n\\nGrowth is my ...  \n",
       "\n",
       "[937 rows x 4 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get filepath and encode individual_jobs/ folder\n",
    "individual_jobs_filepath = os.fsencode('individual_jobs/')\n",
    "\n",
    "# Create empty list to add dataframes to\n",
    "dataframe_list = []\n",
    "\n",
    "# Iterate through folder\n",
    "for file in os.listdir(individual_jobs_filepath):\n",
    "    \n",
    "    # Get actual filename of csv\n",
    "    filename = os.fsdecode(file)\n",
    "    \n",
    "    # Get actual path of csv\n",
    "    dataset_path = os.path.join('individual_jobs/', filename)\n",
    "    \n",
    "    # Read csv into a dataframe\n",
    "    data_df = pd.read_csv(dataset_path, encoding='utf-8')\n",
    "    \n",
    "    # Append dataframe to a list to be concatenated\n",
    "    dataframe_list.append(data_df)\n",
    "\n",
    "# Concat all dataframes into one big dataframe\n",
    "# with all job titles\n",
    "all_jobs_df = pd.concat(dataframe_list).reset_index(drop=True)\n",
    "\n",
    "# Display new dataframe\n",
    "all_jobs_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9fc613e",
   "metadata": {},
   "source": [
    "### Save our super dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "09c2c54e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully saved jobs!\n"
     ]
    }
   ],
   "source": [
    "# Save our dataset\n",
    "all_jobs_df.to_csv(f'scraped_linkedin_jobs.csv', encoding='utf-8', index=False)\n",
    "print(f'Successfully saved jobs!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b68065be",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "underdog",
   "language": "python",
   "name": "underdog"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
